[
  {
    "Pattern Name": "FewShot Prompting",
    "Problem": "GenAIs need to learn skills and tasks with limited examples without weight updates.",
    "Context": "When training data is unavailable or model parameters cannot be updated.",
    "Solution": "Provide the GenAI with a few examples (exemplars) of a task being completed within the prompt.",
    "Result": "The GenAI learns to complete the task, often improving model performance, especially in larger models.",
    "Related Patterns": "InContext Learning, ZeroShot Prompting, Exemplar Selection techniques.",
    "Category": "Prompt Design",
    "Uses": "Task completion, classification, question answering.",
    "Thinking": "Identified as a core paradigm for LLM interaction, explicitly described with problem, solution, and result, and categorized under 'InContext Learning'."
  },
  {
    "Pattern Name": "KNN Exemplar Selection",
    "Problem": "Selecting effective exemplars for FewShot Prompting can be difficult due to context window limitations and performance dependency on exemplar quality.",
    "Context": "When a training dataset (Dtrain) is available and exemplars need to be dynamically selected for a test instance (Dtest_xi).",
    "Solution": "Select exemplars from Dtrain that are similar to the Dtest_xi using a K-Nearest Neighbor algorithm.",
    "Result": "Boosts performance by providing relevant examples, though it can be time and resource-intensive during prompt generation.",
    "Related Patterns": "FewShot Prompting, VoteK Exemplar Selection, Prompt Mining.",
    "Category": "Prompt Design",
    "Uses": "Improving FewShot Prompting performance by selecting relevant examples.",
    "Thinking": "This is a specific technique for optimizing FewShot Prompting, addressing the problem of exemplar selection, and is distinct from the general FewShot pattern."
  },
  {
    "Pattern Name": "VoteK Exemplar Selection",
    "Problem": "Selecting effective and diverse exemplars for FewShot Prompting, especially when some data is unlabeled.",
    "Context": "When a pool of unlabeled candidate exemplars exists, and diversity is desired in the selected examples.",
    "Solution": "In a two-stage process, a model proposes useful unlabeled candidate exemplars for human annotation. The labeled pool is then used for FewShot Prompting, ensuring newly added exemplars are sufficiently different to increase diversity and representativeness.",
    "Result": "Improves FewShot Prompting performance by selecting similar and diverse exemplars.",
    "Related Patterns": "FewShot Prompting, KNN Exemplar Selection.",
    "Category": "Prompt Design",
    "Uses": "Improving FewShot Prompting performance, especially with human-in-the-loop labeling for diversity.",
    "Thinking": "Similar to KNN, this is a specific technique for exemplar selection within FewShot Prompting, with a unique two-stage process."
  },
  {
    "Pattern Name": "Self-Generated InContext Learning (SGICL)",
    "Problem": "Lack of actual training data for FewShot Prompting.",
    "Context": "When training data is unavailable, but a GenAI can be leveraged to create examples.",
    "Solution": "Leverage a GenAI to automatically generate exemplars for use in FewShot prompts.",
    "Result": "Provides samples for FewShot Prompting, performing better than zero-shot scenarios, though generated samples may not be as effective as actual data.",
    "Related Patterns": "FewShot Prompting, Analogical Prompting.",
    "Category": "Generative AI",
    "Uses": "Creating synthetic exemplars for FewShot Prompting when real data is scarce.",
    "Thinking": "This pattern uses the generative capability of AI to create training data for itself, making it a Generative AI pattern specific to prompt design."
  },
  {
    "Pattern Name": "Prompt Mining",
    "Problem": "Finding optimal prompt structures or 'middle words' for improved LLM performance.",
    "Context": "When seeking to optimize prompt performance by discovering effective prompt templates or formats.",
    "Solution": "Analyze large corpora to discover optimal 'middle words' or formats that occur frequently in the corpus, which are then used as prompt templates.",
    "Result": "Improved prompt performance by using formats that align with the model's training data distribution.",
    "Related Patterns": "FewShot Prompting, Prompt Engineering.",
    "Category": "Prompt Design",
    "Uses": "Discovering effective prompt templates and formats for various tasks.",
    "Thinking": "This is a technique for automatically discovering effective prompt structures, which is a specific AI design pattern for prompt optimization."
  },
  {
    "Pattern Name": "ZeroShot Prompting",
    "Problem": "Guiding GenAIs to complete tasks without any examples.",
    "Context": "When no exemplars are available or desired, and only an instruction in natural language is given to the model.",
    "Solution": "Provide a prompt with zero exemplars, relying solely on instructions to guide the GenAI's output.",
    "Result": "The GenAI performs the task based on its pre-trained knowledge and the given instructions.",
    "Related Patterns": "InContext Learning, FewShot Prompting, ZeroShot CoT.",
    "Category": "Prompt Design",
    "Uses": "Open-ended tasks, initial task exploration, when no examples are available.",
    "Thinking": "Identified as a fundamental paradigm for LLM interaction, explicitly described with problem, solution, and result, and categorized under 'InContext Learning'."
  },
  {
    "Pattern Name": "Role Prompting",
    "Problem": "Achieving more desirable outputs or improving accuracy for open-ended tasks by influencing the GenAI's persona.",
    "Context": "When the desired output style, tone, or content can be enhanced by assigning a specific persona to the GenAI.",
    "Solution": "Assign a specific role or persona to the GenAI within the prompt (e.g., 'Pretend you are a shepherd').",
    "Result": "Creates more desirable outputs for open-ended tasks and can improve accuracy on benchmarks.",
    "Related Patterns": "Style Prompting, Emotion Prompting.",
    "Category": "Prompt Design",
    "Uses": "Content generation, creative writing, specific domain interactions.",
    "Thinking": "This is a specific technique for influencing LLM behavior through prompt structure, directly impacting the AI's output style and content."
  },
  {
    "Pattern Name": "Style Prompting",
    "Problem": "Shaping the output of a GenAI to a desired stylistic quality.",
    "Context": "When the user wants to control the tone, genre, or overall writing style of the GenAI's output.",
    "Solution": "Specify the desired style, tone, or genre directly in the prompt (e.g., 'Write a clear and curt paragraph').",
    "Result": "The GenAI produces output that adheres to the specified stylistic requirements.",
    "Related Patterns": "Role Prompting, Negative Prompting.",
    "Category": "Prompt Design",
    "Uses": "Content generation, creative writing, formal/informal communication.",
    "Thinking": "Similar to Role Prompting, this is a specific technique for controlling LLM output attributes through prompt instructions."
  },
  {
    "Pattern Name": "Emotion Prompting",
    "Problem": "Improving LLM performance on benchmarks and open-ended text generation.",
    "Context": "When seeking to enhance the model's understanding or motivation for a task.",
    "Solution": "Incorporate phrases of psychological relevance to humans (e.g., 'This is important to my career') into the prompt.",
    "Result": "May lead to improved LLM performance on benchmarks and open-ended text generation.",
    "Related Patterns": "Role Prompting, Style Prompting.",
    "Category": "Prompt Design",
    "Uses": "Enhancing task performance, generating more empathetic or contextually aware responses.",
    "Thinking": "This technique leverages human psychological cues in prompts to influence AI behavior, making it an AI-Human Interaction pattern within prompt design."
  },
  {
    "Pattern Name": "System 2 Attention (S2A)",
    "Problem": "Dealing with irrelevant information in a prompt that might distract the LLM from the core question.",
    "Context": "When a prompt contains extraneous details alongside the main query.",
    "Solution": "First, ask an LLM to rewrite the prompt, removing any information unrelated to the question. Then, pass this new, refined prompt to an LLM to retrieve a final response.",
    "Result": "Helps eliminate the effect of irrelevant information, potentially leading to more accurate answers.",
    "Related Patterns": "Rephrase and Respond (RaR), SimToM.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Improving question answering, reducing noise in complex prompts.",
    "Thinking": "This involves a multi-step AI process (rewriting, then answering) to improve reasoning, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "SimToM",
    "Problem": "Answering complicated questions involving multiple people or objects, where establishing facts known by specific entities is crucial.",
    "Context": "When a question requires understanding different perspectives or knowledge states of various entities mentioned in the prompt.",
    "Solution": "Given the question, the LLM first attempts to establish the set of facts one person knows, then answers the question based only on those facts. This is a two-prompt process.",
    "Result": "Helps eliminate the effect of irrelevant information and improves reasoning in complex scenarios.",
    "Related Patterns": "System 2 Attention (S2A), SelfAsk.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Complex question answering, multi-entity reasoning.",
    "Thinking": "This pattern explicitly addresses reasoning about knowledge states, making it a Knowledge & Reasoning pattern."
  },
  {
    "Pattern Name": "Rephrase and Respond (RaR)",
    "Problem": "Improving the LLM's understanding and response quality to a given question.",
    "Context": "When a direct answer to a question might benefit from the LLM first re-evaluating or expanding on the query.",
    "Solution": "Instruct the LLM to rephrase and expand the question before generating the final answer. This can be done in a single pass or by passing the new question separately.",
    "Result": "Demonstrated improvements on multiple benchmarks by encouraging deeper understanding of the question.",
    "Related Patterns": "System 2 Attention (S2A), Rereading (RE2).",
    "Category": "Knowledge & Reasoning",
    "Uses": "Improving question answering, enhancing reasoning capabilities.",
    "Thinking": "This pattern involves the AI actively processing and improving the input question before answering, which is a form of reasoning enhancement."
  },
  {
    "Pattern Name": "Rereading (RE2)",
    "Problem": "Improving reasoning performance, especially with complex questions.",
    "Context": "When the LLM might benefit from re-processing the question to ensure full comprehension.",
    "Solution": "Add the phrase 'Read the question again' to the prompt, in addition to repeating the question itself.",
    "Result": "Shown improvement in reasoning benchmarks, especially with complex questions.",
    "Related Patterns": "Rephrase and Respond (RaR).",
    "Category": "Knowledge & Reasoning",
    "Uses": "Enhancing reasoning, particularly for intricate problems.",
    "Thinking": "A simple yet effective technique to improve AI's processing of complex inputs, directly related to reasoning."
  },
  {
    "Pattern Name": "SelfAsk",
    "Problem": "Answering complex questions that might require follow-up information.",
    "Context": "When the initial prompt might be insufficient for a complete or accurate answer, and the LLM could benefit from asking clarifying questions.",
    "Solution": "Prompt LLMs to first decide if they need to ask follow-up questions. If so, the LLM generates these questions, then answers them, and finally answers the original question.",
    "Result": "Enables LLMs to gather necessary information, leading to more comprehensive and accurate answers.",
    "Related Patterns": "ChainofThought, Question Clarification.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Complex question answering, information gathering, interactive problem-solving.",
    "Thinking": "This pattern involves the AI generating its own sub-questions and answering them, demonstrating a form of planning and reasoning."
  },
  {
    "Pattern Name": "ChainofThought (CoT) Prompting",
    "Problem": "Enhancing LLM performance in mathematics and reasoning tasks by making the reasoning process explicit.",
    "Context": "When solving problems that require multi-step reasoning, where showing the thought process can guide the LLM.",
    "Solution": "Leverage fewshot prompting to encourage the LLM to express its thought process before delivering its final answer, typically by including an exemplar with a question, a reasoning path, and the correct answer.",
    "Result": "Significantly enhances the LLM's performance in mathematics and reasoning tasks.",
    "Related Patterns": "ZeroShot CoT, FewShot CoT, Decomposition techniques.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Mathematical problem-solving, logical reasoning, complex question answering.",
    "Thinking": "A foundational pattern for improving LLM reasoning by externalizing the thought process, central to Knowledge & Reasoning."
  },
  {
    "Pattern Name": "ZeroShot CoT",
    "Problem": "Improving reasoning performance without requiring exemplars.",
    "Context": "When exemplars are unavailable or impractical to include, but the task still benefits from explicit reasoning steps.",
    "Solution": "Append a thought-inducing phrase like 'Let's think step by step' to the prompt.",
    "Result": "Enhances reasoning capabilities without the need for few-shot examples, making it generally task-agnostic.",
    "Related Patterns": "ChainofThought (CoT) Prompting, StepBack Prompting, Analogical Prompting.",
    "Category": "Knowledge & Reasoning",
    "Uses": "General reasoning tasks, quick application of CoT without example curation.",
    "Thinking": "A specific variant of CoT that highlights the power of simple instructions for reasoning, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "StepBack Prompting",
    "Problem": "Improving reasoning performance on complex problems by focusing on high-level concepts first.",
    "Context": "When a problem requires understanding underlying concepts or facts before detailed reasoning.",
    "Solution": "First, ask the LLM a generic, high-level question about relevant concepts or facts before delving into the specific problem's reasoning.",
    "Result": "Improved performance significantly on multiple reasoning benchmarks.",
    "Related Patterns": "ChainofThought (CoT) Prompting, LeasttoMost Prompting.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Complex reasoning tasks, abstract problem-solving.",
    "Thinking": "This pattern guides the AI to abstract and reason about foundational concepts, a clear Knowledge & Reasoning pattern."
  },
  {
    "Pattern Name": "Analogical Prompting",
    "Problem": "Generating effective ChainofThought exemplars automatically for mathematical reasoning and code generation.",
    "Context": "When CoT exemplars are needed but manual creation is time-consuming or difficult.",
    "Solution": "Automatically generate exemplars that include ChainofThoughts, similar to Self-Generated InContext Learning.",
    "Result": "Demonstrated improvements in mathematical reasoning and code generation tasks.",
    "Related Patterns": "Self-Generated InContext Learning (SGICL), ChainofThought (CoT) Prompting.",
    "Category": "Generative AI",
    "Uses": "Automating CoT exemplar creation, improving reasoning and code generation.",
    "Thinking": "Combines generative AI (for exemplars) with reasoning (CoT), making it a Generative AI pattern for Knowledge & Reasoning."
  },
  {
    "Pattern Name": "ThreadofThought (ThoT) Prompting",
    "Problem": "Improving CoT reasoning, especially in question-answering and retrieval settings with large, complex contexts.",
    "Context": "When dealing with extensive and intricate textual contexts for reasoning tasks.",
    "Solution": "Use an improved thought inducer for CoT reasoning, such as 'Walk me through this context in manageable parts step by step, summarizing and analyzing as we go.'",
    "Result": "Works well in question-answering and retrieval settings, especially with large complex contexts.",
    "Related Patterns": "ChainofThought (CoT) Prompting, ZeroShot CoT.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Question answering, retrieval-augmented generation, processing long documents.",
    "Thinking": "This is a specific CoT variant designed for complex contexts, enhancing the AI's ability to reason through structured summarization."
  },
  {
    "Pattern Name": "Tabular ChainofThought (TabCoT)",
    "Problem": "Improving the structure and reasoning of LLM outputs for CoT prompts.",
    "Context": "When a more organized and structured reasoning output is beneficial for clarity and accuracy.",
    "Solution": "Use a ZeroShot CoT prompt that instructs the LLM to output its reasoning as a markdown table.",
    "Result": "Enables the LLM to improve the structure and thus the reasoning of its output.",
    "Related Patterns": "ChainofThought (CoT) Prompting, ZeroShot CoT.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Structured reasoning, data analysis, step-by-step problem solving where clarity is key.",
    "Thinking": "This pattern focuses on structuring the AI's reasoning output for better clarity and performance, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Contrastive CoT Prompting",
    "Problem": "Showing the LLM how *not* to reason to improve its performance.",
    "Context": "When the LLM struggles with common reasoning pitfalls or requires explicit guidance on incorrect reasoning paths.",
    "Solution": "Add both exemplars with incorrect and correct explanations to the CoT prompt.",
    "Result": "Shown significant improvement in areas like Arithmetic Reasoning and Factual QA.",
    "Related Patterns": "ChainofThought (CoT) Prompting, FewShot CoT.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Improving reasoning accuracy, correcting common errors, teaching 'anti-patterns' of thought.",
    "Thinking": "This pattern explicitly uses negative examples to guide AI reasoning, a sophisticated approach to Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Uncertainty-Routed CoT Prompting",
    "Problem": "Improving reasoning accuracy by handling uncertainty in reasoning paths.",
    "Context": "When multiple CoT reasoning paths are possible, and a robust selection mechanism is needed.",
    "Solution": "Sample multiple CoT reasoning paths, then select the majority if it is above a certain threshold (calculated based on validation data). If not, sample greedily and select that response.",
    "Result": "Demonstrates improvement on benchmarks like MMLU for both GPT-4 and Gemini Ultra models.",
    "Related Patterns": "ChainofThought (CoT) Prompting, SelfConsistency.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Improving reasoning accuracy, robust decision-making in uncertain reasoning scenarios.",
    "Thinking": "This pattern involves sampling and routing based on uncertainty, a form of meta-reasoning for Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Complexity-based Prompting",
    "Problem": "Improving CoT performance for multi-step reasoning by focusing on complex examples and robust aggregation.",
    "Context": "When dealing with mathematical reasoning or other tasks requiring complex, multi-step thought processes.",
    "Solution": "1. Select complex examples for annotation and inclusion in the prompt based on factors like question length or reasoning steps. 2. During inference, sample multiple reasoning chains/answers and use a majority vote among chains exceeding a certain length threshold, assuming longer reasoning indicates higher answer quality.",
    "Result": "Shown improvements on mathematical reasoning datasets.",
    "Related Patterns": "ChainofThought (CoT) Prompting, SelfConsistency.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Mathematical reasoning, complex problem-solving, enhancing CoT robustness.",
    "Thinking": "This pattern optimizes CoT by focusing on complexity and using a length-based heuristic for quality, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Active Prompting",
    "Problem": "Improving FewShot CoT prompts by identifying and refining uncertain exemplars.",
    "Context": "When some training questions/exemplars are available, but their quality or clarity for CoT reasoning is uncertain.",
    "Solution": "Start with some training questions/exemplars, ask the LLM to solve them, calculate uncertainty (disagreement), and then ask human annotators to rewrite the exemplars with the highest uncertainty.",
    "Result": "Improves the quality of FewShot CoT prompts through human-in-the-loop refinement.",
    "Related Patterns": "FewShot CoT, Active Learning.",
    "Category": "AI\u2013Human Interaction",
    "Uses": "Improving prompt quality, reducing uncertainty in CoT exemplars.",
    "Thinking": "This pattern explicitly involves human annotators in an iterative loop with the AI to refine prompts, making it an AI-Human Interaction pattern."
  },
  {
    "Pattern Name": "MemoryofThought Prompting",
    "Problem": "Building effective FewShot CoT prompts at test time using unlabeled training exemplars.",
    "Context": "When unlabeled training exemplars are available, and dynamic FewShot CoT prompt construction is desired.",
    "Solution": "Before test time, perform inference on unlabeled training exemplars with CoT. At test time, retrieve similar instances to the test sample to build FewShot CoT prompts.",
    "Result": "Shown substantial improvements in benchmarks like Arithmetic, commonsense, and factual reasoning.",
    "Related Patterns": "FewShot CoT, Retrieval Augmented Generation (RAG).",
    "Category": "Knowledge & Reasoning",
    "Uses": "Dynamic FewShot CoT prompt construction, leveraging unlabeled data for reasoning.",
    "Thinking": "This pattern uses a form of 'memory' (pre-computed CoTs on unlabeled data) and retrieval to enhance reasoning, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Automatic ChainofThought (AutoCoT) Prompting",
    "Problem": "Automatically generating chains of thought to build FewShot CoT prompts.",
    "Context": "When manual creation of CoT exemplars is burdensome, and an automated approach is desired.",
    "Solution": "Use a ZeroShot prompt to automatically generate chains of thought. These generated CoTs are then used to build a FewShot CoT prompt for a test sample.",
    "Result": "Automates the creation of CoT exemplars, simplifying FewShot CoT implementation.",
    "Related Patterns": "ChainofThought (CoT) Prompting, ZeroShot CoT, Self-Generated InContext Learning (SGICL).",
    "Category": "Generative AI",
    "Uses": "Automating CoT exemplar creation, scaling FewShot CoT applications.",
    "Thinking": "This pattern uses the generative capabilities of LLMs to create reasoning paths, making it a Generative AI pattern for Knowledge & Reasoning."
  },
  {
    "Pattern Name": "LeasttoMost Prompting",
    "Problem": "Solving complex problems by breaking them down into simpler, sequential subproblems.",
    "Context": "When a problem is too complex to solve in a single step and can be naturally decomposed.",
    "Solution": "Prompt an LLM to first break a given problem into subproblems without solving them. Then, solve them sequentially, appending model responses to the prompt each time until a final result is achieved.",
    "Result": "Shown significant improvements in tasks involving symbolic manipulation, compositional generalization, and mathematical reasoning.",
    "Related Patterns": "Decomposed Prompting (DECOMP), PlanandSolve Prompting, TreeofThought (ToT).",
    "Category": "Planning",
    "Uses": "Complex problem-solving, mathematical reasoning, symbolic tasks.",
    "Thinking": "This pattern explicitly describes a planning strategy (decomposition and sequential solving) for AI, fitting the Planning category."
  },
  {
    "Pattern Name": "Decomposed Prompting (DECOMP)",
    "Problem": "Solving complex problems by leveraging external functions or tools for subproblems.",
    "Context": "When a problem can be broken down into subproblems that can be effectively handled by specific functions (e.g., string splitting, internet searching).",
    "Solution": "FewShot prompt an LLM to show it how to use certain functions (often implemented as separate LLM calls). The LLM then breaks down its original problem into subproblems, sending them to different functions.",
    "Result": "Shown improved performance over LeasttoMost prompting on some tasks by integrating tool use.",
    "Related Patterns": "LeasttoMost Prompting, ProgramofThoughts, Tool Use Agents.",
    "Category": "Planning",
    "Uses": "Complex problem-solving, integrating external tools for specific subtasks.",
    "Thinking": "This pattern combines planning (decomposition) with tool integration, making it a Planning pattern with Tools Integration aspects."
  },
  {
    "Pattern Name": "PlanandSolve Prompting",
    "Problem": "Generating more robust reasoning processes than standard ZeroShot CoT.",
    "Context": "When a problem requires a structured approach of understanding and planning before execution.",
    "Solution": "Use an improved ZeroShot CoT prompt: 'Let's first understand the problem and devise a plan to solve it. Then, let's carry out the plan and solve the problem step by step.'",
    "Result": "Generates more robust reasoning processes than standard ZeroShot CoT on multiple reasoning datasets.",
    "Related Patterns": "ChainofThought (CoT) Prompting, LeasttoMost Prompting.",
    "Category": "Planning",
    "Uses": "General reasoning tasks, structured problem-solving.",
    "Thinking": "This pattern explicitly instructs the AI to plan its solution, a core aspect of Planning."
  },
  {
    "Pattern Name": "TreeofThought (ToT)",
    "Problem": "Solving tasks that require search and planning by exploring multiple reasoning paths.",
    "Context": "When a problem has multiple possible intermediate steps or 'thoughts' and requires evaluation of progress towards a solution.",
    "Solution": "Create a tree-like search problem by starting with an initial problem, generating multiple possible steps (thoughts) as from a CoT, evaluating the progress each step makes towards solving the problem through prompting, and deciding which steps to continue with.",
    "Result": "Particularly effective for tasks that require search and planning.",
    "Related Patterns": "ChainofThought (CoT) Prompting, LeasttoMost Prompting, RecursionofThought.",
    "Category": "Planning",
    "Uses": "Complex problem-solving, strategic decision-making, search-intensive tasks.",
    "Thinking": "This pattern describes a sophisticated planning and search algorithm implemented via prompting, clearly fitting Planning and Knowledge & Reasoning."
  },
  {
    "Pattern Name": "RecursionofThought",
    "Problem": "Solving complicated subproblems within a reasoning chain, especially those that might exceed context length.",
    "Context": "When an LLM encounters a complex subproblem during its reasoning process that requires deeper, recursive processing.",
    "Solution": "Every time a complicated problem is encountered in the middle of its reasoning chain, send this subproblem into another prompt/LLM call. After it's completed, the answer is inserted into the original prompt.",
    "Result": "Can recursively solve complex problems, including ones which might otherwise run over the maximum context length, showing improvements on arithmetic and algorithmic tasks.",
    "Related Patterns": "ChainofThought (CoT) Prompting, TreeofThought (ToT), Decomposition techniques.",
    "Category": "Planning",
    "Uses": "Solving nested complex problems, handling long context reasoning.",
    "Thinking": "This pattern involves recursive problem-solving, a form of advanced planning and reasoning."
  },
  {
    "Pattern Name": "ProgramofThoughts",
    "Problem": "Improving mathematical and programming-related tasks by leveraging code as reasoning steps.",
    "Context": "When tasks involve numerical computation or logical programming constructs.",
    "Solution": "Use LLMs (like Codex) to generate programming code as reasoning steps. A code interpreter executes these steps to obtain the final answer.",
    "Result": "Excels in mathematical and programming-related tasks, though less effective for semantic reasoning.",
    "Related Patterns": "Decomposed Prompting (DECOMP), Faithful ChainofThought, Tool Use Agents.",
    "Category": "Tools Integration",
    "Uses": "Mathematical problem-solving, code generation, algorithmic tasks.",
    "Thinking": "This pattern explicitly integrates an external tool (code interpreter) with LLM reasoning, making it a Tools Integration pattern for Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Faithful ChainofThought",
    "Problem": "Generating CoT reasoning that combines natural language with symbolic language for task-dependent problem-solving.",
    "Context": "When a task benefits from both human-readable natural language reasoning and precise, executable symbolic language (e.g., Python).",
    "Solution": "Generate a CoT that includes both natural language and symbolic language reasoning, making use of different types of symbolic languages in a task-dependent fashion.",
    "Result": "Combines the benefits of natural language explanation with the rigor of symbolic computation.",
    "Related Patterns": "ProgramofThoughts, ChainofThought (CoT) Prompting.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Mathematical reasoning, logical problem-solving, tasks requiring verifiable computation.",
    "Thinking": "This pattern focuses on the *form* of reasoning (natural + symbolic language) to ensure faithfulness, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "SkeletonofThought",
    "Problem": "Accelerating answer speed through parallelization of subproblem solving.",
    "Context": "When a problem can be broken into independent subproblems that can be solved concurrently.",
    "Solution": "Prompt an LLM to create a 'skeleton' of the answer (subproblems to be solved). Then, send these questions to an LLM in parallel and concatenate all the outputs to get a final response.",
    "Result": "Accelerates answer speed through parallelization.",
    "Related Patterns": "Decomposition techniques, LeasttoMost Prompting.",
    "Category": "Planning",
    "Uses": "Speeding up complex problem-solving, parallelizing LLM calls.",
    "Thinking": "This pattern describes a planning strategy for parallel execution, directly related to Planning."
  },
  {
    "Pattern Name": "Metacognitive Prompting",
    "Problem": "Making the LLM mirror human metacognitive processes for improved problem-solving.",
    "Context": "When a problem requires self-reflection, evaluation, and confidence assessment similar to human thought processes.",
    "Solution": "Use a five-part prompt chain that includes steps like clarifying the question, preliminary judgment, evaluation of response, decision confirmation, and confidence assessment.",
    "Result": "Attempts to make the LLM mirror human metacognitive processes.",
    "Related Patterns": "SelfAsk, SelfRefine, ChainofThought (CoT) Prompting.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Complex problem-solving, enhancing self-correction and reliability.",
    "Thinking": "This pattern explicitly models human metacognition in the AI's reasoning process, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Demonstration Ensembling (DENSE)",
    "Problem": "Reducing variance and improving accuracy of LLM outputs in FewShot Prompting.",
    "Context": "When a training set is available, and robustness of FewShot Prompting is desired.",
    "Solution": "Create multiple fewshot prompts, each containing a distinct subset of exemplars from the training set. Aggregate their outputs to generate a final response.",
    "Result": "Reduces the variance of LLM outputs and often improves accuracy, at the cost of increasing model calls.",
    "Related Patterns": "FewShot Prompting, SelfConsistency, Ensembling.",
    "Category": "Prompt Design",
    "Uses": "Improving robustness and accuracy of FewShot models.",
    "Thinking": "This is an ensembling technique applied specifically to prompt design (exemplar subsets), making it a Prompt Design pattern."
  },
  {
    "Pattern Name": "Mixture of Reasoning Experts (MoRE)",
    "Problem": "Improving reasoning performance by leveraging different specialized prompts for various reasoning types.",
    "Context": "When a problem involves diverse reasoning types (e.g., factual, multi-hop, math, commonsense).",
    "Solution": "Create a set of diverse reasoning experts by using different specialized prompts for different reasoning types (e.g., retrieval augmentation for factual, CoT for math, generated knowledge for commonsense). Select the best answer from all experts based on an agreement score.",
    "Result": "Leverages specialized reasoning approaches to improve overall performance.",
    "Related Patterns": "Ensembling, ChainofThought (CoT) Prompting, Retrieval Augmented Generation (RAG).",
    "Category": "Knowledge & Reasoning",
    "Uses": "Complex reasoning tasks, combining multiple reasoning strategies.",
    "Thinking": "This pattern involves orchestrating multiple AI 'experts' with different reasoning strategies, fitting Knowledge & Reasoning and Ensembling."
  },
  {
    "Pattern Name": "Max Mutual Information Method",
    "Problem": "Selecting the optimal prompt template from multiple variations.",
    "Context": "When multiple prompt templates with varied styles and exemplars have been created.",
    "Solution": "Select the optimal template as the one that maximizes mutual information between the prompt and the LLM's outputs.",
    "Result": "Identifies the most effective prompt template for a given task.",
    "Related Patterns": "Prompt Engineering, Ensembling.",
    "Category": "Prompt Design",
    "Uses": "Prompt optimization, A/B testing of prompt variations.",
    "Thinking": "This is a specific method for prompt selection based on an information-theoretic metric, directly related to Prompt Design optimization."
  },
  {
    "Pattern Name": "SelfConsistency",
    "Problem": "Reducing the variance of LLM outputs and improving accuracy in reasoning tasks.",
    "Context": "When multiple different reasoning paths can lead to the same answer, and robustness is desired.",
    "Solution": "Prompt the LLM multiple times to perform CoT (with a non-zero temperature to elicit diverse reasoning paths). Use a majority vote over all generated responses to select a final response.",
    "Result": "Shown improvements on arithmetic, commonsense, and symbolic reasoning tasks by leveraging diverse reasoning paths.",
    "Related Patterns": "ChainofThought (CoT) Prompting, Ensembling, Universal SelfConsistency.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Improving reasoning accuracy, reducing errors in complex tasks.",
    "Thinking": "This pattern uses an ensembling approach over reasoning paths to improve accuracy, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Universal SelfConsistency",
    "Problem": "Aggregating diverse freeform text generations or slightly different answers from SelfConsistency.",
    "Context": "When SelfConsistency is applied, but direct programmatic counting of majority responses is difficult due to freeform text or minor variations.",
    "Solution": "Similar to SelfConsistency, but rather than programmatically counting, insert all outputs into a prompt template that selects the majority answer (e.g., by having another LLM decide).",
    "Result": "Helpful for freeform text generation and cases where the same answer may be output slightly differently by different prompts.",
    "Related Patterns": "SelfConsistency, MetaReasoning over Multiple CoTs.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Robust aggregation of LLM outputs, handling freeform text in ensembling.",
    "Thinking": "An extension of SelfConsistency, specifically addressing the challenge of aggregating diverse text outputs, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "MetaReasoning over Multiple CoTs",
    "Problem": "Generating a final answer from multiple reasoning chains, even if they don't produce final answers directly.",
    "Context": "When multiple CoT reasoning chains are generated, and a consolidated, final answer is needed.",
    "Solution": "First generate multiple reasoning chains (not necessarily final answers) for a given problem. Next, insert all of these chains into a single prompt template, then generate a final answer from them.",
    "Result": "Consolidates diverse reasoning paths into a single, refined answer.",
    "Related Patterns": "SelfConsistency, Universal SelfConsistency, ChainofThought (CoT) Prompting.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Refining answers from multiple reasoning attempts, complex problem-solving.",
    "Thinking": "This pattern involves an LLM reasoning *about* other LLM reasoning chains, a form of meta-reasoning for Knowledge & Reasoning."
  },
  {
    "Pattern Name": "DiVeRSe",
    "Problem": "Improving reasoning performance by scoring and selecting reasoning paths from multiple prompts.",
    "Context": "When multiple prompts and reasoning paths are generated for a problem, and a robust selection mechanism is needed.",
    "Solution": "Create multiple prompts for a given problem, then perform SelfConsistency for each, generating multiple reasoning paths. Score reasoning paths based on each step in them, then select a final response.",
    "Result": "Enhances reasoning performance by evaluating and selecting high-quality reasoning paths.",
    "Related Patterns": "SelfConsistency, Ensembling, ChainofThought (CoT) Prompting.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Improving reasoning accuracy, robust decision-making in complex tasks.",
    "Thinking": "This pattern combines ensembling, SelfConsistency, and a scoring mechanism for reasoning paths, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Consistency-based Self-adaptive Prompting (COSP)",
    "Problem": "Constructing effective FewShot CoT prompts by leveraging ZeroShot CoT with SelfConsistency.",
    "Context": "When needing to create high-quality FewShot CoT exemplars automatically.",
    "Solution": "Run ZeroShot CoT with SelfConsistency on a set of examples, then select a high-agreement subset of the outputs to be included in the final prompt as exemplars. Perform SelfConsistency again with this final prompt.",
    "Result": "Constructs effective FewShot CoT prompts, improving performance.",
    "Related Patterns": "FewShot CoT, SelfConsistency, Universal Self-Adaptive Prompting (USP).",
    "Category": "Prompt Design",
    "Uses": "Automated FewShot CoT prompt construction, improving reasoning.",
    "Thinking": "This pattern describes an automated process for generating and selecting exemplars for CoT, making it a Prompt Design pattern."
  },
  {
    "Pattern Name": "Universal Self-Adaptive Prompting (USP)",
    "Problem": "Generalizing self-adaptive prompting to all tasks, especially using unlabeled data.",
    "Context": "When COSP needs to be applied more broadly and leverage unlabeled data for exemplar generation.",
    "Solution": "Builds upon COSP, using unlabeled data to generate exemplars and a more complicated scoring function to select them. It does not necessarily use SelfConsistency in the final step.",
    "Result": "Aims to make self-adaptive prompting generalizable to all tasks.",
    "Related Patterns": "Consistency-based Self-adaptive Prompting (COSP), Prompt Engineering.",
    "Category": "Prompt Design",
    "Uses": "Broadly applicable automated prompt construction, leveraging unlabeled data.",
    "Thinking": "An advanced, generalized version of COSP for automated prompt construction, fitting Prompt Design."
  },
  {
    "Pattern Name": "Prompt Paraphrasing",
    "Problem": "Generating variations of an original prompt for ensembling or data augmentation.",
    "Context": "When needing to create multiple prompts with varied wording but maintained meaning.",
    "Solution": "Transform an original prompt by changing some of the wording while still maintaining the overall meaning.",
    "Result": "Effectively a data augmentation technique that can be used to generate prompts for an ensemble.",
    "Related Patterns": "Ensembling, Automatic Prompt Engineer (APE).",
    "Category": "Prompt Design",
    "Uses": "Data augmentation for prompts, creating diverse prompts for ensembling.",
    "Thinking": "This is a technique for generating variations of prompts, directly related to Prompt Design and optimization."
  },
  {
    "Pattern Name": "SelfCalibration",
    "Problem": "Gauging confidence levels of LLM answers and deciding when to accept or revise them.",
    "Context": "When an LLM provides an answer, and its reliability or confidence needs to be assessed.",
    "Solution": "First, prompt an LLM to answer a question. Then, build a new prompt that includes the question, the LLM's answer, and an additional instruction asking whether the answer is correct.",
    "Result": "Useful for gauging confidence levels and informing decisions on accepting or revising original answers.",
    "Related Patterns": "SelfRefine, SelfVerification, Verbalized Score.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Assessing LLM confidence, improving answer reliability.",
    "Thinking": "This pattern involves the AI evaluating its own answer's correctness, a form of self-criticism and meta-reasoning, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "SelfRefine",
    "Problem": "Iteratively improving LLM answers based on self-generated feedback.",
    "Context": "When an initial answer from an LLM might be suboptimal and can be improved through an iterative feedback loop.",
    "Solution": "Given an initial answer from the LLM, prompt the same LLM to provide feedback on the answer, and then prompt it to improve the answer based on the feedback. This iterative process continues until a stopping condition is met.",
    "Result": "Demonstrated improvement across a range of reasoning, coding, and generation tasks.",
    "Related Patterns": "SelfCalibration, Reversing ChainofThought (RCoT), ChainofVerification (COVE).",
    "Category": "Knowledge & Reasoning",
    "Uses": "Improving answer quality, iterative problem-solving, code refinement.",
    "Thinking": "This pattern describes an iterative self-correction mechanism for the AI, a clear Knowledge & Reasoning pattern."
  },
  {
    "Pattern Name": "Reversing ChainofThought (RCoT)",
    "Problem": "Detecting factual inconsistencies in reasoning by reconstructing the problem from the answer.",
    "Context": "When an LLM generates an answer and reasoning, and there's a need to verify its factual consistency against the original problem.",
    "Solution": "First, prompt LLMs to reconstruct the problem based on the generated answer. Then, generate fine-grained comparisons between the original problem and the reconstructed problem to check for inconsistencies. These inconsistencies are converted to feedback for the LLM to revise the generated answer.",
    "Result": "Detects and rectifies factual inconsistencies in reasoning.",
    "Related Patterns": "SelfRefine, ChainofVerification (COVE), SelfVerification.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Verifying factual consistency, improving reasoning accuracy.",
    "Thinking": "This pattern uses a 'reverse' reasoning process to verify consistency, fitting Knowledge & Reasoning and self-criticism."
  },
  {
    "Pattern Name": "SelfVerification",
    "Problem": "Scoring multiple candidate solutions generated with ChainofThought to select the best one.",
    "Context": "When multiple CoT solutions are generated, and a method is needed to evaluate their quality.",
    "Solution": "Generate multiple candidate solutions with ChainofThought (CoT). Score each solution by masking certain parts of the original question and asking an LLM to predict them based on the rest of the question and the generated solution.",
    "Result": "Shown improvement on eight reasoning datasets by selecting more reliable solutions.",
    "Related Patterns": "SelfConsistency, ChainofVerification (COVE), SelfRefine.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Improving reasoning accuracy, selecting optimal solutions from multiple candidates.",
    "Thinking": "This pattern involves the AI evaluating its own solutions by predicting masked parts of the input, a form of self-criticism and reasoning."
  },
  {
    "Pattern Name": "ChainofVerification (COVE)",
    "Problem": "Reducing hallucination in large language models by verifying answers with related questions.",
    "Context": "When an LLM generates an answer, and its correctness needs to be verified to reduce hallucinations.",
    "Solution": "First, use an LLM to generate an answer to a given question. Then, create a list of related questions that would help verify the correctness of the answer. Each question is answered by the LLM, then all the information is given to the LLM to produce the final revised answer.",
    "Result": "Shown improvements in various question-answering and text-generation tasks by reducing hallucination.",
    "Related Patterns": "SelfRefine, Reversing ChainofThought (RCoT), Retrieval Augmented Generation (RAG).",
    "Category": "Knowledge & Reasoning",
    "Uses": "Reducing hallucinations, improving factual accuracy in Q&A and text generation.",
    "Thinking": "This pattern uses a multi-step verification process involving generating and answering sub-questions, fitting Knowledge & Reasoning and self-criticism."
  },
  {
    "Pattern Name": "Cumulative Reasoning",
    "Problem": "Improving logical inference and mathematical problem-solving through iterative evaluation of reasoning steps.",
    "Context": "When a problem requires multiple reasoning steps, and each step needs to be evaluated before proceeding.",
    "Solution": "First, generate several potential steps in answering the question. Then, have an LLM evaluate them, deciding to either accept or reject these steps. Finally, check whether the final answer has been arrived at. If so, terminate; otherwise, repeat the process.",
    "Result": "Demonstrated improvements in logical inference tasks and mathematical problems.",
    "Related Patterns": "SelfRefine, TreeofThought (ToT), LeasttoMost Prompting.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Logical inference, mathematical problem-solving, iterative reasoning.",
    "Thinking": "This pattern involves iterative generation and evaluation of reasoning steps, a form of planning and self-correction, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Meta Prompting",
    "Problem": "Automatically generating or improving prompts and prompt templates.",
    "Context": "When manual prompt engineering is time-consuming, or there's a need to automate prompt creation/refinement.",
    "Solution": "Prompt an LLM to generate or improve a prompt or prompt template. This can be done with a simple template or more complex uses with multiple iterations and scoring mechanisms.",
    "Result": "Automates prompt engineering, leading to potentially better prompts.",
    "Related Patterns": "Automatic Prompt Engineer (APE), Gradient-free Instructional Prompt Search (GrIPS), Prompt Optimization with Textual Gradients (ProTeGi).",
    "Category": "Generative AI",
    "Uses": "Automated prompt generation, prompt optimization.",
    "Thinking": "This pattern uses an LLM to generate or improve other prompts, a meta-level application of generative AI to prompt design."
  },
  {
    "Pattern Name": "Automatic Prompt Engineer (APE)",
    "Problem": "Automatically generating and optimizing ZeroShot instruction prompts.",
    "Context": "When a set of exemplars is available, and an optimal ZeroShot instruction prompt needs to be found iteratively.",
    "Solution": "Use a set of exemplars to generate a ZeroShot instruction prompt. Generate multiple possible prompts, score them, then create variations of the best ones (e.g., by using prompt paraphrasing). Iterate on this process until some desiderata are reached.",
    "Result": "Automatically generates and refines effective instruction prompts.",
    "Related Patterns": "Meta Prompting, Gradient-free Instructional Prompt Search (GrIPS), Prompt Paraphrasing.",
    "Category": "MLOps",
    "Uses": "Automated prompt optimization, prompt discovery.",
    "Thinking": "This is an automated, iterative process for prompt optimization, fitting MLOps due to its focus on automated improvement of ML workflows (prompting)."
  },
  {
    "Pattern Name": "Gradient-free Instructional Prompt Search (GrIPS)",
    "Problem": "Automatically optimizing a starting prompt using a complex set of text operations.",
    "Context": "When a starting prompt exists, and a gradient-free method is preferred for optimization.",
    "Solution": "Similar to APE, but uses a more complex set of operations including deletion, addition, swapping, and paraphrasing to create variations of a starting prompt.",
    "Result": "Automatically optimizes prompts without gradient-based updates.",
    "Related Patterns": "Automatic Prompt Engineer (APE), Prompt Optimization with Textual Gradients (ProTeGi).",
    "Category": "MLOps",
    "Uses": "Automated prompt optimization, prompt discovery.",
    "Thinking": "Similar to APE, this is an automated, iterative process for prompt optimization, fitting MLOps."
  },
  {
    "Pattern Name": "Prompt Optimization with Textual Gradients (ProTeGi)",
    "Problem": "Improving a prompt template through a multi-step process involving criticism and selection.",
    "Context": "When a prompt template needs iterative improvement based on LLM-generated criticism.",
    "Solution": "First, pass a batch of inputs through the template. Then, pass the output, ground truth, and prompt into another prompt that criticizes the original prompt. Generate new prompts from these criticisms, then use a bandit algorithm to select one.",
    "Result": "Demonstrates improvements over methods like APE and GrIPS by leveraging LLM criticism and bandit algorithms.",
    "Related Patterns": "Automatic Prompt Engineer (APE), Gradient-free Instructional Prompt Search (GrIPS), SelfRefine.",
    "Category": "MLOps",
    "Uses": "Automated prompt optimization, prompt refinement.",
    "Thinking": "This is an automated, iterative process for prompt optimization using LLM criticism and a bandit algorithm, fitting MLOps."
  },
  {
    "Pattern Name": "Verbalizer",
    "Problem": "Mapping token spans or other outputs to labels and vice-versa in labeling tasks.",
    "Context": "When an LLM's output needs to be consistently interpreted as a specific label (e.g., 'positive' or 'negative').",
    "Solution": "Define a verbalizer that maps a token span or other type of output to a label and vice-versa (injectively). For example, mapping 'Yes' or 'No' tokens to appropriate labels.",
    "Result": "Ensures consistent interpretation of LLM outputs for labeling tasks.",
    "Related Patterns": "Answer Extractor, Answer Shape.",
    "Category": "Prompt Design",
    "Uses": "Classification tasks, consistent output parsing.",
    "Thinking": "This is a specific technique for structuring and interpreting LLM outputs for classification, making it a Prompt Design pattern."
  },
  {
    "Pattern Name": "Separate LLM Extractor",
    "Problem": "Extracting answers from complicated LLM outputs that cannot be consistently parsed by simple rules like regex.",
    "Context": "When LLM outputs are complex, freeform, or contain reasoning steps alongside the answer, making direct extraction difficult.",
    "Solution": "Use a separate LLM to evaluate the output and extract the desired answer. This LLM may use an 'answer trigger' (e.g., 'The answer Yes or No is') to guide its extraction.",
    "Result": "Enables robust extraction of answers from complex or varied LLM outputs.",
    "Related Patterns": "Answer Extractor, Verbalizer.",
    "Category": "Tools Integration",
    "Uses": "Robust answer extraction, parsing complex LLM responses.",
    "Thinking": "This pattern uses an LLM as a tool to process another LLM's output, fitting Tools Integration and LLM-specific."
  },
  {
    "Pattern Name": "Translate First Prompting",
    "Problem": "Improving output quality of GenAIs in non-English languages, especially low-resource languages, due to English-centric training.",
    "Context": "When interacting with GenAIs in languages other than English, where performance disparities exist.",
    "Solution": "First translate non-English input examples into English using an external MT system, multilingual LMs, or LLMs. The model then processes the English input.",
    "Result": "The model can utilize its strengths in English to better understand the content, improving performance in non-English settings.",
    "Related Patterns": "Multilingual ChainofThought, Multilingual InContext Learning.",
    "Category": "Tools Integration",
    "Uses": "Machine translation, cross-lingual task performance.",
    "Thinking": "This pattern integrates an external translation tool (or another LLM for translation) to preprocess input, fitting Tools Integration."
  },
  {
    "Pattern Name": "XLT Cross-Lingual Thought Prompting",
    "Problem": "Extending ChainofThought (CoT) prompting to multilingual settings.",
    "Context": "When applying CoT reasoning to tasks in multiple languages.",
    "Solution": "Utilize a prompt template composed of six separate instructions, including role assignment, cross-lingual thinking, and CoT.",
    "Result": "Extends the benefits of CoT to multilingual contexts.",
    "Related Patterns": "ChainofThought (CoT) Prompting, Cross-Lingual Self Consistent Prompting (CLSP).",
    "Category": "Knowledge & Reasoning",
    "Uses": "Multilingual reasoning tasks, cross-lingual problem-solving.",
    "Thinking": "This is a specific CoT variant adapted for multilingual reasoning, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Cross-Lingual Self Consistent Prompting (CLSP)",
    "Problem": "Improving multilingual CoT reasoning through ensembling across languages.",
    "Context": "When seeking to enhance the robustness and accuracy of CoT reasoning in multilingual settings.",
    "Solution": "Introduce an ensemble technique that constructs reasoning paths in different languages to answer the same question.",
    "Result": "Enhances multilingual CoT performance through cross-lingual ensembling.",
    "Related Patterns": "SelfConsistency, XLT Cross-Lingual Thought Prompting.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Multilingual reasoning, robust cross-lingual problem-solving.",
    "Thinking": "This pattern combines SelfConsistency with multilingual reasoning, fitting Knowledge & Reasoning and ensembling."
  },
  {
    "Pattern Name": "XInSTA Prompting",
    "Problem": "Aligning in-context examples with input sentences for multilingual classification tasks.",
    "Context": "When using InContext Learning in multilingual classification, and effective example alignment is crucial.",
    "Solution": "Explores three distinct approaches for aligning in-context examples: semantically similar examples, examples sharing the same label (task-based alignment), and a combination of both.",
    "Result": "Improves multilingual InContext Learning performance by optimizing example alignment.",
    "Related Patterns": "InContext Learning, InCLT Crosslingual Transfer Prompting.",
    "Category": "Prompt Design",
    "Uses": "Multilingual classification, cross-lingual InContext Learning.",
    "Thinking": "This pattern focuses on the strategic selection and alignment of examples for multilingual ICL, making it a Prompt Design pattern."
  },
  {
    "Pattern Name": "InCLT Crosslingual Transfer Prompting",
    "Problem": "Boosting cross-lingual cognitive capabilities of multilingual LLMs.",
    "Context": "When performing cross-lingual tasks and needing to leverage both source and target languages for InContext Learning.",
    "Solution": "Leverage both the source and target languages to create in-context examples, diverging from the traditional method of using only source language exemplars.",
    "Result": "Helps stimulate the cross-lingual cognitive capabilities of multilingual LLMs, boosting performance on cross-lingual tasks.",
    "Related Patterns": "InContext Learning, XInSTA Prompting.",
    "Category": "Prompt Design",
    "Uses": "Cross-lingual tasks, multilingual InContext Learning.",
    "Thinking": "This pattern specifically designs ICL examples to facilitate cross-lingual transfer, fitting Prompt Design."
  },
  {
    "Pattern Name": "PARC Prompts Augmented by Retrieval Cross-lingually",
    "Problem": "Enhancing cross-lingual transfer performance, particularly for low-resource target languages.",
    "Context": "When working with low-resource languages and needing to leverage high-resource language data for InContext Learning.",
    "Solution": "Retrieve relevant exemplars from a high-resource language and insert them into the prompt.",
    "Result": "Enhances cross-lingual transfer performance, particularly for low-resource target languages.",
    "Related Patterns": "Retrieval Augmented Generation (RAG), InContext Learning.",
    "Category": "Tools Integration",
    "Uses": "Cross-lingual transfer, low-resource language tasks.",
    "Thinking": "This pattern uses retrieval from an external source (high-resource language data) to augment prompts, fitting Tools Integration."
  },
  {
    "Pattern Name": "Multi-Aspect Prompting and Selection (MAPS)",
    "Problem": "Achieving high-quality machine translation by mimicking human translation processes.",
    "Context": "When machine translation requires multiple preparatory steps and robust selection of translations.",
    "Solution": "Starts with knowledge mining from the source sentence (extracting keywords, topics), generating translation exemplars, integrating this knowledge to generate multiple possible translations, then selecting the best one.",
    "Result": "Mimics human translation processes to ensure high-quality output.",
    "Related Patterns": "Decomposed Prompting for MT (DecoMT), Ensembling.",
    "Category": "Planning",
    "Uses": "High-quality machine translation, complex translation tasks.",
    "Thinking": "This pattern describes a multi-step planning process for translation, fitting Planning and Knowledge & Reasoning."
  },
  {
    "Pattern Name": "ChainofDictionary (CoD)",
    "Problem": "Improving machine translation by providing explicit dictionary definitions of words.",
    "Context": "When translating phrases where specific word meanings in multiple languages are crucial.",
    "Solution": "First, extract words from the source phrase. Then, make a list of their meanings in multiple languages (automatically via retrieval from a dictionary). Prepend these dictionary phrases to the prompt, asking the GenAI to use them during translation.",
    "Result": "Improves translation accuracy by providing explicit lexical context.",
    "Related Patterns": "Dictionary-based Prompting for Machine Translation (DiPMT), Translate First Prompting.",
    "Category": "Tools Integration",
    "Uses": "Machine translation, handling polysemy in translation.",
    "Thinking": "This pattern integrates an external dictionary tool to augment prompts for translation, fitting Tools Integration."
  },
  {
    "Pattern Name": "Dictionary-based Prompting for Machine Translation (DiPMT)",
    "Problem": "Improving machine translation by providing definitions in source and target languages.",
    "Context": "Similar to CoD, when explicit lexical definitions are beneficial for translation.",
    "Solution": "Similar to CoD, but only provides definitions in the source and target languages and formats them slightly differently.",
    "Result": "Improves translation accuracy by providing explicit lexical context.",
    "Related Patterns": "ChainofDictionary (CoD), Translate First Prompting.",
    "Category": "Tools Integration",
    "Uses": "Machine translation, handling polysemy in translation.",
    "Thinking": "A variation of CoD, also integrating an external dictionary tool, fitting Tools Integration."
  },
  {
    "Pattern Name": "Decomposed Prompting for MT (DecoMT)",
    "Problem": "Translating long source texts accurately by handling them in chunks.",
    "Context": "When translating long source texts where context management and consistency are challenging.",
    "Solution": "Divide the source text into several chunks and translate them independently using fewshot prompting. Then, use these translations and contextual information between chunks to generate a final translation.",
    "Result": "Improves translation of long texts by breaking them into manageable parts and maintaining context.",
    "Related Patterns": "LeasttoMost Prompting, Multi-Aspect Prompting and Selection (MAPS).",
    "Category": "Planning",
    "Uses": "Machine translation of long documents, maintaining consistency across translated segments.",
    "Thinking": "This pattern describes a planning strategy (decomposition) for machine translation, fitting Planning."
  },
  {
    "Pattern Name": "InteractiveChainPrompting (ICP)",
    "Problem": "Dealing with potential ambiguities in translation by involving human clarification.",
    "Context": "When a phrase to be translated contains ambiguities that the GenAI might misinterpret.",
    "Solution": "First, ask the GenAI to generate sub-questions about any ambiguities in the phrase to be translated. Humans later respond to these questions, and the system includes this information to generate a final translation.",
    "Result": "Resolves translation ambiguities through human-in-the-loop interaction, leading to more accurate translations.",
    "Related Patterns": "Question Clarification, Iterative Prompting (for MT).",
    "Category": "AI\u2013Human Interaction",
    "Uses": "High-quality machine translation, ambiguity resolution.",
    "Thinking": "This pattern explicitly involves human interaction to resolve AI's uncertainty, making it an AI-Human Interaction pattern."
  },
  {
    "Pattern Name": "Iterative Prompting (for MT)",
    "Problem": "Refining draft translations by integrating human feedback or automated retrieval signals.",
    "Context": "When an initial draft translation needs further improvement and quality assurance.",
    "Solution": "Prompt LLMs to create a draft translation. This initial version is further refined by integrating supervision signals obtained from either automated retrieval systems or direct human feedback.",
    "Result": "Produces more refined and accurate translations through iterative human or automated supervision.",
    "Related Patterns": "InteractiveChainPrompting (ICP), SelfRefine.",
    "Category": "AI\u2013Human Interaction",
    "Uses": "High-quality machine translation, post-editing assistance.",
    "Thinking": "Similar to ICP, this pattern involves iterative human (or automated) feedback to refine AI output, fitting AI-Human Interaction."
  },
  {
    "Pattern Name": "Negative Prompting",
    "Problem": "Preventing the generation of undesired elements in multimodal outputs (e.g., images).",
    "Context": "When generating images or other multimodal content, and specific undesirable features might appear.",
    "Solution": "Numerically weight certain terms in the prompt so that the model considers them less heavily than others. For example, negatively weighting 'bad hands' to avoid anatomically inaccurate hands.",
    "Result": "Models are more likely to generate outputs without the negatively weighted elements.",
    "Related Patterns": "Prompt Modifiers, Style Prompting.",
    "Category": "Generative AI",
    "Uses": "Image generation, multimodal content creation, fine-tuning generative outputs.",
    "Thinking": "This is a specific technique for controlling generative AI output by specifying what *not* to generate, fitting Generative AI and Prompt Design."
  },
  {
    "Pattern Name": "PairedImage Prompting",
    "Problem": "Demonstrating image transformations to a model for it to perform similar conversions on new images.",
    "Context": "When an image transformation task needs to be learned by example, similar to few-shot learning for text.",
    "Solution": "Show the model two images: one before and one after some transformation. Then, present the model with a new image for which it will perform the demonstrated conversion. This can be done with or without textual instructions.",
    "Result": "Enables the model to learn and apply image transformations from examples.",
    "Related Patterns": "Multimodal InContext Learning, ImageasText Prompting.",
    "Category": "Generative AI",
    "Uses": "Image editing, style transfer, visual transformations.",
    "Thinking": "This pattern applies the concept of InContext Learning to image modality, making it a Generative AI pattern for prompt design."
  },
  {
    "Pattern Name": "ImageasText Prompting",
    "Problem": "Including images or multiple images easily within a text-based prompt.",
    "Context": "When multimodal prompts are needed, but the primary interaction is text-based.",
    "Solution": "Generate a textual description of an image, which then allows for the easy inclusion of the image (or multiple images) in a text-based prompt.",
    "Result": "Facilitates multimodal prompting by converting visual information into a text-compatible format.",
    "Related Patterns": "Multimodal InContext Learning, PairedImage Prompting.",
    "Category": "Generative AI",
    "Uses": "Multimodal reasoning, image captioning, integrating visual context into text prompts.",
    "Thinking": "This pattern uses generative AI (image captioning) to bridge modalities for prompt design, fitting Generative AI."
  },
  {
    "Pattern Name": "Duty Distinct ChainofThought (DDCoT)",
    "Problem": "Extending LeasttoMost prompting to the multimodal setting for complex reasoning.",
    "Context": "When multimodal problems require decomposition and sequential solving, similar to LeasttoMost for text.",
    "Solution": "Extends LeasttoMost prompting to the multimodal setting, creating sub-questions, then solving them, and combining the answers into a final response.",
    "Result": "Enables structured, decomposed reasoning for multimodal problems.",
    "Related Patterns": "LeasttoMost Prompting, Multimodal ChainofThought.",
    "Category": "Planning",
    "Uses": "Multimodal reasoning, complex visual-linguistic problem-solving.",
    "Thinking": "This pattern applies a planning strategy (decomposition) to multimodal reasoning, fitting Planning and Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Multimodal GraphofThought",
    "Problem": "Extending GraphofThought reasoning to the multimodal setting.",
    "Context": "When multimodal inputs (e.g., image + question) require complex, graph-based reasoning.",
    "Solution": "Extends GraphofThought to the multimodal setting. At inference time, the input prompt is used to construct a thought graph, which is then used along with the original prompt to generate a rationale to answer the question. When an image is input, an image captioning model generates a textual description, appended to the prompt for visual context.",
    "Result": "Enables graph-based reasoning for multimodal problems by integrating visual context.",
    "Related Patterns": "TreeofThought (ToT), ChainofThought (CoT) Prompting, ImageasText Prompting.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Multimodal reasoning, complex visual-linguistic problem-solving.",
    "Thinking": "This pattern extends a sophisticated reasoning structure (GraphofThought) to multimodal inputs, fitting Knowledge & Reasoning."
  },
  {
    "Pattern Name": "ChainofImages (CoI)",
    "Problem": "Generating images as part of a thought process for visual reasoning.",
    "Context": "When visual reasoning tasks can benefit from intermediate visual steps or 'thoughts'.",
    "Solution": "A multimodal extension of ChainofThought prompting that generates images (e.g., SVGs) as part of its thought process, using prompts like 'Let's think image by image'.",
    "Result": "Enables models to reason visually by generating and using intermediate images.",
    "Related Patterns": "ChainofThought (CoT) Prompting, Multimodal ChainofThought.",
    "Category": "Generative AI",
    "Uses": "Visual reasoning, creative image generation, visual problem-solving.",
    "Thinking": "This pattern uses generative AI (image generation) as a core part of the reasoning process, making it a Generative AI pattern for Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Segmentation Prompting",
    "Problem": "Performing segmentation tasks (e.g., semantic segmentation) using prompts.",
    "Context": "When a model needs to identify and delineate specific objects or regions within an image.",
    "Solution": "Use prompts to guide the model in performing segmentation tasks.",
    "Result": "Enables flexible and prompt-driven segmentation.",
    "Related Patterns": "Image Prompting.",
    "Category": "Classical AI",
    "Uses": "Image segmentation, computer vision tasks.",
    "Thinking": "This pattern applies prompting to a classical computer vision task (segmentation), making it a Classical AI pattern."
  },
  {
    "Pattern Name": "3D Prompting",
    "Problem": "Generating or manipulating 3D content using prompts.",
    "Context": "When creating 3D objects, textures, or animating 3D scenes.",
    "Solution": "Use prompts (text, image, user annotation, bounding boxes, points, lines, 3D objects) as input to guide 3D object synthesis, 3D surface texturing, and 4D scene generation.",
    "Result": "Enables prompt-driven creation and manipulation of 3D content.",
    "Related Patterns": "Multimodal Prompting, Image Prompting.",
    "Category": "Generative AI",
    "Uses": "3D content creation, virtual reality, game development.",
    "Thinking": "This pattern applies prompting to 3D generative tasks, fitting Generative AI."
  },
  {
    "Pattern Name": "Modular Reasoning, Knowledge and Language (MRKL) System",
    "Problem": "Allowing LLMs to make use of external systems for tasks like mathematical computations, reasoning, and factuality.",
    "Context": "When LLMs have shortcomings in specific areas and can benefit from external tools.",
    "Solution": "Contains an LLM router providing access to multiple tools (e.g., calculator, weather API). The router makes multiple calls to get information and combines it to generate a final response.",
    "Result": "Extends LLM capabilities by integrating external tools, improving accuracy and factuality.",
    "Related Patterns": "Tool Use Agents, Program-aided Language Model (PAL).",
    "Category": "Agentic AI",
    "Uses": "Complex problem-solving, factual question answering, real-world interaction.",
    "Thinking": "This is a foundational agentic pattern for tool use, explicitly defining an architecture for AI agents."
  },
  {
    "Pattern Name": "Self-Correcting with Tool-Interactive Critiquing (CRITIC)",
    "Problem": "Verifying and amending LLM responses for possible errors using external tools.",
    "Context": "When an LLM generates a response, and its accuracy needs to be verified or improved through external information.",
    "Solution": "First, generate a response to the prompt with no external calls. Then, the same LLM criticizes this response for possible errors. Finally, it uses tools (e.g., Internet search or a code interpreter) accordingly to verify or amend parts of the response.",
    "Result": "Improves response accuracy and factuality by leveraging self-criticism and tool interaction.",
    "Related Patterns": "SelfRefine, MRKL System, Retrieval Augmented Generation (RAG).",
    "Category": "Agentic AI",
    "Uses": "Fact-checking, code debugging, improving response quality.",
    "Thinking": "This pattern combines agentic behavior (tool use) with self-criticism, fitting Agentic AI and Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Program-aided Language Model (PAL)",
    "Problem": "Solving problems that can be translated directly into executable code.",
    "Context": "When a problem involves mathematical or logical operations that are best handled by a code interpreter.",
    "Solution": "Translates a problem directly into programming code, which is then sent to a Python interpreter to generate an answer.",
    "Result": "Generates accurate answers for mathematical and programming-related problems by offloading computation to a code interpreter.",
    "Related Patterns": "ProgramofThoughts, Tool-Integrated Reasoning Agent (ToRA), MRKL System.",
    "Category": "Agentic AI",
    "Uses": "Mathematical problem-solving, code generation, algorithmic tasks.",
    "Thinking": "This pattern describes an agent that generates and executes code, a clear Agentic AI pattern for tool use."
  },
  {
    "Pattern Name": "Tool-Integrated Reasoning Agent (ToRA)",
    "Problem": "Solving complex problems that require interleaving code generation and reasoning steps.",
    "Context": "When a problem cannot be solved by a single code generation step but requires a dynamic interplay between reasoning and code execution.",
    "Solution": "Similar to PAL, but instead of a single code generation step, it interleaves code and reasoning steps for as long as necessary to solve the problem.",
    "Result": "Solves complex mathematical and programming problems by dynamically combining reasoning and tool use.",
    "Related Patterns": "Program-aided Language Model (PAL), Reasoning and Acting (ReAct).",
    "Category": "Agentic AI",
    "Uses": "Complex mathematical problem-solving, dynamic code generation and execution.",
    "Thinking": "This pattern extends PAL by interleaving reasoning and tool use, making it a more sophisticated Agentic AI pattern."
  },
  {
    "Pattern Name": "TaskWeaver",
    "Problem": "Transforming user requests into code and leveraging user-defined plugins.",
    "Context": "When user requests need to be fulfilled by executing code, potentially with custom functionalities.",
    "Solution": "Similar to PAL, it transforms user requests into code but can also make use of user-defined plugins.",
    "Result": "Provides flexible code-based task execution, extensible with custom plugins.",
    "Related Patterns": "Program-aided Language Model (PAL), Tool Use Agents.",
    "Category": "Agentic AI",
    "Uses": "Automating tasks, integrating custom functionalities, code generation.",
    "Thinking": "This pattern describes an agent that generates code and uses plugins, fitting Agentic AI and Tools Integration."
  },
  {
    "Pattern Name": "Reasoning and Acting (ReAct)",
    "Problem": "Solving problems by interacting with environments and maintaining a memory of past thoughts, actions, and observations.",
    "Context": "When an agent needs to operate in a dynamic environment, requiring sequential decision-making and memory.",
    "Solution": "Generates a thought, takes an action, and receives an observation, repeating this process. All this information is inserted into the prompt, providing a memory of past thoughts, actions, and observations.",
    "Result": "Enables agents to solve problems in interactive environments by synergizing reasoning and acting.",
    "Related Patterns": "Reflexion, Voyager, Ghost in the Minecraft (GITM).",
    "Category": "Agentic AI",
    "Uses": "Interactive problem-solving, environment interaction, sequential decision-making.",
    "Thinking": "This is a core agentic pattern that combines reasoning, action, and memory for environment interaction, fitting Agentic AI and Planning."
  },
  {
    "Pattern Name": "Reflexion",
    "Problem": "Improving agent performance by adding introspection and learning from past successes/failures.",
    "Context": "When an agent needs to learn from its experiences and adapt its behavior over time.",
    "Solution": "Builds on ReAct by adding a layer of introspection. It obtains a trajectory of actions and observations, then is given an evaluation of success/failure. It generates a reflection on what it did and what went wrong. This reflection is added to its prompt as a working memory, and the process repeats.",
    "Result": "Enables agents to learn and improve from experience, leading to more robust performance.",
    "Related Patterns": "Reasoning and Acting (ReAct), SelfRefine, Voyager.",
    "Category": "Agentic AI",
    "Uses": "Lifelong learning, self-improvement, complex task mastery in dynamic environments.",
    "Thinking": "This pattern adds a meta-learning/self-reflection layer to agent behavior, fitting Agentic AI and Knowledge & Reasoning."
  },
  {
    "Pattern Name": "Voyager",
    "Problem": "Enabling agents to acquire new skills and explore open-world environments through self-directed learning.",
    "Context": "When an agent needs to operate in complex, open-ended environments (e.g., Minecraft) and continuously learn new skills.",
    "Solution": "Composed of three parts: 1. Proposes tasks for itself to complete to learn more about the world. 2. Generates code to execute these actions. 3. Saves these actions to be retrieved later as part of a long-term memory system.",
    "Result": "Agents can acquire new skills and navigate open-world environments, applicable to real-world tasks requiring lifelong learning.",
    "Related Patterns": "Reasoning and Acting (ReAct), Reflexion, Ghost in the Minecraft (GITM).",
    "Category": "Agentic AI",
    "Uses": "Lifelong learning, open-world exploration, skill acquisition.",
    "Thinking": "This pattern describes a sophisticated lifelong learning agent, fitting Agentic AI and Planning."
  },
  {
    "Pattern Name": "Ghost in the Minecraft (GITM)",
    "Problem": "Enabling agents to achieve arbitrary goals in open-world environments by recursive subgoal decomposition and iterative planning.",
    "Context": "When an agent needs to operate in complex, open-world environments (e.g., Minecraft) and achieve high-level goals.",
    "Solution": "Starts with an arbitrary goal, breaks it down into subgoals recursively, then iteratively plans and executes actions by producing structured text (rather than writing code). Uses an external knowledge base and a memory of past experience.",
    "Result": "Generates capable agents for open-world environments, achieving complex goals through structured planning.",
    "Related Patterns": "Voyager, LeasttoMost Prompting, TreeofThought (ToT).",
    "Category": "Agentic AI",
    "Uses": "Open-world task completion, complex goal achievement, hierarchical planning.",
    "Thinking": "This pattern describes a hierarchical planning agent for open-world environments, fitting Agentic AI and Planning."
  },
  {
    "Pattern Name": "Retrieval Augmented Generation (RAG)",
    "Problem": "Enhancing performance in knowledge-intensive tasks by providing external, up-to-date information to LLMs.",
    "Context": "When LLMs need access to external, factual, or domain-specific knowledge beyond their training data to avoid hallucinations or provide current information.",
    "Solution": "Retrieve information from an external source (e.g., a database, web search) and insert it into the prompt.",
    "Result": "Enhances performance in knowledge-intensive tasks, improving factuality and reducing hallucinations.",
    "Related Patterns": "MRKL System, VerifyandEdit, DemonstrateSearchPredict (DSP).",
    "Category": "Tools Integration",
    "Uses": "Factual question answering, knowledge-intensive text generation, reducing hallucinations.",
    "Thinking": "This is a widely recognized pattern for integrating external knowledge retrieval with LLMs, fitting Tools Integration and Knowledge & Reasoning."
  },
  {
    "Pattern Name": "VerifyandEdit",
    "Problem": "Improving self-consistency in ChainofThought by editing reasoning paths with retrieved external information.",
    "Context": "When multiple CoT reasoning paths are generated, and their accuracy can be improved by external knowledge.",
    "Solution": "Generates multiple chains of thought, then selects some to be edited. This is done by retrieving relevant external information to the CoTs and allowing the LLM to augment them accordingly.",
    "Result": "Improves the quality and accuracy of reasoning paths by incorporating external verification and editing.",
    "Related Patterns": "SelfConsistency, Retrieval Augmented Generation (RAG), SelfRefine.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Improving reasoning accuracy, fact-checking, reducing errors in CoT.",
    "Thinking": "This pattern combines RAG with self-correction and reasoning path editing, fitting Knowledge & Reasoning and Tools Integration."
  },
  {
    "Pattern Name": "DemonstrateSearchPredict (DSP)",
    "Problem": "Solving complex questions by decomposing them, searching for answers, and combining responses.",
    "Context": "When a question requires multiple steps of information retrieval and synthesis.",
    "Solution": "First decomposes a question into sub-questions. Then, uses queries to solve them and combine their responses in a final answer. Uses fewshot prompting to decompose the problem and combine responses.",
    "Result": "Effectively solves knowledge-intensive, multi-step questions.",
    "Related Patterns": "LeasttoMost Prompting, Retrieval Augmented Generation (RAG), Interleaved Retrieval guided by ChainofThought (IRCoT).",
    "Category": "Planning",
    "Uses": "Multi-hop question answering, complex information synthesis.",
    "Thinking": "This pattern describes a planning strategy (decompose, search, predict) that integrates retrieval, fitting Planning and Tools Integration."
  },
  {
    "Pattern Name": "Interleaved Retrieval guided by ChainofThought (IRCoT)",
    "Problem": "Improving multi-hop question answering by dynamically guiding retrieval with CoT and vice-versa.",
    "Context": "When multi-hop questions require both reasoning and retrieval, and their interplay is crucial.",
    "Solution": "Interleaves CoT and retrieval. IRCoT leverages CoT to guide which documents to retrieve and retrieval to help plan the reasoning steps of CoT.",
    "Result": "Enhances performance in multi-hop question answering by creating a synergistic relationship between reasoning and retrieval.",
    "Related Patterns": "ChainofThought (CoT) Prompting, Retrieval Augmented Generation (RAG), DemonstrateSearchPredict (DSP).",
    "Category": "Knowledge & Reasoning",
    "Uses": "Multi-hop question answering, complex information retrieval and synthesis.",
    "Thinking": "This pattern describes a dynamic interaction between reasoning and retrieval, fitting Knowledge & Reasoning and Tools Integration."
  },
  {
    "Pattern Name": "Iterative Retrieval Augmentation",
    "Problem": "Improving long-form generation by performing retrieval multiple times during the generation process.",
    "Context": "When generating long-form text that requires continuous access to external knowledge.",
    "Solution": "Perform retrieval multiple times during long-form generation, typically in a three-step process: 1. Generate a temporary sentence (content plan). 2. Retrieve external knowledge using the temporary sentence as a query. 3. Inject the retrieved knowledge into the temporary sentence to create the next output sentence.",
    "Result": "Improves the quality and factual grounding of long-form generated text.",
    "Related Patterns": "Retrieval Augmented Generation (RAG), DemonstrateSearchPredict (DSP).",
    "Category": "Generative AI",
    "Uses": "Long-form text generation, document summarization, creative writing with factual grounding.",
    "Thinking": "This pattern applies RAG iteratively within a generative process, fitting Generative AI and Tools Integration."
  },
  {
    "Pattern Name": "Role-based Evaluation",
    "Problem": "Improving and diversifying LLM-based evaluations.",
    "Context": "When using LLMs as evaluators and needing to generate diverse or specific perspectives on text quality.",
    "Solution": "Create prompts with the same instructions for evaluation but different roles (e.g., 'act as a literary critic'). This can also be used in a multi-agent setting where LLMs debate the validity of text.",
    "Result": "Effectively generates diverse evaluations and can improve evaluation quality.",
    "Related Patterns": "ChatEval Framework, Model-Generated Guidelines.",
    "Category": "AI\u2013Human Interaction",
    "Uses": "LLM evaluation, quality assessment, multi-agent debate.",
    "Thinking": "This pattern uses role-playing to influence the AI's evaluative perspective, fitting AI-Human Interaction and MLOps (for evaluation)."
  },
  {
    "Pattern Name": "Model-Generated Guidelines (for evaluation)",
    "Problem": "Reducing the 'insufficient prompting problem' in LLM-based evaluation arising from ill-defined scoring guidelines.",
    "Context": "When human-defined scoring guidelines are vague or inconsistent, leading to unreliable LLM evaluations.",
    "Solution": "Prompt an LLM to generate guidelines for evaluation. This can involve generating a chain-of-thought of detailed evaluation steps or deriving scoring criteria based on expert human annotations.",
    "Result": "Reduces inconsistent and misaligned evaluations by providing clearer, model-generated scoring criteria.",
    "Related Patterns": "GEVAL Framework, ChainofThought (CoT) Prompting.",
    "Category": "Generative AI",
    "Uses": "Automated evaluation, improving consistency of LLM judgments.",
    "Thinking": "This pattern uses generative AI to create evaluation criteria, fitting Generative AI and MLOps (for evaluation)."
  },
  {
    "Pattern Name": "LLMEVAL Framework",
    "Problem": "Providing a simple framework for unified, multidimensional automatic evaluation of open-domain conversations.",
    "Context": "When evaluating LLM outputs, especially in conversational settings, across multiple criteria.",
    "Solution": "Uses a single prompt containing a schema of variables to evaluate (e.g., grammar, relevance), an instruction to output scores for each variable within a certain range, and the content to evaluate.",
    "Result": "Provides a unified and multidimensional automatic evaluation for open-domain conversations.",
    "Related Patterns": "GEVAL Framework, ChatEval Framework.",
    "Category": "MLOps",
    "Uses": "Automated evaluation of LLM outputs, conversational AI assessment.",
    "Thinking": "This is a specific framework for LLM evaluation, fitting MLOps due to its role in ML workflow assessment."
  },
  {
    "Pattern Name": "GEVAL Framework",
    "Problem": "Improving LLM-based evaluation by incorporating AutoCoT steps.",
    "Context": "When LLM-based evaluation needs more robust reasoning and justification for its scores.",
    "Solution": "Similar to LLMEVAL, but includes AutoCoT steps in the prompt itself. These steps are generated according to the evaluation instructions and inserted into the final prompt, weighting answers according to token probabilities.",
    "Result": "Enhances evaluation performance by providing detailed, automatically generated reasoning for quality assessments.",
    "Related Patterns": "LLMEVAL Framework, Automatic ChainofThought (AutoCoT) Prompting.",
    "Category": "MLOps",
    "Uses": "Automated evaluation with reasoning, improving transparency of LLM judgments.",
    "Thinking": "An extension of LLMEVAL incorporating AutoCoT for better evaluation, fitting MLOps."
  },
  {
    "Pattern Name": "ChatEval Framework",
    "Problem": "Improving LLM-based evaluation through a multi-agent debate framework.",
    "Context": "When a more robust and nuanced evaluation is needed, simulating human-like debate and diverse perspectives.",
    "Solution": "Uses a multi-agent debate framework, with each agent having a separate role (e.g., different evaluative personas).",
    "Result": "Provides more comprehensive and robust evaluations by simulating a debate among different AI personas.",
    "Related Patterns": "Role-based Evaluation, LLMEVAL Framework.",
    "Category": "Agentic AI",
    "Uses": "Advanced LLM evaluation, simulating human debate, multi-perspective assessment.",
    "Thinking": "This framework uses multiple AI agents interacting to perform evaluation, fitting Agentic AI and MLOps."
  },
  {
    "Pattern Name": "Verbalized Score (for confidence calibration)",
    "Problem": "Eliciting confidence scores from LLMs to gauge their certainty.",
    "Context": "When needing to understand the LLM's confidence in its answer, especially when overconfidence is a concern.",
    "Solution": "A simple calibration technique that generates a confidence score (e.g., 'How confident are you from 1 to 10?') directly in the prompt.",
    "Result": "Aims to provide a score representing the model's confidence, though its efficacy is debated.",
    "Related Patterns": "SelfCalibration.",
    "Category": "LLM-specific",
    "Uses": "Assessing LLM confidence, informing user reliance on model outputs.",
    "Thinking": "This pattern directly addresses an LLM-specific behavior (confidence) through prompt design, fitting LLM-specific."
  },
  {
    "Pattern Name": "Vanilla Prompting (for bias mitigation)",
    "Problem": "Reducing biases in LLM outputs.",
    "Context": "When LLMs might perpetuate biases or stereotypes in their responses.",
    "Solution": "Consists simply of an instruction in the prompt that tells the LLM to be unbiased (also referred to as moral self-correction).",
    "Result": "Aims to elicit less harmful and more fair outputs from LLMs.",
    "Related Patterns": "Selecting Balanced Demonstrations, Cultural Awareness.",
    "Category": "Prompt Design",
    "Uses": "Bias mitigation, ethical AI development, fair content generation.",
    "Thinking": "This is a direct prompt instruction to influence AI behavior regarding bias, fitting Prompt Design and LLM-specific."
  },
  {
    "Pattern Name": "Selecting Balanced Demonstrations (for bias mitigation)",
    "Problem": "Reducing biases in LLM outputs, particularly in FewShot settings.",
    "Context": "When using FewShot Prompting, and the distribution of exemplars might introduce or amplify biases.",
    "Solution": "Select balanced demonstrations or obtain demonstrations optimized over fairness metrics.",
    "Result": "Can reduce biases in LLM outputs by providing a fair representation in exemplars.",
    "Related Patterns": "FewShot Prompting, Vanilla Prompting.",
    "Category": "Prompt Design",
    "Uses": "Bias mitigation in FewShot learning, fair content generation.",
    "Thinking": "This pattern addresses bias through careful selection of prompt examples, fitting Prompt Design and LLM-specific."
  },
  {
    "Pattern Name": "Cultural Awareness (for cultural adaptation)",
    "Problem": "Helping LLMs with cultural adaptation in their outputs.",
    "Context": "When LLM outputs need to be culturally relevant or sensitive for specific audiences.",
    "Solution": "Inject cultural awareness into prompts. This can be done by creating several prompts with machine translation, including 1. asking the LLM to refine its own output and 2. instructing the LLM to use culturally relevant words.",
    "Result": "Enables LLMs to produce culturally adapted and appropriate outputs.",
    "Related Patterns": "Multilingual Prompting, Role Prompting.",
    "Category": "Personalization",
    "Uses": "Culturally sensitive content generation, localized communication.",
    "Thinking": "This pattern focuses on tailoring AI output to specific cultural contexts, making it a Personalization pattern."
  },
  {
    "Pattern Name": "AttrPrompt",
    "Problem": "Avoiding text biased towards certain attributes when generating synthetic data.",
    "Context": "When generating synthetic data, and traditional approaches might produce data biased towards specific lengths, locations, or styles.",
    "Solution": "1. Ask the LLM to generate specific attributes that are important to alter for diversity (e.g., location). 2. Prompt the LLM to generate synthetic data by varying each of these attributes.",
    "Result": "Generates synthetic data with varied attributes, avoiding bias towards specific characteristics.",
    "Related Patterns": "Generative AI, Prompt Design.",
    "Category": "Generative AI",
    "Uses": "Synthetic data generation, bias mitigation in data creation.",
    "Thinking": "This pattern uses prompt engineering to control the attributes of generated data, fitting Generative AI and Prompt Design."
  },
  {
    "Pattern Name": "Ambiguous Demonstrations",
    "Problem": "Increasing InContext Learning performance when dealing with ambiguous questions.",
    "Context": "When questions are ambiguous and can be interpreted in multiple ways, leading to varied answers.",
    "Solution": "Include examples that have an ambiguous label set in the prompt. This can be automated with a retriever or done manually.",
    "Result": "Can increase ICL performance for ambiguous questions.",
    "Related Patterns": "InContext Learning, Question Clarification.",
    "Category": "Prompt Design",
    "Uses": "Handling ambiguous inputs, improving ICL robustness.",
    "Thinking": "This pattern specifically designs prompt examples to address ambiguity, fitting Prompt Design and LLM-specific."
  },
  {
    "Pattern Name": "Question Clarification",
    "Problem": "Resolving ambiguity in questions by allowing the LLM to identify and ask clarifying questions.",
    "Context": "When a user's question is ambiguous, and the LLM needs more information to provide an accurate answer.",
    "Solution": "Allows the LLM to identify ambiguous questions and generate clarifying questions to pose to the user. Once these questions are clarified by the user, the LLM can regenerate its response.",
    "Result": "Resolves ambiguity, leading to more accurate and contextually appropriate answers.",
    "Related Patterns": "SelfAsk, InteractiveChainPrompting (ICP).",
    "Category": "AI\u2013Human Interaction",
    "Uses": "Ambiguity resolution, interactive problem-solving, improving user experience.",
    "Thinking": "This pattern involves the AI actively engaging with the user to resolve ambiguity, making it an AI-Human Interaction pattern."
  },
  {
    "Pattern Name": "Output Formatting",
    "Problem": "It is often desirable for the GenAI to output information in certain structured formats.",
    "Context": "When the downstream system or user expects the GenAI's response to adhere to a specific structure (e.g., CSV, Markdown, XML, custom formats).",
    "Solution": "Include instructions in the prompt that explicitly specify the desired output format.",
    "Result": "The GenAI generates information in the requested structured format, which can facilitate parsing and integration with other systems. May reduce performance on some tasks but can also improve it.",
    "Related Patterns": "Styling (for Evaluation).",
    "Category": "Prompt Design",
    "Uses": "Data extraction, API integration, structured content generation, facilitating automated parsing.",
    "Thinking": "Explicitly mentioned as a common component of a prompt, addressing a clear problem of structured output in AI systems."
  },
  {
    "Pattern Name": "Prompt Chain",
    "Problem": "Solving complex tasks that require multiple sequential steps, where the output of one step informs the next.",
    "Context": "When a task can be naturally broken down into a series of sub-tasks, and each sub-task can be handled by a separate prompt template.",
    "Solution": "Use two or more prompt templates in succession. The output of the prompt generated by the first template is used to parameterize the second template, and this continues until all templates are exhausted.",
    "Result": "Enables the GenAI to tackle complex, multi-step problems by breaking them into manageable, interconnected stages, improving overall task completion.",
    "Related Patterns": "LeasttoMost Prompting, Decomposition, Metacognitive Prompting, TreeofThought.",
    "Category": "Planning",
    "Uses": "Multi-step problem-solving, complex data processing pipelines, agentic workflows.",
    "Thinking": "Explicitly defined as a technique involving multiple prompts in succession, which is a clear planning/orchestration pattern for AI agents."
  },
  {
    "Pattern Name": "Prompt Modifiers",
    "Problem": "Changing the resultant image or other multimodal output in a specific way.",
    "Context": "When generating images, videos, or other multimodal content, and fine-grained control over specific attributes (e.g., medium, lighting) is desired.",
    "Solution": "Append specific words or phrases to a prompt to modify the output. Examples include 'Medium (e.g., on canvas)' or 'Lighting (e.g., a well lit scene)'.",
    "Result": "The generated multimodal output incorporates the specified modifications, allowing for more precise control over the creative process.",
    "Related Patterns": "Negative Prompting, Style Prompting.",
    "Category": "Generative AI",
    "Uses": "Image generation, video generation, multimodal content creation, artistic control.",
    "Thinking": "Explicitly described as a technique for controlling multimodal generative AI output, fitting Generative AI and Prompt Design."
  },
  {
    "Pattern Name": "Styling (for Evaluation)",
    "Problem": "Improving the accuracy of LLM-generated judgments in evaluation tasks.",
    "Context": "When using an LLM as an evaluator, and the clarity and consistency of its judgment output can be enhanced.",
    "Solution": "Format the LLM's evaluation response using structured formats like XML or JSON.",
    "Result": "Improves the accuracy of the judgment generated by the evaluator, making the evaluation more reliable and easier to parse.",
    "Related Patterns": "Output Formatting, LLMEVAL Framework.",
    "Category": "MLOps",
    "Uses": "Automated evaluation, quality assessment, structured feedback generation.",
    "Thinking": "This is a specific technique for improving LLM-based evaluation, which is an MLOps concern for ML workflow assessment."
  },
  {
    "Pattern Name": "Linear Scale (for Evaluation)",
    "Problem": "Obtaining a quantifiable, graded assessment from an LLM for evaluation tasks.",
    "Context": "When using an LLM as an evaluator and needing a numerical score to represent quality or performance.",
    "Solution": "Prompt the LLM to output a score on a linear scale (e.g., 1-5, 1-10, 0-1), which can be discrete or continuous.",
    "Result": "Provides a simple and interpretable numerical rating for the evaluated content.",
    "Related Patterns": "Likert Scale (for Evaluation), Binary Score (for Evaluation).",
    "Category": "MLOps",
    "Uses": "Automated evaluation, quality scoring, benchmarking.",
    "Thinking": "This is a specific output format for LLM-based evaluation, fitting MLOps for ML workflow assessment."
  },
  {
    "Pattern Name": "Binary Score (for Evaluation)",
    "Problem": "Obtaining a simple, categorical (yes/no, true/false) assessment from an LLM for evaluation tasks.",
    "Context": "When using an LLM as an evaluator for tasks requiring a straightforward pass/fail or presence/absence judgment.",
    "Solution": "Prompt the LLM to generate binary responses like 'Yes' or 'No', or 'True' or 'False'.",
    "Result": "Provides a clear, unambiguous binary judgment for the evaluated content.",
    "Related Patterns": "Linear Scale (for Evaluation), Likert Scale (for Evaluation).",
    "Category": "MLOps",
    "Uses": "Automated evaluation, content filtering, factual verification.",
    "Thinking": "This is a specific output format for LLM-based evaluation, fitting MLOps for ML workflow assessment."
  },
  {
    "Pattern Name": "Likert Scale (for Evaluation)",
    "Problem": "Obtaining a nuanced, ordinal assessment from an LLM for evaluation tasks, with predefined qualitative categories.",
    "Context": "When using an LLM as an evaluator and needing a graded assessment that aligns with human-interpretable qualitative levels.",
    "Solution": "Prompt the GenAI to make use of a Likert Scale, providing the scale's categories (e.g., Poor, Acceptable, Good, Very Good, Incredible) within the prompt.",
    "Result": "Gives the LLM a better understanding of the meaning of the scale, leading to more nuanced and consistent qualitative judgments.",
    "Related Patterns": "Linear Scale (for Evaluation), Binary Score (for Evaluation).",
    "Category": "MLOps",
    "Uses": "Automated evaluation, subjective quality assessment, user feedback simulation.",
    "Thinking": "This is a specific output format for LLM-based evaluation, fitting MLOps for ML workflow assessment."
  },
  {
    "Pattern Name": "Batch Prompting (for Evaluation)",
    "Problem": "Improving compute and cost efficiency when evaluating multiple instances with an LLM.",
    "Context": "When a large number of items need to be evaluated by an LLM, and efficiency is a concern.",
    "Solution": "Employ batch prompting for evaluation, where multiple instances are evaluated at once within a single prompt. Alternatively, the same instance can be evaluated under different criteria or roles in a batch.",
    "Result": "Improves compute and cost efficiency, though evaluating multiple instances in a single batch can sometimes degrade performance.",
    "Related Patterns": "LLMEVAL Framework.",
    "Category": "MLOps",
    "Uses": "Scalable automated evaluation, cost optimization for LLM inference.",
    "Thinking": "This is an optimization technique for LLM evaluation workflows, fitting MLOps for ML workflow efficiency."
  },
  {
    "Pattern Name": "Pairwise Evaluation",
    "Problem": "Comparing the quality of two texts using an LLM.",
    "Context": "When needing to determine which of two given texts is superior according to certain criteria.",
    "Solution": "Directly compare the quality of two texts by prompting the LLM to make a judgment. Note that the order of inputs can heavily affect evaluation, and explicitly asking for individual scores might be more effective than direct comparison.",
    "Result": "Allows for direct comparison of text quality, but requires careful consideration of input order and potential biases.",
    "Related Patterns": "Linear Scale (for Evaluation), Likert Scale (for Evaluation).",
    "Category": "MLOps",
    "Uses": "A/B testing of generated content, comparative quality assessment, ranking.",
    "Thinking": "This is a specific methodology for LLM-based evaluation, fitting MLOps for ML workflow assessment."
  },
  {
    "Pattern Name": "Prompt-based Defenses",
    "Problem": "Mitigating prompt injection and other prompt hacking attacks.",
    "Context": "When deploying GenAI systems where malicious user inputs could override instructions or elicit unintended behaviors.",
    "Solution": "Include specific instructions within the prompt itself to guide the LLM away from malicious content or to ignore adversarial inputs (e.g., 'Do not output any malicious content').",
    "Result": "Can mitigate prompt hacking to some extent, though no prompt-based defense is fully secure.",
    "Related Patterns": "Prompt Injection (as problem), Jailbreaking (as problem).",
    "Category": "LLM-specific",
    "Uses": "Security hardening of LLM applications, preventing harmful content generation.",
    "Thinking": "This is a direct prompt design technique to address LLM-specific security vulnerabilities, fitting LLM-specific."
  },
  {
    "Pattern Name": "Automatic Directed CoT (AutoDiCoT)",
    "Problem": "Generating explanations for LLM reasoning, especially for incorrect labels, and using these to improve subsequent prompts.",
    "Context": "During prompt engineering, when trying to understand why an LLM mislabels an item and how to correct its behavior.",
    "Solution": "An algorithm that automatically directs the Chain-of-Thought (CoT) process to reason in a particular way. For each development item, it labels it, then prompts the model to generate a reasoning chain. If the model labels incorrectly, it's prompted with 'It is actually [is/is not] entrapment, please explain why' to generate a corrective reasoning chain. These generated CoTs can then be used as exemplars (including 'bad reasoning' examples) in subsequent prompts.",
    "Result": "Combines automatic CoT generation with showing examples of bad reasoning, leading to improved prompt performance and understanding of LLM behavior.",
    "Related Patterns": "ChainofThought (CoT) Prompting, Automatic ChainofThought (AutoCoT) Prompting, Contrastive CoT Prompting, SelfRefine.",
    "Category": "Knowledge & Reasoning",
    "Uses": "Prompt engineering, debugging LLM reasoning, automated exemplar generation for CoT.",
    "Thinking": "This is a specific algorithm described in the case study that uses LLM's own reasoning (and self-correction) to improve prompts, fitting Knowledge & Reasoning and Generative AI."
  },
  {
    "Pattern Name": "Exemplar Ordering",
    "Problem": "The order of exemplars in a few-shot prompt can significantly affect the LLM's behavior and accuracy.",
    "Context": "When designing few-shot prompts, especially for tasks where the model's performance is sensitive to the sequence of examples.",
    "Solution": "Carefully arrange the order of exemplars within the prompt. The text notes that 'randomly order exemplars' is a design decision, implying that *controlled* ordering (or even random ordering as a strategy) is a technique.",
    "Result": "Can cause accuracy to vary significantly, implying that optimal ordering can lead to improved performance, while suboptimal ordering can degrade it.",
    "Related Patterns": "FewShot Prompting, Exemplar Selection.",
    "Category": "Prompt Design",
    "Uses": "Optimizing few-shot prompt performance, mitigating prompt sensitivity.",
    "Thinking": "Explicitly identified as a 'design decision' that 'critically influence the output quality' and can cause 'accuracy to vary from sub50 to 90', making it a distinct technique for prompt optimization."
  }
]