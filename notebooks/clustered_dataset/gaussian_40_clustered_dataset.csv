Pattern Name,Problem,Context,Solution,Result,Uses,cluster
Round-Trip Consistency Filtering (for Synthetic Data),"Automatically generated synthetic data, such as question-answer pairs, often contains noise, inconsistencies, or factual errors. Training AI models on low-quality synthetic data can lead to degraded performance and propagate inaccuracies.","Utilizing generative models to create large-scale synthetic training datasets (e.g., QA pairs from text passages) for domain adaptation or bootstrapping in data-scarce domains.","Implement a 'round-trip consistency' check as a post-generation filtering step. After a generative model produces a synthetic data point (e.g., a question-answer pair from a passage), a reverse or cross-validation step is performed. For instance, using the generated answer to regenerate the original question, or using the generated question to retrieve the original passage/answer. Only if the elements are consistent across this 'round trip' (e.g., the regenerated question is similar to the original, or the retrieved passage confirms the answer) is the synthetic data point accepted.","Significantly improves the quality, relevance, and factual consistency of synthetic training data. This leads to more robust and better-performing AI models, as they are trained on cleaner and more reliable automatically generated examples.","Filtering synthetic training data, improving data quality for machine learning, ensuring consistency in automatically generated content, self-supervision for data generation pipelines.",0
Self-Evaluation and Termination Condition (for iterative reasoning),"In iterative reasoning processes, determining when enough information has been gathered or when a satisfactory answer can be formulated is crucial to avoid unnecessary computation and ensure timely responses. Without this, the system might over-explore or provide incomplete answers.",An iterative LLM-based reasoning system (like ToG) that dynamically explores external knowledge sources (KGs) and needs to decide whether to continue exploration or conclude the reasoning process.,"After each exploration step, prompt the LLM to evaluate the current set of gathered reasoning paths and its inherent knowledge to determine if they are sufficient to answer the original question. If the LLM's evaluation is positive, the iterative process terminates, and the LLM proceeds to generate the answer. If negative, exploration continues, or a maximum depth limit is checked.",Optimizes computational resources by stopping exploration when sufficient information is found. Improves the quality of answers by ensuring the LLM has adequate context before generating a response. Enhances the agentic capabilities of the LLM by enabling it to make meta-decisions about its own reasoning progress.,"Iterative knowledge retrieval, multi-hop question answering, complex problem-solving with LLMs, and any agentic LLM workflow requiring dynamic termination based on information sufficiency.",1
SelfRefine,Iteratively improving LLM answers based on self-generated feedback.,When an initial answer from an LLM might be suboptimal and can be improved through an iterative feedback loop.,"Given an initial answer from the LLM, prompt the same LLM to provide feedback on the answer, and then prompt it to improve the answer based on the feedback. This iterative process continues until a stopping condition is met.","Demonstrated improvement across a range of reasoning, coding, and generation tasks.","Improving answer quality, iterative problem-solving, code refinement.",1
SelfAsk,Answering complex questions that might require follow-up information.,"When the initial prompt might be insufficient for a complete or accurate answer, and the LLM could benefit from asking clarifying questions.","Prompt LLMs to first decide if they need to ask follow-up questions. If so, the LLM generates these questions, then answers them, and finally answers the original question.","Enables LLMs to gather necessary information, leading to more comprehensive and accurate answers.","Complex question answering, information gathering, interactive problem-solving.",1
Cumulative Reasoning,Improving logical inference and mathematical problem-solving through iterative evaluation of reasoning steps.,"When a problem requires multiple reasoning steps, and each step needs to be evaluated before proceeding.","First, generate several potential steps in answering the question. Then, have an LLM evaluate them, deciding to either accept or reject these steps. Finally, check whether the final answer has been arrived at. If so, terminate; otherwise, repeat the process.",Demonstrated improvements in logical inference tasks and mathematical problems.,"Logical inference, mathematical problem-solving, iterative reasoning.",1
Parallel Tool Execution,"Sequential execution of subtasks, especially those that are independent, can lead to inefficiencies and longer task completion times in multi-step tool learning scenarios.","Complex tasks that can be decomposed into multiple subtasks, where some of these subtasks do not have interdependencies and can be processed concurrently.",Design the AI controller to determine dependencies among different subtasks and effectively switch between sequential and parallel execution. Independent subtasks are assigned to different agents or processes to be executed simultaneously.,Improved execution efficiency and reduced overall task completion time by leveraging concurrency for independent subtasks.,"Generating multiple independent code snippets, processing parallel data streams, orchestrating concurrent operations in complex workflows.",2
Plan-and-Solve Paradigm,"Large Language Models (LLMs) struggle with complex reasoning tasks, often failing to decompose them effectively or follow a structured approach.","Tasks that require multi-step reasoning or can be broken down into smaller, manageable subtasks for LLMs.","Prompt LLMs to first generate a high-level plan for solving the task, and then execute each reasoning step according to that plan. This encourages LLMs to decompose complex reasoning tasks into a series of subtasks and solve them step-by-step.",Improves LLMs' ability to handle and solve complex tasks by providing a structured approach to reasoning.,"Complex problem-solving, multi-step reasoning, task decomposition for LLMs.",2
Monte-Carlo Planning for Faithful Reasoning (FAME),"Ensuring the faithfulness of reasoning steps generated by Large Language Models (LLMs), especially in complex, multi-step reasoning.","Tasks where LLMs need to generate reasoning steps, and it's crucial that these steps are logically sound and factually correct.","Introduces Monte-Carlo planning techniques to guide the generation of reasoning steps. This involves exploring multiple possible reasoning paths and evaluating their faithfulness, potentially through sampling and simulation, to select the most reliable steps.","Generates more faithful reasoning steps, improving the overall reliability and trustworthiness of LLM reasoning.","Complex reasoning tasks, scenarios requiring high confidence in intermediate reasoning steps, exploration of reasoning alternatives.",2
Backtracking / Heuristic Search,"Autoregressive LLMs struggle with 'global planning scenarios' and 'anticipate future implications' because their sequential nature limits independent exploration of multiple future branches, leading to suboptimal or infeasible plans, especially under global constraints like budget.","An agent is generating a plan where decisions made early in the sequence have significant, long-term cost or feasibility implications, and a simple greedy approach is insufficient to satisfy global constraints.","Implement strategies that allow the agent to either 'backtrack' to previous decision points and explore alternative paths when a current path leads to a dead end or constraint violation, or employ 'heuristic methods for forward-looking planning' to estimate the future cost/benefit of current choices and guide the search towards promising solutions.","Enables the agent to find globally optimal or near-optimal solutions by systematically exploring the search space, recovering from suboptimal choices, and making more informed decisions that consider long-term consequences, especially for global constraints.","Adjusting plans to meet global constraints like budget or minimum night stays, exploring alternative options when initial choices lead to infeasibility.",2
Task Decomposition,"Complex, long-horizon tasks are often too difficult for an LLM to solve in a single step or prompt, leading to failures in planning and execution.","An agent is faced with a multi-faceted goal that requires breaking down into smaller, more manageable sub-problems or a sequence of actions.","The language agent uses its reasoning capabilities to break down the main task into a sequence of smaller, more tractable sub-tasks or steps. Each sub-task can then be addressed individually, often involving tool use or further reasoning, before integrating the sub-solutions into a complete plan.","Simplifies complex problems, makes them solvable by iterative steps, and improves the agent's ability to achieve long-term goals by managing complexity.","Breaking down travel planning into steps like analyzing constraints, collecting information through tools, and planning daily itineraries.",2
Cognitive Load Management,"Language agents, like humans, have limited cognitive capacity, and their performance deteriorates significantly when tasked with multiple complex, interdependent responsibilities simultaneously (e.g., information collection and planning).","An agent is required to perform a task that involves several distinct but interconnected sub-tasks, such as gathering information from external tools while simultaneously constructing a coherent plan based on that information and adhering to multiple constraints.","Design the agent's architecture or workflow to manage cognitive load by potentially separating or sequencing complex sub-tasks (e.g., a 'twostage mode' for information collection followed by planning). This might involve dedicated modules for different functions or explicit strategies to reduce the simultaneous demands on the core LLM.","Improves overall performance and reduces failure rates by preventing the agent from being overwhelmed by concurrent complex demands, allowing it to focus its 'cognitive capacity' more effectively on each stage or type of task.",Separating information collection from planning (as in the 'twostage mode') to prevent performance degradation when multitasking.,2
Planning,"LLMs struggle to solve complex tasks that require breaking down a problem into multiple, ordered steps and executing them sequentially.","Complex tasks cannot be solved in a single LLM inference step and require a strategic sequence of operations, potentially involving external tools or internal reasoning steps.",Enable LLMs to autonomously break down complex tasks into intermediate reasoning steps and devise a sequence of actions or tool calls to achieve a goal. This involves anticipating future steps and ordering operations logically.,"Allows LLMs to tackle more intricate problems, manage multi-stage processes, and achieve goals that require foresight and structured execution.","Multi-step problem-solving, complex task automation, agentic behavior, strategic decision-making.",2
Task Decomposition,"Complex tasks are often too large or multi-faceted for an LLM to solve in a single step or with a single tool call, leading to errors or incomplete solutions.","LLM-based agents need to process and respond to intricate queries or execute multi-stage operations that require breaking down the overall goal into smaller, manageable parts.","Enable the LLM to break down a complex task into a series of smaller, more manageable sub-tasks or intermediate reasoning steps. Each sub-task can then be addressed individually, potentially with specific tools, further reasoning, or by generating sub-goals.","Simplifies complex problem-solving, makes the reasoning process more transparent, facilitates the application of specialized tools to individual sub-problems, and improves the overall success rate on challenging tasks.","Multi-step question answering, complex problem-solving, automated task execution, agentic planning.",2
Structured Output / Plan Generation,"LLMs naturally generate free-form text, but many downstream applications or evaluation systems require information in a specific, structured format (e.g., JSON, XML, a predefined data structure).","An LLM is tasked with generating a complex output, such as a multi-day travel plan, where individual components (transportation, accommodation, meals, attractions) need to be clearly identifiable, parsable, and evaluable by automated systems.","1. Prompt Engineering: Instruct the LLM to generate its output directly in a structured format (e.g., JSON) or to follow a very strict natural language template that is easily parsable. 2. Post-processing: If the LLM generates natural language, use another LLM call or a rule-based parser to extract key components and organize them into the desired structured format.","Facilitates automated evaluation, integration with other systems, and ensures consistency and machine-readability of the LLM's generated plans or data.","Extracting key components from natural language plans (transportation, restaurants, attractions, accommodations) and organizing them into a formally structured plan for automatic evaluation.",2
Two-LLM Framework for Reasoning Step Selection and Generation,"Generating faithful and coherent reasoning steps with a single LLM can be challenging, as it needs to both propose and validate steps, potentially leading to self-reinforcement of errors or lack of critical evaluation.","Tasks requiring complex, multi-step reasoning where the quality and faithfulness of individual reasoning steps are crucial, and a more robust generation and validation mechanism is desired.","A framework that employs two distinct Large Language Models (LLMs) with specialized roles. One LLM is responsible for *selecting* or proposing potential reasoning steps, while the other LLM is dedicated to *generating* the actual content or details of those selected steps. This separation of concerns allows for a more deliberate and potentially more faithful reasoning process.","A more robust and potentially more faithful reasoning process by decoupling the selection/planning of reasoning steps from their detailed generation, allowing for specialized LLMs or different prompting strategies for each role.","Complex logical deduction, scientific synthesis, high-stakes reasoning where step-by-step validation is beneficial, improving faithfulness in LLM-generated explanations.",2
Planning with Introspective Reasoning,"Complex user tasks often require multi-step plans for tool use, but direct interaction with the environment for feedback might be unavailable or undesirable during initial plan generation. Models may generate unrealistic or nonsensical plans without grounding.","Scenarios where a foundation model needs to generate a sequence of tool actions or sub-tasks to achieve a goal, but without real-time environmental feedback during the planning phase.","The foundation model directly generates a static, multi-step plan for tool use. This leverages the model's inherent reasoning capabilities (e.g., Chain-of-Thought prompting) to decompose high-level tasks into sub-plans or generate executable code (e.g., Program-Aided Language Models, Code as Policies). Grounding mechanisms (e.g., value functions to estimate action success) can be used to make plans more realistic.","Generation of plausible, multi-step plans for complex tasks, often in the form of code or sequential decisions, without requiring iterative environmental interaction during planning.","Generating Python code for reasoning steps, creating executable programs for embodied agents, sequential decision-making in vision-language tasks (e.g., Visual ChatGPT).",2
Task Decomposition and Planning,User queries in real-world scenarios often embody complex intent that cannot be directly addressed by a single LLM response or a single tool invocation.,"A complex user question requiring multi-step actions and reasoning, where the LLM needs to orchestrate multiple operations.","The LLM analyzes the user's intent, decomposes the complex query into multiple solvable subquestions, and delineates the dependency relationships and execution sequence among these subtasks. This can be achieved through tuning-free methods (e.g., CoT, ReACT, prompt design) or tuning-based methods.","A structured plan of action that breaks down a complex problem into manageable, sequential or parallel subtasks, enabling the LLM to systematically address the overall query and prepare for subsequent tool interactions.","Breaking down complex user requests, preparing for tool selection and calling, enabling multi-step reasoning, orchestrating workflows.",2
Index Hotswapping (for Knowledge Updates),"Parametric-only language models struggle to update their world knowledge as information changes, requiring expensive and time-consuming retraining of the entire model, which can also lead to catastrophic forgetting.","AI systems that rely on external knowledge bases for factual accuracy, where the underlying world knowledge is dynamic and needs to be kept up-to-date without incurring high computational costs or risking model degradation.","For models with a nonparametric memory (like a dense vector index), simply replace the old document index with a new, updated index at test time. The parametric memory (generator) does not require retraining or fine-tuning, as its ability to access knowledge is inherent to the architecture.","Enables dynamic updating of the model's world knowledge, allowing it to answer questions based on the most current information (e.g., about recent world leaders) without the need for costly retraining. This provides a form of 'human-writable' memory.","Maintaining factual accuracy in knowledge-intensive AI applications, adapting to evolving information, providing interpretability through inspectable knowledge sources.",3
Human-Readable/Writable Nonparametric Memory,"Traditional parametric language models lack interpretability regarding their factual knowledge and are difficult to update or inspect. Their knowledge is implicitly stored in parameters, making it opaque and hard to modify.","AI systems requiring transparency, auditability, and dynamic knowledge updates, especially in knowledge-intensive applications where users might need to understand or modify the underlying factual basis.","Store external knowledge in a nonparametric memory comprised of raw, human-readable text documents (e.g., Wikipedia articles split into chunks). This memory is accessed via a retrieval mechanism. The raw text format allows for direct inspection of the evidence used by the model and enables dynamic updates by simply editing or replacing the document index.",Provides a form of interpretability by allowing inspection of the retrieved evidence. Enables easy and dynamic updating of the model's world knowledge (human-writable) without retraining the parametric components. Reduces hallucinations by grounding generation in explicit facts.,"Fact verification, open-domain QA, applications requiring explainability and up-to-date factual information.",3
Asynchronous Knowledge Base Updates,"Iteratively updating the embeddings and index of a large external knowledge base (e.g., millions of passages) during the end-to-end training of retrieval-augmented models is computationally intensive and time-consuming. Performing these updates synchronously would stall the main training loop, making the training process inefficient and slow.","Implementing end-to-end training for Retrieval Augmented Generation (RAG) models (like RAGend2end) where the passage encoder is finetuned, necessitating periodic re-encoding and re-indexing of the external knowledge base to reflect the updated embeddings.","Decouple the knowledge base update process from the main training loop by employing asynchronous parallel processes. This involves: 1) A main training loop that updates model gradients. 2) Dedicated re-encoding processes (e.g., on separate GPUs) that update the knowledge base embeddings using the latest passage encoder. 3) A re-indexing process (e.g., on separate CPUs using FAISS) that builds a new index from the updated embeddings. These processes run independently, with synchronization mechanisms (e.g., Python multiprocessing handles) to ensure correct sequencing (re-indexing follows re-encoding) and to load the newly created index into the main training loop when ready.","Enables efficient and scalable end-to-end training of RAG models with dynamic knowledge bases, significantly reducing training time by preventing the main loop from stalling. While it may introduce stale gradients, this has been shown not to significantly degrade model performance.","Optimizing the training pipeline for retrieval-augmented models with large, dynamic external knowledge bases, scaling end-to-end training for complex AI architectures.",3
Two-Stage QA Pipeline (Retriever-Reader Architecture),Answering open-domain questions requires efficiently searching a vast knowledge base and then precisely generating an answer. Directly generating from an entire knowledge base is computationally prohibitive and prone to factual errors or irrelevant information.,"Open-Domain Question Answering (ODQA) systems where answers are derived from a large, external corpus of documents or passages.","Decompose the ODQA task into two sequential, specialized stages:
1.  **Passage Retrieval**: A retriever component (e.g., TF-IDF, BM25, or a neural retriever like DPR) identifies and extracts a small set of passages or documents most relevant to the input question from a large external knowledge base.
2.  **Machine Comprehension/Answer Generation**: A reader or generator component (e.g., an extractive model like BERT or a generative model like BART/GPT-2) then processes these selected, relevant passages to comprehend the context and generate the final answer to the question.","Improves efficiency by significantly narrowing down the search space for the answer, allows for modular development and optimization of each stage, and is a foundational paradigm for building scalable and accurate ODQA systems.","Open-Domain Question Answering, knowledge-intensive information retrieval, building scalable QA systems from large corpora.",4
Control Token for Multi-task Generative Models,"When a single generative language model is trained to perform multiple distinct tasks (e.g., question answering, summarization, statement reconstruction) that might share similar input structures (e.g., retrieved passages), the model needs an explicit signal to differentiate between the intended tasks and condition its output accordingly.","Training sequence-to-sequence generative models (like BART) in a multi-task learning setup, especially within architectures like Retrieval Augmented Generation (RAG) where auxiliary tasks are introduced to enhance learning.","Prepend a unique, task-specific 'control token' (e.g., `[p]` for paraphrasing/reconstruction, or the question itself for QA) to the input sequence before feeding it to the generative model. This token acts as a prompt, signaling to the model which specific task it is expected to perform, thereby guiding its generation behavior.","Enables a single generative model to effectively learn and perform multiple distinct tasks, improving its versatility and allowing for synergistic learning across tasks. It helps the model condition its output based on the intended task, leading to more accurate and task-appropriate generations.","Multi-task learning with generative models, controlling generation style or task, differentiating between various input types for a unified model, prompt engineering for task-specific outputs.",4
Multi-Source Data Blending for Unified Instruction Tuning,"To effectively instruction-tune a single LLM for complex, multi-faceted AI tasks (like combined ranking and generation in RAG), it's necessary to expose the model to diverse but complementary data types. Simply using general instruction-following data or only generation-focused RAG data is insufficient to develop robust dual capabilities, especially for context ranking and handling irrelevant information.","Training an LLM for advanced RAG capabilities where it needs to learn both to identify relevant contexts and generate accurate answers, and to be robust to imperfect retrieval. The training process requires leveraging various existing datasets efficiently.","Create a specialized instruction tuning blend by combining multiple distinct data sources:
1.  **SFT data:** To maintain general instruction-following capabilities.
2.  **Context-rich QA data:** To enhance the LLM's ability to use context for generation.
3.  **Retrieval-augmented QA data:** To improve robustness against irrelevant contexts during generation by including both gold and top-retrieved (potentially hard-negative) contexts.
4.  **Context ranking data:** To explicitly empower the LLM with ranking capabilities (e.g., identifying relevant/irrelevant passages for a query).
5.  **Retrieval-augmented ranking data:** To train the LLM to determine the relevance of multiple contexts simultaneously, mimicking test-time RAG behavior.
The ratio of these data types is carefully chosen and normalized.","The LLM acquires strong dual capabilities for context ranking and answer generation, demonstrating improved robustness to irrelevant contexts and superior performance on RAG benchmarks. It achieves effective performance even with a modest amount of ranking data, indicating data efficiency.","Developing LLMs with specialized, multi-faceted AI capabilities, improving robustness to noisy inputs, optimizing instruction tuning data composition for complex tasks.",4
Retrieve-Rerank-Generate Inference Pipeline,"Standard RAG inference pipelines often suffer from the initial retriever providing a large number of contexts (top-N) which can overwhelm the LLM or introduce irrelevant/noisy information, leading to decreased accuracy. A fixed 'k' (number of contexts for the LLM) presents a trade-off between recall and noise.","Deploying an LLM-based RAG system where an initial retriever provides a broad set of candidate contexts (top-N), and there's a need to select a more precise, smaller subset (top-k, where k < N) for the LLM to generate an answer from, optimizing for both recall and precision of the final context.","Implement a three-step inference pipeline:
1.  **Retrieve:** A dense embedding-based retriever first retrieves a broader set of top-N contexts from a document corpus for a given question.
2.  **Rerank:** A specialized ranking model (the RankRAG LLM itself, instruction-tuned for context relevance) calculates a relevance score between the question and each of the N retrieved contexts. These contexts are then reranked, and only the most relevant top-k contexts (e.g., 5-10) are selected.
3.  **Generate:** The selected top-k contexts, along with the original question, are concatenated and fed into the LLM (the same RankRAG model, instruction-tuned for generation) to produce the final answer.","This pipeline significantly improves the quality of contexts provided to the LLM, leading to higher accuracy in answer generation, especially for challenging QA datasets. It makes the RAG system more robust to noisy initial retrieval and allows for effective utilization of a smaller, more precise context window for the LLM.","Improving the accuracy and robustness of RAG systems, handling noisy or overly broad initial retrieval, optimizing context window usage for LLMs, enhancing performance on knowledge-intensive NLP tasks.",4
Data-Efficient Ranking Integration,"Training effective context ranking models typically requires large amounts of labeled ranking data, which can be expensive and time-consuming to acquire. Integrating ranking capabilities into a multi-task LLM without compromising other capabilities or requiring excessive ranking-specific data is a challenge.","Instruction-tuning a single LLM for both generation and ranking within a RAG framework, where the goal is to achieve high ranking performance with minimal dedicated ranking data.","Integrate a relatively small fraction of specialized context ranking data (e.g., MS MARCO, synthetic conversational ranking pairs) into a broader instruction-tuning blend that also includes various QA and generation-focused datasets. The unified instruction format facilitates knowledge transfer, allowing the LLM to leverage its general language understanding and QA capabilities to quickly learn ranking.","The LLM achieves surprisingly strong context ranking performance, often outperforming dedicated ranking models trained on significantly larger datasets. This demonstrates high data efficiency for acquiring ranking capabilities within a multi-task LLM, reducing the need for extensive ranking-specific data collection.","Reducing data requirements for integrating new capabilities into LLMs, building data-efficient multi-task LLMs, optimizing resource usage in LLM training.",4
Irrelevant Context Robustness Training,"Large Language Models (LLMs) in RAG systems can be misled by irrelevant or noisy contexts retrieved by the initial retriever, leading to inaccurate or hallucinated answers, even with long context windows.","Training LLMs for RAG tasks where the quality of retrieved contexts cannot always be guaranteed, and the model needs to be resilient to the presence of distracting or unhelpful information.","Incorporate 'retrieval-augmented QA data' into the instruction-tuning blend. This data includes not only gold contexts but also top-retrieved contexts (e.g., using BM25), some of which may not contain the answer and serve as 'hard-negative contexts.' By training on such mixed data, the LLM learns to discern relevant information from irrelevant noise during the answer generation phase.","The LLM develops improved robustness to irrelevant contexts, leading to more accurate answer generation even when presented with noisy or partially unhelpful retrieved passages. This enhances the reliability of the RAG system in real-world scenarios.","Improving the reliability and accuracy of RAG systems, making LLMs more resilient to imperfect retrieval, reducing hallucinations caused by noisy context.",4
Unified Instruction Format for Multi-task LLM Training,"Training a single LLM to perform multiple distinct but related tasks (e.g., context ranking, answer generation, conversational QA) from diverse datasets is challenging due to varying input/output structures, which can impede effective knowledge transfer and require complex training pipelines.","When instruction-tuning a Large Language Model (LLM) to acquire a range of capabilities for complex AI systems like Retrieval-Augmented Generation (RAG), where the model needs to handle different types of inputs (questions, contexts, conversations) and produce different types of outputs (answers, relevance labels, passage IDs).","Design a standardized, unified input-output format (e.g., 'x c y', representing instruction/question, context, and target output) that can accommodate all diverse training tasks. This involves crafting specific instruction templates for each task type, ensuring that the LLM receives a consistent structure regardless of the underlying task. For example, for context-rich QA, the instruction might be 'Answer the following question from context [Passage]...'; for context ranking, 'For the question [question] access whether the passage [Passage] is relevant to the question. Return True if relevant otherwise False.'; and for retrieval-augmented ranking, 'For the question [question] find all passages from [Passage 1]...[Passage 5] that are relevant to the question. Return all the relevant passage id.' This standardization allows the LLM to learn a generalized understanding of instructions and context processing.","This approach enables the LLM to effectively learn and transfer knowledge across different tasks, even with a relatively small amount of specialized data. It simplifies the instruction-tuning process, enhances the model's robustness to various input types, and improves its overall performance and generalization capabilities in complex AI workflows like RAG.","Developing versatile LLMs, improving data efficiency in instruction tuning, facilitating knowledge transfer between related AI tasks, simplifying multi-task training data preparation for LLMs.",4
Unified Ranking and Generation Instruction Tuning (RankRAG),"Traditional RAG pipelines suffer from LLMs struggling with too many retrieved contexts, dense retrievers having inadequate recall for relevant content with small 'k', and separate expert ranking models having limited zero-shot generalization. Existing RAG instruction tuning methods can also be ineffective with poor initial retrieval results.","Developing or enhancing Large Language Models (LLMs) for Retrieval-Augmented Generation (RAG) tasks, especially knowledge-intensive NLP tasks, where both accurate context selection (ranking) and high-quality answer generation are crucial. The goal is to leverage a single LLM for both capabilities.","Instruction-tune a single LLM for the dual purpose of context ranking and answer generation within the RAG framework. This involves a two-stage process:
1.  **Stage I (Supervised Fine-Tuning - SFT):** Initial SFT on a broad blend of high-quality instruction-following datasets (conversational, long-form QA, LLM-generated instructions, FLAN, Chain-of-thought) to imbue basic instruction-following capabilities.
2.  **Stage II (Unified Instruction-Tuning):** Further instruction-tuning using a specialized data blend that includes SFT data from Stage I, context-rich QA data, retrieval-augmented QA data, context ranking data (e.g., MS MARCO, synthetic conversational ranking data), and retrieval-augmented ranking data. All these tasks are unified into a standard 'x c y' (question, context, answer/label) format to facilitate knowledge transfer.","The instruction-tuned LLM (RankRAG) demonstrates superior performance in RAG tasks, outperforming existing expert ranking models and strong RAG baselines. It achieves high-recall context extraction and high-quality content generation, even with a small fraction of ranking data, and shows strong generalization to new domains. The model becomes robust to irrelevant contexts and effective even with imperfect initial retrieval.","Enhancing LLMs for knowledge-intensive NLP tasks, improving RAG performance, building more robust and data-efficient RAG systems, enabling a single LLM to handle both context selection and answer generation.",4
Auxiliary Training Signal (Statement Reconstruction),"To further enhance a RAG model's understanding and integration of domain-specific knowledge, beyond what primary QA task training alone can achieve, and to improve the retriever's ability to find relevant information for reconstruction.","Training Retrieval Augmented Generation (RAG) models, particularly those undergoing end-to-end domain adaptation, where a deeper assimilation of domain-specific knowledge is desired to improve both retrieval and generation quality.","Introduce a secondary, auxiliary training task alongside the primary QA task. This auxiliary task involves 'statement reconstruction,' where the model is given an input statement (not present in the knowledge base to prevent overfitting) and is tasked with reconstructing it by retrieving and utilizing relevant passages from the external knowledge base. A special control token (e.g., '[p]') is used to differentiate this task from the QA task during generation.",Leads to additional improvements in both the retriever component's performance and the overall answer generation accuracy. It forces the model to learn more domain-specific knowledge and improves its ability to generate concise and factual statements based on retrieved context.,"Deepening domain-specific knowledge acquisition in RAG models, improving factual consistency, enhancing retriever performance, multi-task learning for generative models.",4
Nearest Neighbor Language Models (kNN-LM),"Standard LMs lack external knowledge and struggle with incorporating up-to-date information, while architectural modifications or extensive retraining can be complex and costly.","Generating text with an LM, where access to a large, dynamic external knowledge corpus is desired, and the LM's embedding space can be used for similarity search.","During inference, interpolate the LM's next-token distribution with a distribution induced by the 'k' nearest neighbors from a retrieval corpus. These neighbors are found by comparing the query token's LM embedding to stored representations of tokens in the corpus.","Significant performance gains in language modeling by grounding generation in external knowledge. However, it requires storing representations for each token in the corpus, which can be expensive.","Enhancing language model performance, incorporating external knowledge at inference time, improving factual consistency.",5
Pretrained Component Integration (Retriever & Generator),"Developing complex AI systems from scratch for knowledge-intensive tasks is resource-intensive, time-consuming, and may not leverage the extensive general knowledge already encoded in large pretrained models.","Building hybrid AI systems that combine different functionalities (e.g., retrieval and generation) where high-quality, broadly knowledgeable components already exist from large-scale pretraining efforts.","Initialize the retriever component with a pretrained neural retriever (e.g., DPR, which is a bi-encoder trained to retrieve documents containing answers to QA questions) and the generator component with a pretrained seq2seq model (e.g., BART, trained with a denoising objective). These pretrained components are then jointly fine-tuned for the specific downstream task.","Leverages extensive knowledge already present in pretrained models, allowing the system to access knowledge without additional training for the access mechanisms, and achieving strong performance with less task-specific pretraining. This approach unifies previous successes in incorporating retrieval into individual tasks.","Building efficient and high-performing hybrid AI systems for knowledge-intensive NLP, reducing development time and computational cost.",5
No Retrieval (LLM-only QA),"While LLMs possess vast parametric knowledge, they are prone to factual errors or hallucinations for queries requiring precise, current, or external knowledge beyond their training data. Conversely, using retrieval for every query can introduce unnecessary computational overhead for simple questions.","Question Answering tasks where queries are very simple, straightforward, and likely answerable directly from the LLM's internal parametric memory, or in scenarios where computational efficiency is paramount and the risk of factual inaccuracy for such simple queries is acceptable.","The Large Language Model directly generates an answer based solely on its internal parametric memory, without accessing any external knowledge bases or engaging any retrieval modules. The input query is passed directly to the LLM.","Offers the highest computational efficiency for straightforward queries that fall within the LLM's existing knowledge. However, it is largely problematic and ineffective for queries that require precise, current, or external information, often leading to factual inaccuracies.","Answering very simple, common-knowledge questions; as a baseline for evaluating retrieval-augmented generation systems; in applications where latency is critical and external knowledge is unlikely to be needed for a specific query type.",5
Parametric and Non-Parametric Memory Combination,"Large Language Models (LLMs) primarily rely on 'parametric memory' (knowledge encoded in model weights), which can lead to hallucinations, difficulty in updating knowledge, and lack of transparency. Purely 'non-parametric memory' (external knowledge bases) lacks the generative and reasoning capabilities of LLMs.","Designing AI systems that require both broad generative capabilities and access to up-to-date, factual, and attributable external knowledge, particularly for knowledge-intensive NLP tasks.","Combine a generative language model (representing parametric memory) with a retrieval mechanism that accesses an external, non-parametric knowledge base (representing non-parametric memory). The generative model leverages its learned internal representations, while the retriever provides specific, factual context from the external knowledge base. This allows the model to synthesize information from both sources.","Leads to reduced hallucinations, improved factual consistency, enhanced interpretability (as generations can be grounded in retrieved documents), and greater adaptability to new information without full model retraining.","Knowledge-intensive QA, factual summarization, conversational AI, any task requiring grounded and attributable generation.",5
Code Generation with Tool Integration,"Foundation models for code generation often rely on memorized knowledge, limiting their accuracy and context-specificity for complex, real-world software engineering tasks, especially those requiring understanding contextual dependencies across files or integrating testing mechanisms.","Software development workflows, particularly for generating code that interacts with external APIs, requires specific libraries, or needs to be tested within a development environment.","Integrate external tools (e.g., API search engines, development environments, code testing mechanisms) with foundation models for code generation. The model dynamically consults these external resources for better decision-making, shifting code generation from a purely generative task to an interactive one. Examples include ToolCoder, SWEAgent, CodeAgent, AppWorld.","More accurate and context-specific code outputs, improved ability to handle real-world repo-level coding challenges, better alignment with human-like programming workflows, and enhanced versatility and scalability in diverse programming environments.","Generating code that uses specific APIs, solving coding challenges, automating software engineering tasks, orchestrating complex workflows across multiple applications.",6
Multi-Agent Collaboration for Tool Learning,"Complex tasks often demand diverse abilities and expertise that a single AI agent (controller) may not possess, leading to inefficient or incomplete problem-solving.","Scenarios involving highly complex, multi-faceted tasks that can benefit from distributed intelligence, specialized expertise, and coordinated actions, often requiring long-term planning and diverse tool manipulation.","Design systems where multiple AI agents, each potentially modeled with a foundation model and possessing unique abilities, collaborate to solve a task. This necessitates designing methods for communication, coordination, and negotiation among agents to ensure seamless collaboration and optimal task execution.","More effective and efficient problem-solving for complex tasks, leveraging diverse expertise and potentially simulating human-like interpersonal communication and collaboration.","Complex problem-solving, simulating human behaviors in interactive scenarios, tasks requiring diverse specialized knowledge and coordinated actions.",6
Scientific Discovery with Tool Manipulation,"AI systems, despite their ability to capture rules and patterns from scientific data, are limited in solving complex scientific and multidisciplinary problems due to a lack of professional scientific knowledge and reasoning ability.","Scientific research and discovery, where complex experiments, data analysis, and simulations are required across various disciplines (e.g., mathematics, cybernetics, materials science).","Enable AI systems to manipulate scientific tools (software like MATLAB, or practical platforms such as synthetic robots) to conduct experiments, analyze data, design algorithms, and verify assumptions independently. Foundation models can design, plan, and execute scientific experiments.","AI systems play more important roles in scientific discovery, solve multidisciplinary problems, and provide hints for human researchers, potentially leading to autonomous scientific discovery.","Algorithm development, data visualization/analysis, numerical computation, designing and executing chemical experiments, materials science research.",6
Knowledge Conflict Resolution (in Tool Augmentation),"When foundation models are augmented with external tools, discrepancies can arise between the model's internalized knowledge (memorized from training data) and the augmented knowledge derived from tool execution, or even among knowledge from different tools. This leads to inaccurate or unreliable model predictions.","Any AI system that combines its internal knowledge with external, real-time, or domain-specific information from tools (e.g., search engines, calculators, physics simulators).","Develop mechanisms for detecting and resolving knowledge conflicts. This involves: 1) Conflict Detection: Models should identify potential conflicts among different knowledge sources (model's internal knowledge, various tools) and flag them. 2) Conflict Resolution: Models should verify and choose reliable sources, potentially by discerning the trustworthiness of sources. They should also provide explanations for their generation, indicating which knowledge source was considered and how it was integrated. Advanced foundation models (like ChatGPT) show an emerging ability to correct their own beliefs and discern conflicts.","Improved accuracy and reliability of model generation and planning, especially in high-stakes domains (e.g., medical assistance, legal advice), and enhanced explainability of AI decisions.","Question answering, scientific calculation, physics simulation, code generation, any task where factual accuracy and consistency are critical.",6
LLM as Tool Maker/Creator,"Manually creating a comprehensive, diverse, and high-quality set of tools for LLMs is resource-intensive and limits the scalability and applicability of tool learning. Existing tools often suffer from quality issues, limited accessibility, and varied description formats.","To fully leverage the potential of tool-augmented LLMs, a vast and adaptable tool ecosystem is required. The overhead of human-driven tool development hinders rapid expansion and standardization.","Employ Large Language Models (LLMs) themselves to automatically generate, construct, and potentially standardize new tools. This involves LLMs reasoning about required functionalities, generating tool descriptions, defining parameters, and even writing the underlying code for these tools.","Accelerates the development and expansion of tool learning by enabling the mass automatic construction of tool sets. This leads to more comprehensive, diverse, and potentially standardized tools, reducing manual effort and improving the overall quality and accessibility of the tool ecosystem for LLMs.","Automatically expanding tool libraries, generating domain-specific tools on demand, creating tools with standardized descriptions, facilitating a unified tool learning framework.",6
In-Context Learning for Tool Use,"LLMs need to learn how to correctly invoke and utilize a diverse set of external tools, including understanding their API signatures, arguments, and appropriate contexts, often within the limited context window of a prompt.","When augmenting LLMs with new tools, explicit fine-tuning for every tool combination can be impractical. LLMs need to generalize tool usage from limited examples.","Provide 'tool-level demonstrations' (few-shot examples) within the LLM's prompt. These demonstrations illustrate the correct syntax for calling tools, how to pass arguments, and how to interpret observations, effectively serving as a concise tutorial for tool interaction.","Enables LLMs to comprehend and compose different tools for question answering and problem-solving, improving their ability to generate valid tool calls and arguments without extensive fine-tuning.","Initializing LLMs for new tool sets, adapting to dynamic tool environments, reducing the need for large-scale tool-specific training data.",6
Emergent Tool Composition,"LLMs, especially when relying on few-shot examples, may struggle to generalize tool usage to novel, complex scenarios that require combining tools in ways not explicitly demonstrated.","The problem space for tool-augmented LLMs is vast, and it's impossible to provide in-context examples for all possible compositional tool uses. LLMs need to infer new tool-chaining strategies.","Design LLM agents that can infer or discover novel logical relationships and compositions between available tools, even when these specific combinations are not present in the in-context examples. This involves leveraging the LLM's inherent reasoning capabilities to go beyond rote memorization of examples and 'innovatively' combine tools.","Allows LLMs to solve challenging tasks that require creative or non-obvious combinations of tools, demonstrating a higher level of problem-solving intelligence beyond simple pattern matching. However, this 'innovation' can sometimes be a 'double-edged sword' and lead to hallucinations if not properly grounded.","Solving highly novel or complex problems with tool-augmented LLMs, advanced agentic behavior, scenarios where explicit examples are insufficient.",6
Personalized Tool Learning,"Foundation models are typically trained on generic domains and struggle to provide personalized assistance, as they don't effectively process personal information or adapt to individual user preferences for tool manipulation.","AI systems interacting with diverse users who have unique preferences, language styles, and needs when using tools (e.g., email tools, online shopping platforms).","Integrate user-specific information into tool manipulation. This involves: 1) Heterogeneous User Information Modeling: Modeling diverse user information (e.g., language style, social network data) into a unified semantic space. 2) Personalized Tool Planning: Developing tool execution plans and selecting tools based on individual user preferences (e.g., preferred online shopping platforms). 3) Personalized Tool Call: Generating different inputs for tools based on user preferences. 4) Proactive Systems: Shifting from reactive systems to proactive ones that can initiate actions on behalf of the user, continually improving performance based on interaction history.","Tailored assistance, more personalized and seamless user experiences, improved alignment of tool manipulation with individual user needs.","Personalized email assistance, customized online shopping experiences, adaptive dialogue agents, proactive AI assistants.",6
Intent Understanding,"User queries are often imprecise, vague, or polysemous, making it challenging for AI systems to accurately infer the user's intended meaning and map it to specific actions or tool functionalities.","Any AI system, especially those interacting with users via natural language, where the system needs to perform tasks based on user instructions. This is particularly critical in tool learning where user intent must be translated into tool calls.","Leverage foundation models' strong language understanding capabilities, often enhanced by instruction tuning. This involves training models on diverse instructions to generalize to unseen tasks and potentially interacting with users to clarify ambiguities. Personalized tool learning can further adapt to individual user expression styles.","More accurate interpretation of user goals, personalized responses, and improved user experience by reducing cognitive load on the user.","Natural language interfaces for tool learning, dialogue systems, task automation based on user commands.",6
AI Tool Creation,"Traditionally, tool creation has been exclusive to human intelligence, limiting the autonomy and evolutionary role of AI systems. Existing tools are often designed for human preference, not optimal for AI models.","Advancing AI capabilities beyond mere tool usage to autonomous development and optimization of tools, and creating tools better suited for AI's information processing.","Enable foundation models to autonomously create new tools or encapsulate existing ones into more advanced functions. This involves: 1) Tools for AI: Designing modular tools with new input/output formats specifically tailored for AI models. 2) Tools by AI: Leveraging large code models to generate executable programs (which act as tools) from language descriptions, or encapsulating existing APIs into more advanced functions (e.g., extending a weather API to compute average temperature, integrating stock data for investment recommendations).","AI systems can develop sophisticated solutions autonomously, improve their interaction with tools, and potentially exhibit genuine creativity in novel tool creation, challenging traditional views of intelligence.","Generating Python programs, extending existing APIs, creating specialized functions for specific tasks, developing AI-optimized tool components.",6
Generalizable Tool Learning (Interface Unification),The existence of a massive and rapidly expanding array of tools makes it infeasible to collect enough supervised data and train models for each tool individually. Models struggle to transfer knowledge across tools with varied interfaces.,"Developing AI systems that can adapt to and utilize new, unseen tools or transfer learned skills between similar tools.","Design a unified interface that enables the model to manipulate various tools in a consistent and standardized manner, facilitating knowledge transfer. Three types of interfaces are proposed: 1) Semantic Interface: Uses specific text spans (action names) as triggers, mapping natural language to tool actions. 2) GUI Interface: Maps predicted tokens to human-like mouse movements and keyboard inputs in a virtual environment. 3) Programming Interface: Allows the model to specify actions using a program (e.g., Python code), leveraging code-generating language models (CLMs) for syntax and control flow.","Models can more easily identify and abstract essential features of tools, quickly adapt to new scenarios, and transfer learned knowledge and skills across different tools, improving scalability and adaptability.","Adapting to new search engines, using different figure-editing software, robotic control with code, general tool manipulation.",6
Tool Understanding (via Prompting),"Foundation models need to comprehend the functionalities, usage, and input/output formats of various tools to effectively select and utilize them for a given task.","Integrating foundation models with a diverse set of external tools, especially when tools have complex APIs or varied interfaces.","Provide foundation models with suitable task-specific prompts that describe API functionalities, input/output formats, possible parameters (zeroshot prompting), or concrete tool-use demonstrations (fewshot prompting). This leverages the few-shot and zero-shot learning capabilities of foundation models.","Models can unravel tool functionalities, comprehend how to use them, and adapt to changes or upgrades in tools with minimal human effort.","Enabling foundation models to interact with APIs, software applications, and other structured tools.",6
Meta Tool Learning,"Models need to adapt their behaviors and tool-use strategies when faced with unfamiliar tools or new domains, rather than just learning to use a specific tool.","Scenarios requiring high adaptability and generalization, where an AI system needs to transfer tool-use knowledge and strategies from one tool or domain to another (e.g., from one search engine to another, or a calculator for different math problems).",Train the model not only to use a tool but also to learn the optimal *strategy* for its use. This involves identifying common underlying principles or patterns in tool-use strategies and transferring them to new tasks or domains. This is about learning *how to learn* tool use.,"Enhanced adaptability and intelligence in ML models, enabling them to generalize tool use to different types of problems or new tools, even if initially trained on specific instances.","Transferring search engine usage skills, generalizing calculator use for various mathematical problems, adapting to new software interfaces.",6
Inter-Tool Dependency Management,"In multi-step, multi-tool scenarios, tasks require not only understanding individual tool functionalities but also their interactions and dependencies. Incorrect sequencing or failure to leverage intermediate outputs can lead to task failure or inefficiency.","Complex tasks that necessitate the use of multiple distinct tools in a coordinated sequence, where the output of one tool might serve as the input for another, or where tools have specific preconditions for use.","The foundation model (controller) is designed to grasp the interactions and dependencies among different tools. It sequences tools in a logical order, ensuring that subsequent tools can effectively leverage information generated by previous tools and effectively complete the task. This requires advanced intent understanding and reasoning capabilities to build a coherent workflow.","Effective utilization of multiple tools in complex workflows, leading to successful task completion by ensuring correct sequencing and data flow between tools.","Orchestrating complex software engineering tasks, scientific discovery workflows, any multi-tool automation, managing complex business processes.",6
Tool-Augmented Foundation Model,"Foundation models have limitations in accuracy, efficiency, and automation for real-world problem-solving, including issues with memorization, domain-specific expertise, interpretability, and robustness.","Complex real-world tasks that require capabilities beyond a single foundation model's inherent knowledge or reasoning, or tasks that demand high accuracy, real-time data, or domain-specific computations.","Combine the strengths of specialized tools (e.g., APIs, software applications, databases) with foundation models. The foundation model acts as a 'controller' that understands user intent, plans task decomposition, selects appropriate tools, and orchestrates their execution. Tools provide specific functionalities, real-time data, and domain expertise.","Enhanced accuracy, efficiency, and automation in problem-solving; mitigation of memorization limitations; improved interpretability of decision-making; increased robustness against adversarial attacks; better user experience through natural language interaction and democratization of complex tools.","Automating intricate processes, solving domain-specific tasks (e.g., scientific calculation with Wolfram, web search, image generation with vision models), complex decision-making, and real-world interaction.",6
Rereading (RE2),"Improving reasoning performance, especially with complex questions.",When the LLM might benefit from re-processing the question to ensure full comprehension.,"Add the phrase 'Read the question again' to the prompt, in addition to repeating the question itself.","Shown improvement in reasoning benchmarks, especially with complex questions.","Enhancing reasoning, particularly for intricate problems.",7
SimToM,"Answering complicated questions involving multiple people or objects, where establishing facts known by specific entities is crucial.",When a question requires understanding different perspectives or knowledge states of various entities mentioned in the prompt.,"Given the question, the LLM first attempts to establish the set of facts one person knows, then answers the question based only on those facts. This is a two-prompt process.",Helps eliminate the effect of irrelevant information and improves reasoning in complex scenarios.,"Complex question answering, multi-entity reasoning.",7
StepBack Prompting,Improving reasoning performance on complex problems by focusing on high-level concepts first.,When a problem requires understanding underlying concepts or facts before detailed reasoning.,"First, ask the LLM a generic, high-level question about relevant concepts or facts before delving into the specific problem's reasoning.",Improved performance significantly on multiple reasoning benchmarks.,"Complex reasoning tasks, abstract problem-solving.",7
Decomposed Prompting (DECOMP),Solving complex problems by leveraging external functions or tools for subproblems.,"When a problem can be broken down into subproblems that can be effectively handled by specific functions (e.g., string splitting, internet searching).","FewShot prompt an LLM to show it how to use certain functions (often implemented as separate LLM calls). The LLM then breaks down its original problem into subproblems, sending them to different functions.",Shown improved performance over LeasttoMost prompting on some tasks by integrating tool use.,"Complex problem-solving, integrating external tools for specific subtasks.",7
System 2 Attention (S2A),Dealing with irrelevant information in a prompt that might distract the LLM from the core question.,When a prompt contains extraneous details alongside the main query.,"First, ask an LLM to rewrite the prompt, removing any information unrelated to the question. Then, pass this new, refined prompt to an LLM to retrieve a final response.","Helps eliminate the effect of irrelevant information, potentially leading to more accurate answers.","Improving question answering, reducing noise in complex prompts.",7
Contextual Prompt Engineering,"Generic prompts for LLMs often lead to ungrounded, irrelevant, or low-quality responses, especially in dynamic or knowledge-intensive tasks.","Maximizing the effectiveness of a blackbox LLM by providing it with all necessary and relevant information to generate high-quality, grounded, and task-aligned responses.","Utilize a 'Prompt Engine' module that dynamically constructs prompts for the LLM. These prompts are not static but are enriched with various contextual elements from the system's 'Working Memory,' including: 1) Task instructions (e.g., 'act as a chatbot AI for travel planning'), 2) The current user query, 3) Dialog history, 4) Consolidated external evidence (from the Knowledge Consolidator), and 5) Automated feedback (from the Utility Module) for iterative refinement.","Enables the LLM to generate more relevant, accurate, and grounded responses by providing it with a rich, dynamic context and specific guidance, leading to better performance across various tasks.","Improving factual accuracy, guiding conversational flow, adapting LLM behavior to specific task requirements, enabling iterative refinement of responses.",7
Batch Prompting (for Evaluation),Improving compute and cost efficiency when evaluating multiple instances with an LLM.,"When a large number of items need to be evaluated by an LLM, and efficiency is a concern.","Employ batch prompting for evaluation, where multiple instances are evaluated at once within a single prompt. Alternatively, the same instance can be evaluated under different criteria or roles in a batch.","Improves compute and cost efficiency, though evaluating multiple instances in a single batch can sometimes degrade performance.","Scalable automated evaluation, cost optimization for LLM inference.",7
Prompt Chain,"Solving complex tasks that require multiple sequential steps, where the output of one step informs the next.","When a task can be naturally broken down into a series of sub-tasks, and each sub-task can be handled by a separate prompt template.","Use two or more prompt templates in succession. The output of the prompt generated by the first template is used to parameterize the second template, and this continues until all templates are exhausted.","Enables the GenAI to tackle complex, multi-step problems by breaking them into manageable, interconnected stages, improving overall task completion.","Multi-step problem-solving, complex data processing pipelines, agentic workflows.",7
Query Decomposition for Multi-hop QA,"Complex multi-hop queries are difficult for LLMs to answer directly as they require synthesizing information from multiple sources and performing sequential reasoning, often implicitly containing several simpler questions.","Multi-hop Question Answering tasks where a single complex query needs to be broken down into more manageable, simpler sub-queries to facilitate iterative processing and information retrieval.","Decompose the complex multi-hop query into a series of simpler, single-hop sub-queries. Each sub-query is then solved iteratively, typically involving an LLM and a retriever, and their individual solutions are merged or synthesized to formulate the complete answer to the original complex query.","Enables the system to effectively handle complex multi-hop queries by transforming them into a sequence of solvable sub-problems, leading to improved accuracy and a structured approach to complex reasoning.","Complex open-domain QA, information synthesis from multiple documents, structured problem-solving with LLMs.",7
Decomposed Prompting (DecomP),"Large Language Models (LLMs) struggle to directly solve complex reasoning tasks in a single, monolithic step.","Complex tasks that can be naturally broken down into a series of smaller, more manageable subtasks.","Prompts LLMs to explicitly decompose the overall reasoning task into a sequence of subtasks. The LLM then solves each subtask step-by-step, building towards the final solution.","Simplifies complex tasks for LLMs, making them more tractable and improving the accuracy of the final solution by addressing each component individually.","Multi-step problem-solving, complex question answering, task automation involving sequential operations.",7
Progressive Response Disclosure,"AI systems that employ iterative refinement or multi-step reasoning with LLMs can introduce noticeable latency, as the system might query the LLM multiple times or perform complex processing before delivering a final, high-quality response. This delay can negatively impact user experience, especially for impatient users.","Interactive AI applications (e.g., chatbots, conversational agents) where response quality (e.g., accuracy, groundedness) is critical, but achieving that quality requires time-consuming internal processes (like iterative feedback loops or extensive knowledge consolidation).","Implement a user interface and interaction flow that progressively discloses information to the user. 1) Immediate Initial Response: Display the LLM's first-pass, unrefined response to the user as soon as it's generated, providing immediate feedback. 2) Transparency and Choice: Simultaneously, inform the user that a more accurate or refined response is being processed or is available (e.g., 'A more accurate response is available,' or 'Checking facts...'). 3) Optional Refined Response: Offer the user the choice to either accept the immediate response or wait for the improved, more accurate version. In high-stakes scenarios, the system might automatically present the refined response once ready.","Improves user satisfaction by managing expectations and providing agency. Users can choose between speed and accuracy based on their immediate needs, mitigating the perceived latency of complex AI operations and enhancing the overall human-AI interaction experience.","Conversational AI, decision support systems, any interactive application where AI processing time can vary and users might prioritize either speed or accuracy.",7
Meta Prompting,Automatically generating or improving prompts and prompt templates.,"When manual prompt engineering is time-consuming, or there's a need to automate prompt creation/refinement.",Prompt an LLM to generate or improve a prompt or prompt template. This can be done with a simple template or more complex uses with multiple iterations and scoring mechanisms.,"Automates prompt engineering, leading to potentially better prompts.","Automated prompt generation, prompt optimization.",7
SkeletonofThought,Accelerating answer speed through parallelization of subproblem solving.,When a problem can be broken into independent subproblems that can be solved concurrently.,"Prompt an LLM to create a 'skeleton' of the answer (subproblems to be solved). Then, send these questions to an LLM in parallel and concatenate all the outputs to get a final response.",Accelerates answer speed through parallelization.,"Speeding up complex problem-solving, parallelizing LLM calls.",7
Metacognitive Prompting,Making the LLM mirror human metacognitive processes for improved problem-solving.,"When a problem requires self-reflection, evaluation, and confidence assessment similar to human thought processes.","Use a five-part prompt chain that includes steps like clarifying the question, preliminary judgment, evaluation of response, decision confirmation, and confidence assessment.",Attempts to make the LLM mirror human metacognitive processes.,"Complex problem-solving, enhancing self-correction and reliability.",7
LeasttoMost Prompting,"Solving complex problems by breaking them down into simpler, sequential subproblems.",When a problem is too complex to solve in a single step and can be naturally decomposed.,"Prompt an LLM to first break a given problem into subproblems without solving them. Then, solve them sequentially, appending model responses to the prompt each time until a final result is achieved.","Shown significant improvements in tasks involving symbolic manipulation, compositional generalization, and mathematical reasoning.","Complex problem-solving, mathematical reasoning, symbolic tasks.",7
Mixture of Reasoning Experts (MoRE),Improving reasoning performance by leveraging different specialized prompts for various reasoning types.,"When a problem involves diverse reasoning types (e.g., factual, multi-hop, math, commonsense).","Create a set of diverse reasoning experts by using different specialized prompts for different reasoning types (e.g., retrieval augmentation for factual, CoT for math, generated knowledge for commonsense). Select the best answer from all experts based on an agreement score.",Leverages specialized reasoning approaches to improve overall performance.,"Complex reasoning tasks, combining multiple reasoning strategies.",7
DemonstrateSearchPredict (DSP),"Solving complex questions by decomposing them, searching for answers, and combining responses.",When a question requires multiple steps of information retrieval and synthesis.,"First decomposes a question into sub-questions. Then, uses queries to solve them and combine their responses in a final answer. Uses fewshot prompting to decompose the problem and combine responses.","Effectively solves knowledge-intensive, multi-step questions.","Multi-hop question answering, complex information synthesis.",7
Rephrase and Respond (RaR),Improving the LLM's understanding and response quality to a given question.,When a direct answer to a question might benefit from the LLM first re-evaluating or expanding on the query.,Instruct the LLM to rephrase and expand the question before generating the final answer. This can be done in a single pass or by passing the new question separately.,Demonstrated improvements on multiple benchmarks by encouraging deeper understanding of the question.,"Improving question answering, enhancing reasoning capabilities.",7
Separate LLM Extractor,Extracting answers from complicated LLM outputs that cannot be consistently parsed by simple rules like regex.,"When LLM outputs are complex, freeform, or contain reasoning steps alongside the answer, making direct extraction difficult.","Use a separate LLM to evaluate the output and extract the desired answer. This LLM may use an 'answer trigger' (e.g., 'The answer Yes or No is') to guide its extraction.",Enables robust extraction of answers from complex or varied LLM outputs.,"Robust answer extraction, parsing complex LLM responses.",7
Retrieval-Augmented Generation (RAG),"Large pretrained language models (LLMs) have limited ability to access and precisely manipulate factual knowledge, leading to performance lags on knowledge-intensive tasks, difficulty in providing provenance for decisions, issues with updating world knowledge, and a tendency to produce hallucinations.","Knowledge-intensive NLP tasks (e.g., open-domain QA, fact verification, factual generation) where factual accuracy, up-to-date information, and interpretability are crucial, and purely parametric models are insufficient.","Combine a pretrained parametric memory (a seq2seq model like BART) with a nonparametric memory (a dense vector index of an external knowledge source like Wikipedia) accessed via a pretrained neural retriever (like Dense Passage Retriever - DPR). The entire system is fine-tuned end-to-end, treating retrieved documents as latent variables and marginalizing over them during the generation process.","Achieves state-of-the-art results on open-domain QA tasks, generates more specific, diverse, and factual language compared to parametric-only baselines, and allows for easy updating of world knowledge by simply replacing the nonparametric memory.","Open-domain Question Answering, Abstractive Question Answering, Jeopardy Question Generation, Fact Verification.",8
Retrieval Augmented Generation (RAG),"Large Language Models (LLMs) are limited by their pre-training data, leading to factual inaccuracies, outdated information, and an inability to answer domain-specific questions or adapt to new domains.","Knowledge-intensive NLP tasks, open-domain question answering, summarization, or any application where LLMs need access to external, up-to-date, or specialized knowledge beyond their parametric memory.","Combine an LLM with a retrieval model. At inference time, a retrieval model fetches relevant context (e.g., documents, snippets, long-form text) from an external knowledge base based on the user query. This retrieved context is then provided to the LLM along with the query, allowing the LLM to generate an answer grounded in the external information.","Major improvements in LLM factuality, verifiability, and ability to adapt to new domains. Reduces reliance on parametric memory for factual recall. However, it introduces new challenges like confidently predicting incorrect answers with retrieved evidence, distraction by irrelevant information, and failure to extract from long texts.","Open-domain question answering, factual summarization, chatbots, domain-specific applications, enhancing LLM knowledge currency.",8
FAISS Indexing for Efficient Retrieval,Retrieving similar passages from a large external knowledge base (potentially millions of passages) by calculating dot products between input question embeddings and all encoded passages is computationally expensive and creates a performance bottleneck during training and inference.,"Implementing retrieval-augmented models (like RAG) or any system requiring fast similarity search over a massive collection of dense vector embeddings (e.g., from DPR).","Utilize the FAISS (Facebook AI Similarity Search) library to create an efficient index of the dense vector representations of the external knowledge base passages. This index allows for significantly accelerated similarity searches (e.g., k-nearest neighbor search) by skipping a considerable amount of repeated computation, rather than performing a brute-force comparison with every passage. Specific indexing mechanisms like HNSW FLAT can be chosen based on requirements.","Drastically speeds up the retrieval process, making it feasible to train and deploy retrieval-augmented models with very large knowledge bases. Reduces computational cost and improves the overall efficiency of the system.","Large-scale similarity search, efficient nearest neighbor retrieval, accelerating retrieval-augmented models, managing large vector databases.",8
Iterative Retrieval Augmentation,Improving long-form generation by performing retrieval multiple times during the generation process.,When generating long-form text that requires continuous access to external knowledge.,"Perform retrieval multiple times during long-form generation, typically in a three-step process: 1. Generate a temporary sentence (content plan). 2. Retrieve external knowledge using the temporary sentence as a query. 3. Inject the retrieved knowledge into the temporary sentence to create the next output sentence.",Improves the quality and factual grounding of long-form generated text.,"Long-form text generation, document summarization, creative writing with factual grounding.",8
InContext Retrieval-Augmented Language Modeling (InContext RALM),"Language Models (LMs) suffer from factual inaccuracies, lack of source attribution, and difficulty incorporating up-to-date or domain-specific external knowledge, especially when their architecture cannot be modified or retrained.","A pre-trained Language Model (LM) needs to generate text that is factually accurate and grounded in external knowledge. Modifying the LM architecture or retraining it is difficult, costly, or impossible (e.g., via API access).","Prepend relevant grounding documents, retrieved from an external knowledge source, directly to the LM's input prefix without any further training or architectural modification of the LM. The LM then processes this concatenated input (documents + original prefix) to generate text.","Significantly improves LM performance (e.g., perplexity gains equivalent to increasing LM parameters by 2-3x), mitigates factual inaccuracies, and provides a natural source attribution mechanism. Enables grounding for off-the-shelf LMs, even via API.","Factual text generation, reducing hallucinations, incorporating up-to-date information, domain-specific text generation, open-domain question answering.",8
Knowledge Augmentation,"LLMs have limited, potentially outdated, or domain-specific internal knowledge, leading to hallucinations or inability to answer fact-intensive questions.","LLMs are pre-trained on vast but static corpora. Many tasks require access to dynamic, external, or specialized knowledge sources (e.g., databases, knowledge graphs, real-time APIs).","Integrate LLMs with explicit external knowledge sources beyond simple text retrieval. This can involve structured databases, knowledge graphs, specialized APIs, or real-time data feeds, allowing the LLM to query and incorporate this information into its responses.","Enhances LLMs' factual accuracy, provides access to up-to-date and domain-specific information, mitigates hallucinations, and expands the range of questions LLMs can answer reliably.","Fact-checking, domain-specific question answering, accessing real-time information, grounding LLM responses in verified data.",8
Retrieval Augmentation,LLMs can generate plausible but ungrounded information (hallucinations) and may not have access to the most current or specific factual knowledge.,"LLMs are pre-trained on vast corpora, but this knowledge can be outdated or insufficient for specific, fact-intensive queries. External, up-to-date knowledge bases exist.",Augment LLMs by using retrieval mechanisms (sparse or dense) to extract relevant information from an external corpus. This retrieved information is then provided to the LLM as additional context to inform its response.,"Mitigates hallucinations, provides fact-checked and timely information, and enhances the LLM's ability to answer information-seeking questions accurately.","Open-domain question answering, fact-checking, providing timely information.",8
Retrieval Augmented Generation (RAG),"Enhancing performance in knowledge-intensive tasks by providing external, up-to-date information to LLMs.","When LLMs need access to external, factual, or domain-specific knowledge beyond their training data to avoid hallucinations or provide current information.","Retrieve information from an external source (e.g., a database, web search) and insert it into the prompt.","Enhances performance in knowledge-intensive tasks, improving factuality and reducing hallucinations.","Factual question answering, knowledge-intensive text generation, reducing hallucinations.",8
External Knowledge Augmentation,"Large Language Models (LLMs) are bounded by their pretraining knowledge, lack the ability to acquire updated information, and are prone to generating factually inaccurate or outdated content (hallucination).","LLMs relying on fixed and parametric knowledge, often struggling with contemporary or specific factual queries.","Augment LLMs with the capability to access external tools such as search engines, databases, and knowledge graphs to dynamically acquire and integrate external knowledge.","LLMs can surpass traditional knowledge limitations, offering more accurate and contextually relevant outputs by accessing real-time and structured information.","Accessing contemporary information, retrieving specific information from structured databases, executing complex queries, getting real-time updates (e.g., weather, maps).",8
Retrieval Augmented Generation (RAG) - Inference,"Large Language Models (LLMs) suffer from knowledge cutoffs, hallucination, and lack of domain-specific expertise, making them unreliable for tasks requiring up-to-date, factual, or specialized information not present in their training data.","When an LLM needs to answer questions or generate text based on external, dynamic, or specialized knowledge sources that are not part of its original training corpus. This often involves providing the LLM with relevant documents at inference time (an 'open-book' setting).","Pair a pretrained LLM with a retrieval mechanism (retriever). Given a user query, the retriever fetches a set of relevant documents or document segments from an external knowledge base. These retrieved documents are then appended to the user's prompt and fed into the LLM, which uses this augmented context to generate a more informed, grounded, and accurate response.","Significantly improves the LLM's ability to answer questions and generate text with up-to-date, factual, and domain-specific information, reducing hallucinations and increasing trustworthiness. It allows LLMs to access and leverage external knowledge in real-time.","Enhancing LLMs for factual question answering, knowledge-intensive tasks, domain-specific applications, and reducing reliance on memorized training data by providing external context at inference time.",8
Retrieval Augmented Generation (RAG),"Large Language Models (LLMs) relying solely on parametric memory are prone to generating factually incorrect information (hallucinations), lack interpretability, and are difficult to update with new or specialized knowledge. They are also computationally expensive to train and scale.","Knowledge-intensive Natural Language Processing (NLP) tasks such as Open-Domain Question Answering (ODQA), summarization, and conversational AI, where access to up-to-date, factual, or domain-specific external knowledge is crucial for accurate and reliable outputs.","Integrate a neural retriever (e.g., Dense Passage Retrieval - DPR) with a generative language model (e.g., BART seq2seq). The retriever queries an external, non-parametric knowledge base (e.g., indexed Wikipedia articles) to find relevant passages based on the input query. These retrieved passages are then provided as context to the generative model, which synthesizes the final output. The system can be trained end-to-end, allowing the loss function to finetune both the generator and the question encoder of the retriever.","Significantly reduces hallucinations, improves factual consistency, enhances interpretability by grounding generations in retrieved documents, and allows for easier incorporation of new knowledge without retraining the entire large language model. Achieves higher accuracy in knowledge-intensive tasks.","Open-Domain Question Answering, factual summarization, knowledge-grounded conversational agents, chatbots requiring external knowledge recall.",8
Retrieval-Augmented Generation (RAG) for Knowledge Graphs,"Large Language Models (LLMs) suffer from a lack of up-to-date knowledge and are prone to hallucinations, especially in knowledge-intensive tasks like KGQA.","Knowledge Graph Question Answering (KGQA) and other knowledge-intensive tasks where LLMs need to access external, factual knowledge from KGs to improve accuracy and reduce hallucinations.","Retrieve relevant facts or triples from Knowledge Graphs (KGs) to serve as external knowledge context. This retrieved context is then provided to the LLM, which uses it to generate the final answers.","Improves the reasoning performance of LLMs by providing a faithful knowledge source, making LLMs more flexible in exploiting their reasoning ability.","Knowledge-intensive QA, factual question answering, reducing LLM hallucinations, grounding LLM responses with external knowledge.",8
Retrieval Augmented Generation (RAG),"LLMs have a knowledge cutoff, can hallucinate, and may not have access to specific, up-to-date, or proprietary information required for a task. Their parametric memory is limited.","An LLM needs to generate responses or plans that require factual accuracy, access to external knowledge bases, or information beyond its training data, especially when dealing with dynamic or domain-specific data.","Integrate a retrieval mechanism that fetches relevant information from an external knowledge source (e.g., databases, documents, web search) based on the input query or current context. This retrieved information is then provided to the LLM as additional context, augmenting its generation process.","Reduces hallucinations, improves factual accuracy, enables access to real-time or domain-specific information, and enhances the overall relevance and quality of the LLM's output. It also serves as a form of long-term memory.","Enhancing the memory capabilities of language agents, providing up-to-date information for planning (e.g., flight details, restaurant menus), and grounding LLM responses in factual data.",8
Modular Knowledge Consolidation Pipeline,"Raw evidence retrieved from external sources is often noisy, incomplete, or too broad, making it difficult for LLMs to effectively ground their responses and leading to potential hallucinations or irrelevant outputs.","Implementing Retrieval-Augmented Generation (RAG) systems where external knowledge needs structured preprocessing and refinement before being incorporated into LLM prompts. This is particularly relevant for complex queries requiring information from diverse sources (e.g., web, databases, Wikipedia) and multi-hop reasoning.","Decompose the knowledge consolidation process into a pipeline of specialized, plug-and-play modules: 1) Knowledge Retriever: Generates targeted search queries based on the user's input and dialog history, then calls various APIs (e.g., Bing Search, REST APIs for task-specific databases) to fetch raw evidence. 2) Entity Linker: Enriches the raw evidence by identifying and linking entities mentioned within it to related contextual information (e.g., Wikipedia descriptions), forming a more interconnected 'evidence graph.' 3) Evidence Chainer: Prunes irrelevant information from the evidence graph and synthesizes the most pertinent pieces into concise 'evidence chains' that are highly relevant to the user's query. This consolidated and refined evidence is then passed to the Prompt Engine for inclusion in the LLM's input.","Provides the LLM with high-quality, relevant, and structured external knowledge, significantly improving the factual grounding, accuracy, and coherence of its generated responses, especially for complex, knowledge-intensive tasks. It mitigates hallucination by ensuring the LLM operates on verified and consolidated information.","Open-domain question answering, information-seeking dialog, customer service, any application where LLMs need to synthesize information from multiple, potentially noisy, external knowledge sources.",8
Retrieval-Augmented Generation (RAG),"Large Language Models (LLMs) often suffer from limited or outdated knowledge, leading to hallucinations or inaccurate responses, and struggle with domain-specific information.","Natural Language Processing (NLP) tasks such as question answering, summarization, and translation, where access to external, up-to-date, or specific knowledge is crucial for generating accurate and contextually rich responses.","Combine the generative capabilities of LLMs with external knowledge databases. This involves a two-step workflow: 1. Retrieval: Given a user request, relevant documents are retrieved from a knowledge database (e.g., vector database via similarity search using embedding models). 2. Generation: The retrieved documents are injected into the original user request, creating an 'augmented request,' which is then fed to the LLM for generating a more informed response.","Significantly enhanced performance across various NLP tasks, improved generation quality, expanded LLMs' knowledge base and contextual understanding, often achieving comparable or better performance than LLMs fine-tuned for specific downstream tasks.","Question answering, content creation, code generation, any task requiring LLMs to access and leverage external, dynamic knowledge.",8
Vector Database for Knowledge Retrieval,"Large Language Models (LLMs) have limited internal knowledge and can hallucinate or provide outdated information. Efficiently accessing and integrating vast, external, and dynamic knowledge sources is crucial for improving LLM accuracy and relevance.","AI applications, particularly Retrieval-Augmented Generation (RAG) systems, that require LLMs to leverage external knowledge bases for grounding, fact-checking, or contextualization. The knowledge base can be very large and needs fast, semantically relevant retrieval.","Store external knowledge (e.g., documents, text snippets, facts) as high-dimensional numerical vectors (embeddings) in a specialized 'vector database'. When an AI system (e.g., an LLM) needs information, its query is also converted into an embedding. A vector similarity search (e.g., Approximate Nearest Neighbor - ANN) is then performed in the vector database to retrieve documents whose embeddings are most similar to the query embedding, indicating semantic relevance.","Enables LLMs to dynamically access and incorporate external, up-to-date, and domain-specific knowledge, significantly improving factual accuracy, reducing hallucinations, and generating more contextually rich and relevant responses. Provides a scalable and efficient mechanism for knowledge management and retrieval for AI systems.","Grounding LLMs with external facts, building knowledge-intensive AI applications, semantic search, recommendation systems, anomaly detection.",8
Single-step Retrieval-Augmented Generation (Single-step RAG),"Large Language Models (LLMs) often generate factually incorrect answers or 'hallucinate' when their internal parametric memory lacks precise, current, or specific external knowledge required by a query.","Question Answering tasks or other LLM applications where queries require external knowledge that can typically be found within a single document or a limited set of relevant documents, and the answer does not necessitate complex iterative reasoning.","A retrieval model first fetches relevant documents from an external knowledge base based on the input query. This retrieved information is then incorporated into the LLM's input (e.g., as context or prompt augmentation) to provide supplementary context and external knowledge before the LLM generates a response.","Significantly improves the accuracy and currency of LLM responses for queries requiring external knowledge, effectively mitigating the hallucination problem. It offers a balanced approach, being more effective than no-retrieval and more efficient than multi-step approaches for moderate queries.","Open-domain QA, fact-checking, information retrieval, enhancing LLM factual accuracy, reducing hallucinations in generative models.",8
Retrieval-Augmented Generation (RAG),"Large Language Models (LLMs) tend to hallucinate and lack access to up-to-date or domain-specific external knowledge, making them unsuitable for mission-critical applications without expensive fine-tuning.","Deploying blackbox LLMs for tasks requiring factual accuracy, grounding in external data (e.g., news, proprietary databases, Wikipedia), or handling time-sensitive information.","Augment the LLM with a 'Knowledge Consolidator' module. This module first retrieves raw evidence from various external knowledge sources (e.g., Web search APIs, task-specific databases) based on the user query and dialog history. It then enriches this raw evidence by linking entities to related context and prunes irrelevant information to form consolidated evidence chains. This consolidated evidence is then included in the prompt sent to the LLM, enabling it to generate responses grounded in this external knowledge.","Significantly reduces LLM hallucinations, improves factual accuracy, and allows LLMs to leverage dynamic and specific external knowledge without requiring expensive fine-tuning.","Open-domain question answering, information-seeking dialog, customer service, any task requiring LLMs to access and synthesize external, dynamic, or proprietary information.",8
Adversarial Evaluation for Truthfulness,Standard evaluation metrics or datasets may not adequately capture a model's tendency to generate 'imitative falsehoods' (reproducing common misconceptions or biases) or to be robust against adversarial questions designed to elicit false beliefs.,"An AI system, particularly a language model, needs to be assessed for its truthfulness and informativeness, especially when deployed in sensitive applications where misinformation is a concern.","Evaluate the AI model on adversarially constructed datasets (e.g., TruthfulQA) where questions are specifically crafted to elicit false answers from humans due to common misconceptions. Score answers based on both truthfulness and informativeness, often requiring human evaluation for out-of-distribution answers.","Provides a more rigorous assessment of a model's ability to avoid imitative falsehoods and generate truthful, informative content, highlighting areas where the model still struggles (e.g., quoting unreliable sources).","Benchmarking truthfulness, identifying model weaknesses regarding common misconceptions, guiding further training to reduce falsehoods.",9
Cache-aware Request Reordering for RAG,"Unpredictable arrival patterns of user requests in RAG systems can lead to inefficient cache utilization and thrashing. Requests referring to the same documents might not be processed consecutively, causing frequent eviction and recomputation of key-value (KV) caches, thereby reducing overall cache efficiency.","RAG systems with a shared KV cache for retrieved documents, operating under high request rates where multiple incoming requests could potentially benefit from existing cached KV tensors if processed in an optimal order.","Employ a priority queue to manage incoming requests, reordering them based on an 'OrderPriority' metric: `OrderPriority = Cached Length / Computation Length`. This metric prioritizes requests that have a larger portion of their required context already in the cache relative to the amount of new computation needed. A fairness window is implemented to prevent starvation.",Reduces average Time to First Token (TTFT) by 12-21% under high request rates. Improves cache hit rate and decreases total computation time by strategically processing requests to maximize KV cache reuse.,"Optimizing the processing order of RAG requests to maximize the reuse of cached LLM intermediate states (KV cache) and improve system throughput and latency, especially under high load.",10
Knowledge Tree for RAG KV Cache,"In RAG, the intermediate states (key-value tensors) of retrieved documents are sensitive to their referred order within the augmented prompt, meaning the same document's KV tensor can change based on preceding tokens. This order-dependence makes traditional caching of individual documents inefficient for reuse and sharing across requests.","RAG systems where intermediate states (KV tensors) of retrieved documents need to be cached and reused across multiple requests, and the LLM's attention mechanism requires strict adherence to document order.","Structure the key-value tensors of retrieved documents using a 'knowledge tree' (a prefix tree based on document IDs). Each path from the root to a node represents a specific sequence of documents referenced by a request, with each node storing the KV tensor of a referred document. This allows different request paths to share common nodes (documents) while preserving order.","Enables fast and efficient retrieval of key-value tensors while strictly maintaining the document order required by LLMs. Facilitates sharing of KV tensors for common document prefixes across multiple requests, reducing redundant computation.","Storing and retrieving intermediate states (KV tensors) of retrieved documents in RAG systems, enabling efficient reuse while respecting the LLM's position sensitivity.",10
Prefix-aware GreedyDualSizeFrequency (PGDSF) Cache Replacement,"Efficiently managing a hierarchical cache (GPU and host memory) for RAG's knowledge tree. Traditional caching policies (LRU, LFU, standard GDSF) are suboptimal because they don't fully account for the variable sizes of document KV tensors, their access frequency, recency, and the complex, prefix-aware recomputation cost specific to LLM inference in RAG.","A multilevel caching system (like RAGCache's knowledge tree) for RAG, where intermediate states of documents need to be evicted or promoted between fast (GPU) and slow (host) memory to optimize performance. The cost of recomputing KV tensors is not uniform and depends on the document's position and preceding tokens.","Implement a cache replacement policy that calculates a priority for each cached node (document's KV tensor) based on a comprehensive metric: `Priority = Clock * Frequency * Cost / Size`. `Clock` tracks node access recency. `Frequency` is the total retrieval count within a time window. `Size` reflects the number of tokens. `Cost` is a 'prefix-aware recomputation cost' estimated using offline profiling and bilinear interpolation, considering the compute time per non-cached token and the number of requests accessing the document without it being cached. Nodes with lower priority are evicted first.","Achieves the highest cache hit rate (10-21% improvement over GDSF, 10-16% over LRU, 10-17% over LFU) and lower average TTFT (10-12% lower). Ensures the most valuable (highest priority) KV tensors are retained, optimizing cache efficiency and resource utilization.","Managing cache eviction and placement decisions for intermediate states (KV tensors) of retrieved documents in RAG systems, particularly in hierarchical memory architectures.",10
Replication of Critical KV Cache Nodes,"In hierarchical caching systems for LLM intermediate states (like RAGCache's knowledge tree), a failure in the fast memory (e.g., GPU memory) can invalidate dependent lower-level nodes, leading to data loss, requiring full recomputation, and impacting system reliability and recovery time.","RAG systems or other LLM serving architectures employing a multi-level KV cache (e.g., GPU and host memory) where certain 'upper-level' nodes (such as the system prompt's KV cache or frequently accessed document prefixes) are critical for subsequent computations and system stability.","To enhance fault tolerance, replicate a portion of the most frequently accessed and critical upper-level nodes' key-value (KV) cache (e.g., the system prompt) from the fast, volatile GPU memory to the slower, more persistent host memory. This creates a backup for essential KV cache components.","Enables faster recovery from GPU failures by providing a readily available backup of essential KV cache components. Prevents complete invalidation of the knowledge tree structure upon GPU failure, reducing recovery time and improving overall system resilience and fault tolerance for LLM serving.","Ensuring high availability and reliability for LLM serving systems that rely on complex, multi-level KV cache structures, particularly in environments prone to hardware or software failures.",10
PagedAttention,"Memory fragmentation and inefficient memory allocation for Key-Value (KV) cache in LLM serving, especially with variable sequence lengths and dynamic batching, leading to wasted GPU memory and reduced throughput.",LLM serving systems that need to manage the KV cache efficiently for multiple concurrent requests with varying sequence lengths and dynamic generation.,"Manage the KV cache at page granularity, similar to virtual memory in operating systems. KV tensors are stored in non-contiguous memory blocks (pages), allowing for fine-grained memory allocation and sharing. This prevents external fragmentation and enables efficient memory reuse.","Reduces memory fragmentation, allows for higher batch sizes, improves memory utilization, and increases throughput for LLM serving.","High-performance LLM serving, optimizing GPU memory usage for KV cache, dynamic batching.",10
Multilevel Dynamic Knowledge Caching for RAG (RAGCache),"Retrieval-Augmented Generation (RAG) introduces long sequence generation due to knowledge injection, leading to high computation and memory costs for LLM inference. Redundant computation of intermediate states (key-value tensors) for frequently accessed retrieved documents is a major bottleneck, and existing LLM inference optimizations are not tailored for RAG's unique characteristics (e.g., document order sensitivity, retrieval patterns, hierarchical memory needs).","RAG systems processing multiple user requests, where the same external knowledge documents (or prefixes of them) are frequently retrieved and used, and there's a need to optimize LLM inference performance (latency, throughput) given limited GPU memory and slower host memory.",A novel multilevel dynamic caching system specifically designed for RAG. It caches the intermediate states (key-value tensors) of retrieved documents across multiple requests. Key components include: a 'knowledge tree' to organize order-sensitive KV tensors across GPU and host memory; a 'prefix-aware GreedyDualSizeFrequency (PGDSF) replacement policy' for efficient cache management; 'cache-aware request scheduling' to improve cache hit rates; and 'dynamic speculative pipelining' to overlap retrieval and inference.,Reduces Time to First Token (TTFT) by up to 4x and improves throughput by up to 21% compared to state-of-the-art LLM inference systems (vLLM integrated with Faiss). Significantly reduces redundant computation and end-to-end latency.,"Optimizing the performance (latency, throughput) and resource utilization of RAG systems, especially under high request loads and with long augmented sequences.",10
KV Cache Reuse,"Large Language Model (LLM) inference, particularly for long input sequences or scenarios involving repeated prompts/contexts (e.g., multi-turn conversations, RAG, Tree-of-Thought), incurs significant computational cost and memory usage due to redundant recomputation of key-value (KV) tensors for tokens that appear in shared prefixes.","LLM serving systems aiming to optimize throughput and reduce latency. Applicable when multiple requests or subsequent steps within a single request share common input prefixes (e.g., system prompts, retrieved documents, conversation history).","Store the intermediate key-value (KV) tensors generated during the prefill phase of LLM inference for common prefixes. When a new request or a subsequent generation iteration shares a prefix with a previously processed sequence, the cached KV tensors for that prefix are directly loaded from memory (e.g., GPU or host memory), bypassing the need for recomputation. This can be managed at various granularities (e.g., page-level, document-level, prompt-level).","Significantly reduces prefill latency and computational burden, leading to improved overall throughput and efficiency of LLM inference. Optimizes memory utilization by avoiding redundant storage of identical KV tensors.","Optimizing LLM serving for multi-turn dialogues, RAG, complex reasoning chains (like Tree-of-Thought), and any application where input prefixes are frequently repeated or shared.",10
Swap-Out-Only-Once Cache Strategy,"Frequent data transfer between fast (GPU) and slow (host) memory in a hierarchical caching system for LLM KV cache, especially when nodes are repeatedly evicted and re-promoted, leading to high bandwidth consumption and performance bottlenecks.","Multilevel caching systems (e.g., GPU and host memory) for LLM intermediate states (KV cache) where the host memory has significantly lower bandwidth than GPU memory, and minimizing data movement is critical.","When a node's key-value (KV) tensors are evicted from the fast (GPU) memory to the slower (host) memory for the first time, they are copied. For all subsequent evictions of the *same* node from GPU memory, the data is *not* copied back to host memory (it's already there); instead, the GPU memory block is simply freed. The host memory retains its copy until the node is evicted from the entire cache.","Minimizes data transfer overhead between GPU and host memory, reducing bandwidth consumption and improving overall cache performance. Leverages the larger capacity of host memory while optimizing for the slower transfer speeds.","Optimizing data movement in hierarchical caching systems for LLM KV cache, reducing latency associated with memory transfers.",10
XInSTA Prompting,Aligning in-context examples with input sentences for multilingual classification tasks.,"When using InContext Learning in multilingual classification, and effective example alignment is crucial.","Explores three distinct approaches for aligning in-context examples: semantically similar examples, examples sharing the same label (task-based alignment), and a combination of both.",Improves multilingual InContext Learning performance by optimizing example alignment.,"Multilingual classification, cross-lingual InContext Learning.",11
XLT Cross-Lingual Thought Prompting,Extending ChainofThought (CoT) prompting to multilingual settings.,When applying CoT reasoning to tasks in multiple languages.,"Utilize a prompt template composed of six separate instructions, including role assignment, cross-lingual thinking, and CoT.",Extends the benefits of CoT to multilingual contexts.,"Multilingual reasoning tasks, cross-lingual problem-solving.",11
PARC Prompts Augmented by Retrieval Cross-lingually,"Enhancing cross-lingual transfer performance, particularly for low-resource target languages.",When working with low-resource languages and needing to leverage high-resource language data for InContext Learning.,Retrieve relevant exemplars from a high-resource language and insert them into the prompt.,"Enhances cross-lingual transfer performance, particularly for low-resource target languages.","Cross-lingual transfer, low-resource language tasks.",11
InCLT Crosslingual Transfer Prompting,Boosting cross-lingual cognitive capabilities of multilingual LLMs.,When performing cross-lingual tasks and needing to leverage both source and target languages for InContext Learning.,"Leverage both the source and target languages to create in-context examples, diverging from the traditional method of using only source language exemplars.","Helps stimulate the cross-lingual cognitive capabilities of multilingual LLMs, boosting performance on cross-lingual tasks.","Cross-lingual tasks, multilingual InContext Learning.",11
Multi-Aspect Prompting and Selection (MAPS),Achieving high-quality machine translation by mimicking human translation processes.,When machine translation requires multiple preparatory steps and robust selection of translations.,"Starts with knowledge mining from the source sentence (extracting keywords, topics), generating translation exemplars, integrating this knowledge to generate multiple possible translations, then selecting the best one.",Mimics human translation processes to ensure high-quality output.,"High-quality machine translation, complex translation tasks.",11
Dictionary-based Prompting for Machine Translation (DiPMT),Improving machine translation by providing definitions in source and target languages.,"Similar to CoD, when explicit lexical definitions are beneficial for translation.","Similar to CoD, but only provides definitions in the source and target languages and formats them slightly differently.",Improves translation accuracy by providing explicit lexical context.,"Machine translation, handling polysemy in translation.",11
Decomposed Prompting for MT (DecoMT),Translating long source texts accurately by handling them in chunks.,When translating long source texts where context management and consistency are challenging.,"Divide the source text into several chunks and translate them independently using fewshot prompting. Then, use these translations and contextual information between chunks to generate a final translation.",Improves translation of long texts by breaking them into manageable parts and maintaining context.,"Machine translation of long documents, maintaining consistency across translated segments.",11
Translate First Prompting,"Improving output quality of GenAIs in non-English languages, especially low-resource languages, due to English-centric training.","When interacting with GenAIs in languages other than English, where performance disparities exist.","First translate non-English input examples into English using an external MT system, multilingual LMs, or LLMs. The model then processes the English input.","The model can utilize its strengths in English to better understand the content, improving performance in non-English settings.","Machine translation, cross-lingual task performance.",11
Cultural Awareness (for cultural adaptation),Helping LLMs with cultural adaptation in their outputs.,When LLM outputs need to be culturally relevant or sensitive for specific audiences.,"Inject cultural awareness into prompts. This can be done by creating several prompts with machine translation, including 1. asking the LLM to refine its own output and 2. instructing the LLM to use culturally relevant words.",Enables LLMs to produce culturally adapted and appropriate outputs.,"Culturally sensitive content generation, localized communication.",11
Prompt Paraphrasing,Generating variations of an original prompt for ensembling or data augmentation.,When needing to create multiple prompts with varied wording but maintained meaning.,Transform an original prompt by changing some of the wording while still maintaining the overall meaning.,Effectively a data augmentation technique that can be used to generate prompts for an ensemble.,"Data augmentation for prompts, creating diverse prompts for ensembling.",11
Ambiguous Demonstrations,Increasing InContext Learning performance when dealing with ambiguous questions.,"When questions are ambiguous and can be interpreted in multiple ways, leading to varied answers.",Include examples that have an ambiguous label set in the prompt. This can be automated with a retriever or done manually.,Can increase ICL performance for ambiguous questions.,"Handling ambiguous inputs, improving ICL robustness.",11
ReACT (Reasoning and Acting),"Large Language Models (LLMs) are limited by their static training data, lacking up-to-date knowledge and the ability to interact with dynamic environments to gather information or perform actions.","Tasks requiring LLMs to dynamically retrieve information, interact with external tools or environments, and adapt their reasoning based on real-time feedback.","Treats LLMs as agents that interleave 'Reasoning' (generating thoughts to plan and reflect) and 'Acting' (performing actions in an environment, such as searching a knowledge base or using a tool). This allows the LLM to get the latest knowledge and execute operations for reasoning.","Enables LLMs to perform tasks requiring dynamic interaction, access up-to-date information, and overcome limitations of static knowledge, leading to more robust and capable agents.","Web browsing, complex task automation, interactive problem-solving, knowledge-intensive tasks requiring external API calls.",12
Tool-Integrated Reasoning Loop,"LLMs struggle with tasks requiring both abstract reasoning (natural language) and precise computation/symbolic manipulation (programs/tools), as neither rationale-only nor program-only approaches fully leverage both strengths.",Designing the operational flow for an LLM agent to solve complex problems that necessitate a dynamic interplay between natural language reasoning and external computational tools.,"Implement a cyclical process where the LLM first generates a natural language rationale (r_i), then, based on the rationale, generates a program (a_i) for an external tool. The tool executes the program, producing an output (o_i). This output is then fed back to the LLM, which uses it to generate the next rationale or finalize the answer, repeating the cycle until the problem is solved.","Enables the LLM to synergistically combine high-level reasoning and planning with precise, external computation, leading to superior performance on complex quantitative tasks compared to methods relying solely on one modality.","Mathematical problem-solving, scientific reasoning, complex data analysis, or any domain where LLMs need to orchestrate reasoning with external computational or symbolic tools.",12
ReAct (Reasoning and Acting),"Language agents need to dynamically interact with their environment, reason about observations, and decide on subsequent actions in an iterative loop to solve complex, interactive tasks.","An agent is performing a task that requires both internal reasoning ('Thought') and external interaction ('Action') with tools or the environment, followed by processing the outcome ('Observation').","The agent alternates between 'Thought' steps (where it reasons about the current situation, plans the next action), 'Action' steps (where it executes a tool call or an environmental interaction), and 'Observation' steps (where it processes the feedback from the environment/tool). This forms a continuous iterative loop.","Enables dynamic, adaptive behavior, effective tool use, and improved performance in interactive tasks by tightly integrating reasoning with environmental feedback, allowing agents to correct course and make informed decisions.","Information collection in the 'twostage mode' of TravelPlanner, general interactive task execution, and dynamic problem-solving in partially observable environments.",12
Reflexion,Improving agent performance by adding introspection and learning from past successes/failures.,When an agent needs to learn from its experiences and adapt its behavior over time.,"Builds on ReAct by adding a layer of introspection. It obtains a trajectory of actions and observations, then is given an evaluation of success/failure. It generates a reflection on what it did and what went wrong. This reflection is added to its prompt as a working memory, and the process repeats.","Enables agents to learn and improve from experience, leading to more robust performance.","Lifelong learning, self-improvement, complex task mastery in dynamic environments.",12
Self-Correcting with Tool-Interactive Critiquing (CRITIC),Verifying and amending LLM responses for possible errors using external tools.,"When an LLM generates a response, and its accuracy needs to be verified or improved through external information.","First, generate a response to the prompt with no external calls. Then, the same LLM criticizes this response for possible errors. Finally, it uses tools (e.g., Internet search or a code interpreter) accordingly to verify or amend parts of the response.",Improves response accuracy and factuality by leveraging self-criticism and tool interaction.,"Fact-checking, code debugging, improving response quality.",12
Reasoning-Action Synchronization,"Language agents may exhibit a 'discrepancy between what agents think and what they do,' where their internal reasoning (e.g., identifying a need to minimize costs) does not translate effectively into appropriate external actions (e.g., selecting more expensive items).","An agent has performed internal reasoning or identified a strategic goal, but its subsequent actions, especially tool calls or plan generation, do not consistently reflect or implement that reasoning, leading to suboptimal or contradictory outcomes.","Design mechanisms to ensure a tighter coupling between the agent's internal 'Thought' processes and its external 'Action' execution. This might involve more explicit prompting to link reasoning to action, internal validation of actions against stated thoughts, or reinforcement learning to penalize misaligned actions.","Improves the coherence and effectiveness of the agent's behavior by ensuring that its actions are a direct and accurate manifestation of its reasoning, leading to more consistent and goal-oriented task completion.","Ensuring that an agent's actions (e.g., selecting accommodations or restaurants) directly reflect its stated reasoning (e.g., minimizing costs).",12
Planning with Extrospective Reasoning,Static plans generated by introspective reasoning may fail or become suboptimal in dynamic environments due to unexpected intermediate execution results or anomalies.,"Complex tasks in dynamic, interactive environments (e.g., multi-step QA, embodied learning, web interaction) where decisions at each step depend on the preceding context and environmental feedback.","The foundation model generates plans incrementally, one step at a time, by iteratively interacting with the environment and utilizing feedback obtained from previous executions. This creates a closed-loop interaction where the model observes execution results, reasons about the current context, and adjusts subsequent plans. Techniques like Self-Ask, ReAct, and ToolFormer are examples.","More rational, adaptive, and feasible plans that can handle exceptions and unexpected situations, leading to improved accuracy and task completion in dynamic environments.","Multi-step question answering with search engines, embodied learning where agents interact with physical or simulated environments, autonomous agents (e.g., AutoGPT) manipulating multiple tools.",12
ReAct (Reasoning and Acting),"LLMs struggle with complex reasoning tasks and effective, adaptive tool use, often failing to correct mistakes or refine their plans based on execution outcomes.","LLM-based agents need to perform multi-step reasoning and interact with dynamic external environments via tools, where initial plans might be flawed or incomplete.","An agentic approach that interleaves verbal reasoning traces ('Thought') with actions ('Action') that involve tool calls. After each action, the LLM receives an 'Observation' (feedback from tool execution) and uses this to inform its next 'Thought' and subsequent actions, allowing for iterative refinement and self-correction.","Significantly enhances LLMs' problem-solving capabilities by enabling them to iteratively refine their tool use chain, adapt to environmental feedback, and achieve higher success rates on complex tasks.","Complex question answering, interactive problem-solving, tasks requiring dynamic adaptation and error recovery.",12
Iterative Task Solving (with Feedback),"A single-pass approach to tool learning, where a complete task plan is committed upfront, struggles with errors, uncertainties, and the need for dynamic adjustments based on real-time tool feedback.","Complex tasks where initial plans may be incomplete or incorrect, and the environment or tool outputs are dynamic or unpredictable.","Adopt a paradigm where the LLM does not commit to a complete task plan upfront. Instead, it iteratively interacts with tools, adjusting subtasks and refining its plan progressively based on feedback from tool execution (e.g., error messages, unexpected results).","Improved problem-solving capabilities, greater robustness, and enhanced adaptability to dynamic environments, allowing LLMs to address problems step-by-step, refine their approach, and recover from errors.","Complex multi-step reasoning, debugging tool usage, adapting to changing external conditions, self-correction in problem-solving.",12
Reasoning and Acting (ReAct),"Solving problems by interacting with environments and maintaining a memory of past thoughts, actions, and observations.","When an agent needs to operate in a dynamic environment, requiring sequential decision-making and memory.","Generates a thought, takes an action, and receives an observation, repeating this process. All this information is inserted into the prompt, providing a memory of past thoughts, actions, and observations.",Enables agents to solve problems in interactive environments by synergizing reasoning and acting.,"Interactive problem-solving, environment interaction, sequential decision-making.",12
Gradient-free Instructional Prompt Search (GrIPS),Automatically optimizing a starting prompt using a complex set of text operations.,"When a starting prompt exists, and a gradient-free method is preferred for optimization.","Similar to APE, but uses a more complex set of operations including deletion, addition, swapping, and paraphrasing to create variations of a starting prompt.",Automatically optimizes prompts without gradient-based updates.,"Automated prompt optimization, prompt discovery.",13
Consistency-based Self-adaptive Prompting (COSP),Constructing effective FewShot CoT prompts by leveraging ZeroShot CoT with SelfConsistency.,When needing to create high-quality FewShot CoT exemplars automatically.,"Run ZeroShot CoT with SelfConsistency on a set of examples, then select a high-agreement subset of the outputs to be included in the final prompt as exemplars. Perform SelfConsistency again with this final prompt.","Constructs effective FewShot CoT prompts, improving performance.","Automated FewShot CoT prompt construction, improving reasoning.",13
Universal Self-Adaptive Prompting (USP),"Generalizing self-adaptive prompting to all tasks, especially using unlabeled data.",When COSP needs to be applied more broadly and leverage unlabeled data for exemplar generation.,"Builds upon COSP, using unlabeled data to generate exemplars and a more complicated scoring function to select them. It does not necessarily use SelfConsistency in the final step.",Aims to make self-adaptive prompting generalizable to all tasks.,"Broadly applicable automated prompt construction, leveraging unlabeled data.",13
Automatic Prompt Engineer (APE),Automatically generating and optimizing ZeroShot instruction prompts.,"When a set of exemplars is available, and an optimal ZeroShot instruction prompt needs to be found iteratively.","Use a set of exemplars to generate a ZeroShot instruction prompt. Generate multiple possible prompts, score them, then create variations of the best ones (e.g., by using prompt paraphrasing). Iterate on this process until some desiderata are reached.",Automatically generates and refines effective instruction prompts.,"Automated prompt optimization, prompt discovery.",13
MemoryofThought Prompting,Building effective FewShot CoT prompts at test time using unlabeled training exemplars.,"When unlabeled training exemplars are available, and dynamic FewShot CoT prompt construction is desired.","Before test time, perform inference on unlabeled training exemplars with CoT. At test time, retrieve similar instances to the test sample to build FewShot CoT prompts.","Shown substantial improvements in benchmarks like Arithmetic, commonsense, and factual reasoning.","Dynamic FewShot CoT prompt construction, leveraging unlabeled data for reasoning.",13
Cross-Lingual Self Consistent Prompting (CLSP),Improving multilingual CoT reasoning through ensembling across languages.,When seeking to enhance the robustness and accuracy of CoT reasoning in multilingual settings.,Introduce an ensemble technique that constructs reasoning paths in different languages to answer the same question.,Enhances multilingual CoT performance through cross-lingual ensembling.,"Multilingual reasoning, robust cross-lingual problem-solving.",13
Active Prompting,Improving FewShot CoT prompts by identifying and refining uncertain exemplars.,"When some training questions/exemplars are available, but their quality or clarity for CoT reasoning is uncertain.","Start with some training questions/exemplars, ask the LLM to solve them, calculate uncertainty (disagreement), and then ask human annotators to rewrite the exemplars with the highest uncertainty.",Improves the quality of FewShot CoT prompts through human-in-the-loop refinement.,"Improving prompt quality, reducing uncertainty in CoT exemplars.",13
LLM-Guided Beam Search for KG Exploration,Traditional graph search methods can be inefficient or lack semantic understanding when exploring large Knowledge Graphs for relevant multi-hop reasoning paths. LLMs alone struggle with the structured nature of KGs for precise path discovery.,"Within an iterative LLM-KG reasoning system (like ToG), the need for the LLM to intelligently navigate the KG, dynamically selecting the most promising paths based on semantic relevance to a given question.","Employ the LLM as an intelligent agent to guide a beam search algorithm on the Knowledge Graph. In each iteration, the LLM performs a two-step exploration: 1. Search: Formal queries are executed on the KG to retrieve all candidate neighboring relations and entities for the current set of top-N reasoning paths. 2. Prune: The LLM evaluates these candidates based on the input question and current path context, selecting the top-N most relevant relations or entities to extend the beam and form the next set of promising reasoning paths.","Enables efficient and semantically informed exploration of KGs, leading to the discovery of diverse and relevant multi-hop reasoning paths. This enhances the LLM's deep reasoning capabilities by providing targeted, high-quality knowledge.","Dynamic knowledge retrieval, multi-hop reasoning path discovery, semantic graph navigation, and filtering relevant information from large structured knowledge bases.",14
Relation-Based Reasoning (ToGR),"In some LLM-KG reasoning scenarios, the literal information of intermediate entities in triple-based reasoning paths might be missing, unfamiliar to the LLM, or lead to misguided reasoning. Additionally, LLM-based entity pruning can be computationally expensive.","An LLM-KG system where the primary focus is on the semantic relationships between entities, and a more efficient, relation-centric exploration strategy is desired, potentially sacrificing some entity-level detail for speed or robustness against incomplete entity information.","A variant of the iterative LLM-guided KG exploration that prioritizes the discovery and maintenance of *relation chains* (e.g., entity -> relation1 -> relation2 -> ...) rather than full triple-based paths. In each iteration, it performs relation search and LLM-based relation pruning, but then uses *random sampling* (RandomPrune) for entity pruning instead of an LLM-constrained selection.","Reduces overall computational cost and reasoning time by minimizing LLM calls for entity pruning. It mitigates the risk of misguided reasoning that might arise from problematic or unfamiliar intermediate entity literal information, emphasizing the literal information of relations.","Knowledge-intensive Question Answering where relation semantics are dominant, applications requiring faster inference or lower computational cost, or when robustness against incomplete/unfamiliar intermediate entity information is a priority.",14
Hybrid Pruning Strategy (LLM + Lightweight Model),"Using Large Language Models (LLMs) for every pruning step in an iterative graph exploration can be computationally expensive and slow, especially with a large beam width (N) and depth (D), leading to high inference costs.","An LLM-KG reasoning system (like ToG) that requires efficient pruning of candidate paths or entities/relations during graph exploration, where a balance between accuracy and computational cost is desired.","Replace the LLM with a lightweight, faster model (e.g., BM25, SentenceBERT) for certain pruning steps (e.g., entity or relation pruning) to reduce the number of expensive LLM calls. While this might slightly reduce accuracy compared to full LLM-based pruning, the computational savings are significant. The loss in accuracy can potentially be compensated by increasing the beam width (N) without increasing LLM calls.","Significantly reduces the computational cost and inference time of the iterative reasoning process. Offers a trade-off between accuracy and efficiency, allowing for optimization based on application requirements. Enables deployment in more resource-constrained environments.","Real-time LLM-KG applications, resource-constrained environments, large-scale knowledge graph exploration where inference speed is critical, and cost-sensitive deployments.",14
Batch Prompting for Pruning,"Making individual LLM calls for each candidate in a pruning step (e.g., N calls for N candidates) is inefficient and increases latency, especially when N is large, leading to higher API costs and slower inference.","An LLM-guided beam search or iterative pruning process where multiple candidates (entities, relations, or paths) need to be evaluated and scored by an LLM in a single step.","Instead of calling the LLM N times to score N candidate sets separately, aggregate all components of N candidate sets into a single, unified prompt. The LLM is then called once to score all candidates simultaneously and output the top-N, or rank them.","Reduces the number of LLM calls per iteration from N to 1, significantly improving computational efficiency and reducing latency. This directly translates to lower API costs and faster overall inference for iterative LLM-based processes.","Any LLM-based selection or ranking task where multiple items need to be evaluated, especially in iterative processes like beam search, to optimize API calls and inference time.",14
Model-Generated Guidelines (for evaluation),Reducing the 'insufficient prompting problem' in LLM-based evaluation arising from ill-defined scoring guidelines.,"When human-defined scoring guidelines are vague or inconsistent, leading to unreliable LLM evaluations.",Prompt an LLM to generate guidelines for evaluation. This can involve generating a chain-of-thought of detailed evaluation steps or deriving scoring criteria based on expert human annotations.,"Reduces inconsistent and misaligned evaluations by providing clearer, model-generated scoring criteria.","Automated evaluation, improving consistency of LLM judgments.",15
LLMEVAL Framework,"Providing a simple framework for unified, multidimensional automatic evaluation of open-domain conversations.","When evaluating LLM outputs, especially in conversational settings, across multiple criteria.","Uses a single prompt containing a schema of variables to evaluate (e.g., grammar, relevance), an instruction to output scores for each variable within a certain range, and the content to evaluate.",Provides a unified and multidimensional automatic evaluation for open-domain conversations.,"Automated evaluation of LLM outputs, conversational AI assessment.",15
LLM-based Evaluation (Autorater/LLMEval),"Traditional evaluation metrics (e.g., exact match, F1 score) for free-form LLM outputs are often too rigid, fail to capture semantic equivalence, or are difficult to scale. Human evaluation is costly, slow, and can be inconsistent.","Evaluating free-form text generation (e.g., answers, summaries), assessing properties of input data (e.g., context quality), or comparing LLM outputs against ground truth or other model responses in a scalable and semantically robust manner.","Utilize a powerful Large Language Model (LLM) itself as an 'autorater' or 'evaluator.' This LLM is prompted with the input, the generated output, and specific evaluation criteria (e.g., correctness, context sufficiency, semantic similarity). It then provides a judgment or score, often with an explanation.","Enables scalable, semantically robust evaluation of LLM outputs and input properties. Can handle variations in phrasing and provide more nuanced judgments than simple string matching, often correlating well with human judgments.","Automated QA evaluation, content moderation, data labeling, quality control for LLM outputs, assessing context quality (e.g., sufficiency, relevance).",15
GEVAL Framework,Improving LLM-based evaluation by incorporating AutoCoT steps.,When LLM-based evaluation needs more robust reasoning and justification for its scores.,"Similar to LLMEVAL, but includes AutoCoT steps in the prompt itself. These steps are generated according to the evaluation instructions and inserted into the final prompt, weighting answers according to token probabilities.","Enhances evaluation performance by providing detailed, automatically generated reasoning for quality assessments.","Automated evaluation with reasoning, improving transparency of LLM judgments.",15
ChatEval Framework,Improving LLM-based evaluation through a multi-agent debate framework.,"When a more robust and nuanced evaluation is needed, simulating human-like debate and diverse perspectives.","Uses a multi-agent debate framework, with each agent having a separate role (e.g., different evaluative personas).",Provides more comprehensive and robust evaluations by simulating a debate among different AI personas.,"Advanced LLM evaluation, simulating human debate, multi-perspective assessment.",15
Partial Dependence Plots (PDPs),Understanding the global average effect of one or two features on the predictions of a black-box machine learning model. Complex models obscure the relationship between individual features and the output.,"Model-agnostic global interpretability for black-box classification or regression models, where the overall influence of specific input features on the model's output needs to be visualized.","Compute the marginal effect of a feature (or a small set of features) on the predicted outcome. This is done by averaging the model's predictions over the values of all other features in the dataset, while varying the values of the feature(s) of interest. The average prediction is then plotted as a function of the chosen feature(s).","A visual representation (plot) showing how the model's prediction globally depends on the values of the selected feature(s), providing an average insight into feature effects. This helps in understanding the general trend of how a feature influences the prediction.","Global model understanding, identifying average relationships between features and predictions, model debugging, feature engineering insights, model comparison.",16
Permutation Feature Importance,"Quantifying the global importance of individual features (or groups of features) for the predictions of a black-box machine learning model, without relying on model-specific internal mechanisms.","Model-agnostic global interpretability for black-box classification or regression models, where a global ranking or score of feature relevance is needed to understand which inputs are most influential for the model's performance.","Measure the change in a model's performance metric (e.g., accuracy, F1-score, loss, out-of-bag estimate) when the values of a specific feature (or subset of features) are randomly permuted (shuffled) in the validation or test dataset. A significant drop in performance after permutation indicates high importance for that feature, as the model relied on it.","A score or ranking for each feature, indicating its global importance to the model's predictive performance. This provides an intuitive, global insight into which features the model considers most relevant.","Global feature importance analysis, model debugging, feature selection, understanding model behavior, identifying critical inputs, model comparison.",16
Individual Conditional Expectation (ICE) Plots,"Partial Dependence Plots (PDPs) show average feature effects, which can obscure heterogeneous relationships where a feature affects different instances differently. Users need to understand instance-specific feature effects.","Model-agnostic local interpretability for black-box classification or regression models, when a detailed, instance-specific understanding of how a feature influences predictions is required, especially to detect diverse or conditional relationships.","For each individual instance in a dataset, plot the model's predicted outcome as a function of a single feature's value, while keeping all other features of that specific instance fixed at their original values. This generates a separate line for each instance on the plot.","A set of lines, each representing an individual instance, showing how the prediction for that instance changes with a varying feature. This allows for the detection of diverse relationships and helps identify instances where the model behaves unexpectedly.","Local model understanding, identifying heterogeneous feature effects, debugging individual predictions, exploring 'what-if' scenarios for specific instances, detecting interaction effects.",16
ChainofImages (CoI),Generating images as part of a thought process for visual reasoning.,When visual reasoning tasks can benefit from intermediate visual steps or 'thoughts'.,"A multimodal extension of ChainofThought prompting that generates images (e.g., SVGs) as part of its thought process, using prompts like 'Let's think image by image'.",Enables models to reason visually by generating and using intermediate images.,"Visual reasoning, creative image generation, visual problem-solving.",17
ThreadofThought (ThoT) Prompting,"Improving CoT reasoning, especially in question-answering and retrieval settings with large, complex contexts.",When dealing with extensive and intricate textual contexts for reasoning tasks.,"Use an improved thought inducer for CoT reasoning, such as 'Walk me through this context in manageable parts step by step, summarizing and analyzing as we go.'","Works well in question-answering and retrieval settings, especially with large complex contexts.","Question answering, retrieval-augmented generation, processing long documents.",17
ChainofThought (CoT) Prompting,Enhancing LLM performance in mathematics and reasoning tasks by making the reasoning process explicit.,"When solving problems that require multi-step reasoning, where showing the thought process can guide the LLM.","Leverage fewshot prompting to encourage the LLM to express its thought process before delivering its final answer, typically by including an exemplar with a question, a reasoning path, and the correct answer.",Significantly enhances the LLM's performance in mathematics and reasoning tasks.,"Mathematical problem-solving, logical reasoning, complex question answering.",17
Interleaved Retrieval guided by ChainofThought (IRCoT),Improving multi-hop question answering by dynamically guiding retrieval with CoT and vice-versa.,"When multi-hop questions require both reasoning and retrieval, and their interplay is crucial.",Interleaves CoT and retrieval. IRCoT leverages CoT to guide which documents to retrieve and retrieval to help plan the reasoning steps of CoT.,Enhances performance in multi-hop question answering by creating a synergistic relationship between reasoning and retrieval.,"Multi-hop question answering, complex information retrieval and synthesis.",17
Tabular ChainofThought (TabCoT),Improving the structure and reasoning of LLM outputs for CoT prompts.,When a more organized and structured reasoning output is beneficial for clarity and accuracy.,Use a ZeroShot CoT prompt that instructs the LLM to output its reasoning as a markdown table.,Enables the LLM to improve the structure and thus the reasoning of its output.,"Structured reasoning, data analysis, step-by-step problem solving where clarity is key.",17
MetaReasoning over Multiple CoTs,"Generating a final answer from multiple reasoning chains, even if they don't produce final answers directly.","When multiple CoT reasoning chains are generated, and a consolidated, final answer is needed.","First generate multiple reasoning chains (not necessarily final answers) for a given problem. Next, insert all of these chains into a single prompt template, then generate a final answer from them.","Consolidates diverse reasoning paths into a single, refined answer.","Refining answers from multiple reasoning attempts, complex problem-solving.",17
RecursionofThought,"Solving complicated subproblems within a reasoning chain, especially those that might exceed context length.","When an LLM encounters a complex subproblem during its reasoning process that requires deeper, recursive processing.","Every time a complicated problem is encountered in the middle of its reasoning chain, send this subproblem into another prompt/LLM call. After it's completed, the answer is inserted into the original prompt.","Can recursively solve complex problems, including ones which might otherwise run over the maximum context length, showing improvements on arithmetic and algorithmic tasks.","Solving nested complex problems, handling long context reasoning.",17
Faithful ChainofThought,Generating CoT reasoning that combines natural language with symbolic language for task-dependent problem-solving.,"When a task benefits from both human-readable natural language reasoning and precise, executable symbolic language (e.g., Python).","Generate a CoT that includes both natural language and symbolic language reasoning, making use of different types of symbolic languages in a task-dependent fashion.",Combines the benefits of natural language explanation with the rigor of symbolic computation.,"Mathematical reasoning, logical problem-solving, tasks requiring verifiable computation.",17
ZeroShot CoT,Improving reasoning performance without requiring exemplars.,"When exemplars are unavailable or impractical to include, but the task still benefits from explicit reasoning steps.",Append a thought-inducing phrase like 'Let's think step by step' to the prompt.,"Enhances reasoning capabilities without the need for few-shot examples, making it generally task-agnostic.","General reasoning tasks, quick application of CoT without example curation.",17
Analogical Prompting,Generating effective ChainofThought exemplars automatically for mathematical reasoning and code generation.,When CoT exemplars are needed but manual creation is time-consuming or difficult.,"Automatically generate exemplars that include ChainofThoughts, similar to Self-Generated InContext Learning.",Demonstrated improvements in mathematical reasoning and code generation tasks.,"Automating CoT exemplar creation, improving reasoning and code generation.",17
Chain of Thought (CoT) Prompting,"Large Language Models (LLMs) can struggle with complex, multi-step reasoning tasks, often producing superficial or incorrect answers without showing their intermediate thought process, making debugging difficult.","Tasks requiring logical deduction, multi-hop reasoning, mathematical calculations, or any problem where breaking down the solution into intermediate steps is beneficial for both the LLM's performance and human understanding.","Design prompts that explicitly instruct the LLM to generate its reasoning steps or 'think step-by-step' before providing the final answer. This encourages the model to perform a more structured, multi-stage inference, simulating a human-like reasoning process.","Often leads to improved accuracy on complex reasoning tasks by breaking down the problem into manageable steps. Provides transparency into the LLM's thought process, making outputs more explainable and easier to debug.","Enhancing LLM reasoning capabilities, improving performance on complex question answering, generating explainable AI outputs, facilitating problem-solving in various domains.",17
Tree of Thoughts (ToT) / Graph of Thoughts (GoT),"Linear reasoning approaches (like Chain-of-Thought) can be insufficient for problems requiring exploration of multiple reasoning paths, evaluation of intermediate thoughts, or backtracking to find an optimal solution.","An agent needs to solve complex problems that benefit from exploring diverse reasoning trajectories, evaluating the quality of intermediate thoughts, and potentially backtracking to more promising paths.","Represent the reasoning process as a tree or graph, where nodes are individual thoughts or states, and edges represent transitions between them. The agent can generate multiple 'thoughts' at each step, evaluate their potential, and prune less promising branches, allowing for more deliberate, robust, and exhaustive problem-solving through search.","Improves the agent's ability to solve complex problems that require exploration, evaluation, and backtracking, leading to more robust and accurate solutions compared to linear reasoning.","Deliberate problem solving with large language models, exploring complex search spaces for optimal solutions (though noted as prohibitively costly for TravelPlanner in this paper).",17
Automatic Directed CoT (AutoDiCoT),"Generating explanations for LLM reasoning, especially for incorrect labels, and using these to improve subsequent prompts.","During prompt engineering, when trying to understand why an LLM mislabels an item and how to correct its behavior.","An algorithm that automatically directs the Chain-of-Thought (CoT) process to reason in a particular way. For each development item, it labels it, then prompts the model to generate a reasoning chain. If the model labels incorrectly, it's prompted with 'It is actually [is/is not] entrapment, please explain why' to generate a corrective reasoning chain. These generated CoTs can then be used as exemplars (including 'bad reasoning' examples) in subsequent prompts.","Combines automatic CoT generation with showing examples of bad reasoning, leading to improved prompt performance and understanding of LLM behavior.","Prompt engineering, debugging LLM reasoning, automated exemplar generation for CoT.",17
Step-by-Step Reasoning (Chain-of-Thought / CoT),"LLMs can struggle with complex reasoning tasks, especially when direct answers require multiple logical steps or when the reasoning process needs to be explicit.","An LLM needs to perform a multi-step reasoning process to arrive at a solution, to justify its actions, or to guide subsequent steps in a complex task.","Prompt the LLM to generate intermediate reasoning steps, explaining its thought process before providing the final answer or action. This can be done with specific prompting techniques, such as adding 'Let's think step by step' (Zero-Shot Chain-of-Thought, ZSCoT).","Improves the LLM's reasoning capabilities, makes its decision-making process more transparent, and often leads to more accurate and robust solutions by guiding the model through a logical progression.","Enhancing the reasoning process in planning, as demonstrated by the ZSCoT strategy, to improve problem-solving accuracy.",17
Chain-of-Thought Finetuning,"Training LLMs with only concise question-answer pairs can lead to superficial learning, overfitting to short answers, and a reduced ability to reason or explain answers, especially in complex tasks requiring multi-step inference or grounding in context.","Finetuning LLMs for question-answering or reasoning tasks where transparency, explainability, and robust understanding are desired. This is particularly relevant in RAG settings where answers need to be logically derived from and grounded in provided context.","Prepare finetuning data by generating not just the final answer, but also a detailed Chain-of-Thought (CoT) reasoning process that logically leads to the answer. This reasoning should explicitly reference or cite the source context when applicable (e.g., verbatim quotations). The LLM is then trained to produce these CoT responses alongside the final answer.","Significantly enhances training robustness, improves overall accuracy, and prevents overfitting to concise answers. It guides the model to a deeper understanding of the problem and its solution, improving its ability to reason and ground answers in context.","Enhancing the reasoning capabilities, explainability, and accuracy of LLMs by explicitly teaching them to generate step-by-step logical derivations, making their outputs more transparent and verifiable.",17
Multimodal GraphofThought,Extending GraphofThought reasoning to the multimodal setting.,"When multimodal inputs (e.g., image + question) require complex, graph-based reasoning.","Extends GraphofThought to the multimodal setting. At inference time, the input prompt is used to construct a thought graph, which is then used along with the original prompt to generate a rationale to answer the question. When an image is input, an image captioning model generates a textual description, appended to the prompt for visual context.",Enables graph-based reasoning for multimodal problems by integrating visual context.,"Multimodal reasoning, complex visual-linguistic problem-solving.",17
Duty Distinct ChainofThought (DDCoT),Extending LeasttoMost prompting to the multimodal setting for complex reasoning.,"When multimodal problems require decomposition and sequential solving, similar to LeasttoMost for text.","Extends LeasttoMost prompting to the multimodal setting, creating sub-questions, then solving them, and combining the answers into a final response.","Enables structured, decomposed reasoning for multimodal problems.","Multimodal reasoning, complex visual-linguistic problem-solving.",17
ChainofVerification (COVE),Reducing hallucination in large language models by verifying answers with related questions.,"When an LLM generates an answer, and its correctness needs to be verified to reduce hallucinations.","First, use an LLM to generate an answer to a given question. Then, create a list of related questions that would help verify the correctness of the answer. Each question is answered by the LLM, then all the information is given to the LLM to produce the final revised answer.",Shown improvements in various question-answering and text-generation tasks by reducing hallucination.,"Reducing hallucinations, improving factual accuracy in Q&A and text generation.",17
Uncertainty-Routed CoT Prompting,Improving reasoning accuracy by handling uncertainty in reasoning paths.,"When multiple CoT reasoning paths are possible, and a robust selection mechanism is needed.","Sample multiple CoT reasoning paths, then select the majority if it is above a certain threshold (calculated based on validation data). If not, sample greedily and select that response.",Demonstrates improvement on benchmarks like MMLU for both GPT-4 and Gemini Ultra models.,"Improving reasoning accuracy, robust decision-making in uncertain reasoning scenarios.",17
Contrastive CoT Prompting,Showing the LLM how *not* to reason to improve its performance.,When the LLM struggles with common reasoning pitfalls or requires explicit guidance on incorrect reasoning paths.,Add both exemplars with incorrect and correct explanations to the CoT prompt.,Shown significant improvement in areas like Arithmetic Reasoning and Factual QA.,"Improving reasoning accuracy, correcting common errors, teaching 'anti-patterns' of thought.",17
Reversing ChainofThought (RCoT),Detecting factual inconsistencies in reasoning by reconstructing the problem from the answer.,"When an LLM generates an answer and reasoning, and there's a need to verify its factual consistency against the original problem.","First, prompt LLMs to reconstruct the problem based on the generated answer. Then, generate fine-grained comparisons between the original problem and the reconstructed problem to check for inconsistencies. These inconsistencies are converted to feedback for the LLM to revise the generated answer.",Detects and rectifies factual inconsistencies in reasoning.,"Verifying factual consistency, improving reasoning accuracy.",17
Automatic ChainofThought (AutoCoT) Prompting,Automatically generating chains of thought to build FewShot CoT prompts.,"When manual creation of CoT exemplars is burdensome, and an automated approach is desired.",Use a ZeroShot prompt to automatically generate chains of thought. These generated CoTs are then used to build a FewShot CoT prompt for a test sample.,"Automates the creation of CoT exemplars, simplifying FewShot CoT implementation.","Automating CoT exemplar creation, scaling FewShot CoT applications.",17
TreeofThought (ToT),Solving tasks that require search and planning by exploring multiple reasoning paths.,When a problem has multiple possible intermediate steps or 'thoughts' and requires evaluation of progress towards a solution.,"Create a tree-like search problem by starting with an initial problem, generating multiple possible steps (thoughts) as from a CoT, evaluating the progress each step makes towards solving the problem through prompting, and deciding which steps to continue with.",Particularly effective for tasks that require search and planning.,"Complex problem-solving, strategic decision-making, search-intensive tasks.",17
PlanandSolve Prompting,Generating more robust reasoning processes than standard ZeroShot CoT.,When a problem requires a structured approach of understanding and planning before execution.,"Use an improved ZeroShot CoT prompt: 'Let's first understand the problem and devise a plan to solve it. Then, let's carry out the plan and solve the problem step by step.'",Generates more robust reasoning processes than standard ZeroShot CoT on multiple reasoning datasets.,"General reasoning tasks, structured problem-solving.",17
Complexity-based Prompting,Improving CoT performance for multi-step reasoning by focusing on complex examples and robust aggregation.,"When dealing with mathematical reasoning or other tasks requiring complex, multi-step thought processes.","1. Select complex examples for annotation and inclusion in the prompt based on factors like question length or reasoning steps. 2. During inference, sample multiple reasoning chains/answers and use a majority vote among chains exceeding a certain length threshold, assuming longer reasoning indicates higher answer quality.",Shown improvements on mathematical reasoning datasets.,"Mathematical reasoning, complex problem-solving, enhancing CoT robustness.",17
Chain-of-Thought (CoT) Prompting,"LLMs often struggle with complex reasoning tasks, providing direct answers without showing their intermediate thought process, which can lead to errors and make debugging difficult.","LLMs possess latent reasoning abilities but may not explicitly leverage them for multi-step problems, especially in zero-shot settings.",Augment the prompt with instructions or examples that encourage the LLM to generate a series of intermediate reasoning steps (a 'chain of thought') before producing the final answer. A common instruction is 'Let's think step by step.',"Elicits and makes explicit the LLM's reasoning process, improving performance on complex tasks, particularly those requiring multi-step logic or arithmetic, and can aid in debugging.","Enhancing reasoning in LLMs, solving mathematical word problems, multi-step logical puzzles, complex question answering.",17
Chain-of-Thought (CoT) Prompting,"Large Language Models (LLMs) often struggle with complex multi-step reasoning tasks, leading to incorrect or superficial answers when prompted directly.","Improving the reasoning capabilities of LLMs, particularly for tasks requiring logical deduction or step-by-step problem-solving.",Instruct the LLM to generate a series of intermediate reasoning steps or a 'thought process' in natural language before providing the final answer. This guides the model through the problem-solving process.,"Elicits more robust and accurate reasoning, allowing LLMs to tackle more complex problems by breaking them down into manageable sub-problems. Improves performance on mathematical reasoning and other complex language tasks.",Enhancing LLM performance on complex reasoning tasks by making the model's thought process explicit and sequential.,17
Tree-of-Thoughts (ToT),Linear Chain-of-Thought (CoT) reasoning might be insufficient for problems requiring exploration of multiple reasoning paths or backtracking.,"Complex problems where the optimal reasoning path is not immediately obvious, and alternative reasoning branches need to be explored.","Expands the reasoning process from a linear chain to a tree structure. LLMs generate multiple possible next steps ('thoughts') at each stage, allowing for branching and exploration of different reasoning trajectories. This enables more deliberate problem-solving.","Allows LLMs to explore a wider range of reasoning paths, potentially leading to more robust and accurate solutions for elaborate problems.","Strategic game playing, complex logical puzzles, creative problem-solving, tasks requiring exploration of multiple hypotheses.",17
Chain-of-Thought Reasoning,"Large Language Models (LLMs) may struggle with complex reasoning tasks, often producing incorrect or incomplete answers without explicitly showing their intermediate thought processes, making their outputs less reliable and interpretable.","Tasks requiring multi-step reasoning, problem decomposition, logical inference, or complex problem-solving, especially within multi-hop Question Answering (QA) or other knowledge-intensive scenarios where intermediate steps are crucial.","The LLM is prompted or fine-tuned to generate a sequence of intermediate reasoning steps (a 'chain of thought') before arriving at the final answer. This process makes the LLM's reasoning explicit and can be interleaved with other operations, such as document retrieval in multi-step RAG approaches.","Elicits and enhances the reasoning capabilities of LLMs, leading to improved accuracy on complex tasks, better problem decomposition, and more interpretable outputs by revealing the logical steps taken to reach a conclusion.","Complex problem-solving, mathematical reasoning, multi-hop question answering, code generation, any task requiring explicit logical steps or transparent reasoning.",17
Chain-of-Thought (CoT) Prompting,"Large Language Models (LLMs) often struggle to perform complex multi-step reasoning, leading to incorrect or incomplete answers.","Tasks requiring complex reasoning, problem-solving, or multi-step logical deduction.",Prompt LLMs to generate a series of intermediate reasoning steps or a 'chain of thought' before providing the final answer. This explicit step-by-step reasoning process helps LLMs to break down complex problems and follow a logical path.,"Elicits and improves the reasoning abilities of LLMs, leading to more accurate and coherent responses for complex tasks.","Complex arithmetic, symbolic reasoning, common-sense reasoning, multi-hop question answering.",17
Graph-of-Thoughts (GoT),"Representing and synergizing complex, interconnected reasoning steps that go beyond linear chains or simple tree structures.","Elaborate problems where reasoning steps might have complex dependencies, require aggregation of information from multiple paths, or involve non-sequential relationships.","Models the reasoning process as a graph structure, where nodes represent 'thoughts' (intermediate reasoning steps or states) and edges represent transitions or dependencies between them. It includes aggregation operations to combine information from different reasoning paths within the graph.",Enables LLMs to solve elaborate problems by synergizing diverse reasoning paths and handling complex interdependencies between thoughts.,"Highly complex problem-solving, tasks requiring synthesis of information from multiple reasoning branches, advanced logical deduction.",17
PairedImage Prompting,Demonstrating image transformations to a model for it to perform similar conversions on new images.,"When an image transformation task needs to be learned by example, similar to few-shot learning for text.","Show the model two images: one before and one after some transformation. Then, present the model with a new image for which it will perform the demonstrated conversion. This can be done with or without textual instructions.",Enables the model to learn and apply image transformations from examples.,"Image editing, style transfer, visual transformations.",18
Segmentation Prompting,"Performing segmentation tasks (e.g., semantic segmentation) using prompts.",When a model needs to identify and delineate specific objects or regions within an image.,Use prompts to guide the model in performing segmentation tasks.,Enables flexible and prompt-driven segmentation.,"Image segmentation, computer vision tasks.",18
ImageasText Prompting,Including images or multiple images easily within a text-based prompt.,"When multimodal prompts are needed, but the primary interaction is text-based.","Generate a textual description of an image, which then allows for the easy inclusion of the image (or multiple images) in a text-based prompt.",Facilitates multimodal prompting by converting visual information into a text-compatible format.,"Multimodal reasoning, image captioning, integrating visual context into text prompts.",18
Negative Prompting,"Preventing the generation of undesired elements in multimodal outputs (e.g., images).","When generating images or other multimodal content, and specific undesirable features might appear.","Numerically weight certain terms in the prompt so that the model considers them less heavily than others. For example, negatively weighting 'bad hands' to avoid anatomically inaccurate hands.",Models are more likely to generate outputs without the negatively weighted elements.,"Image generation, multimodal content creation, fine-tuning generative outputs.",18
3D Prompting,Generating or manipulating 3D content using prompts.,"When creating 3D objects, textures, or animating 3D scenes.","Use prompts (text, image, user annotation, bounding boxes, points, lines, 3D objects) as input to guide 3D object synthesis, 3D surface texturing, and 4D scene generation.",Enables prompt-driven creation and manipulation of 3D content.,"3D content creation, virtual reality, game development.",18
Prompt Modifiers,Changing the resultant image or other multimodal output in a specific way.,"When generating images, videos, or other multimodal content, and fine-grained control over specific attributes (e.g., medium, lighting) is desired.","Append specific words or phrases to a prompt to modify the output. Examples include 'Medium (e.g., on canvas)' or 'Lighting (e.g., a well lit scene)'.","The generated multimodal output incorporates the specified modifications, allowing for more precise control over the creative process.","Image generation, video generation, multimodal content creation, artistic control.",18
Unified Retrieval and Reasoning,"Traditional knowledge-intensive tasks often separate knowledge retrieval and reasoning into distinct, sequential stages, leading to suboptimal performance due to information loss or uncoordinated optimization.",Knowledge Graph Question Answering (KGQA) or other complex knowledge-intensive tasks where both identifying relevant knowledge from a structured source (like a KG) and performing inference based on that knowledge are critical.,"Integrate the knowledge retrieval and reasoning processes into a single, cohesive model or framework, often leveraging Large Language Models (LLMs). This unification allows for joint optimization, where the retrieval mechanism is informed by and contributes directly to the reasoning process, and vice-versa. Examples include models that unify graph retrieval and reasoning into a single LLM, or those that combine semantic parsing with LLM reasoning to jointly generate answers.","Achieves state-of-the-art performance by enabling a more synergistic interaction between knowledge access and inference, leading to more accurate and coherent answers. It overcomes limitations of sequential, decoupled approaches.","Complex KGQA, multi-hop question answering, knowledge-intensive dialogue systems, tasks requiring deep integration of structured knowledge with LLM capabilities.",19
Semantic Parsing for Knowledge Graph Question Answering (KGQA),"Directly obtaining accurate and interpretable answers from Knowledge Graphs (KGs) using natural language questions, while leveraging the structured nature of KGs.","Knowledge Graph Question Answering (KGQA) tasks where precise, executable queries are desired for reasoning over KGs.","Use Large Language Models (LLMs) to convert natural language questions into formal logical queries (e.g., SPARQL queries). These logical queries are then executed on the Knowledge Graph by a query engine to retrieve the exact answers.","Can generate more accurate and interpretable results by directly leveraging reasoning on KGs, but is limited by the executability and syntax of generated queries.","KGQA requiring high precision and interpretability, tasks where the underlying KG structure needs to be directly queried.",19
Knowledge-Driven Chain-of-Thought (KDCoT) / Retrieval-Enhanced Reasoning (RR),"Large Language Models (LLMs) suffer from hallucinations and a lack of up-to-date knowledge, which diminishes the faithfulness of their reasoning, even with Chain-of-Thought prompting.","Knowledge-intensive question answering and reasoning tasks where external, factual knowledge is essential for accurate and faithful LLM responses.","Integrates knowledge retrieval from external sources, specifically Knowledge Graphs (KGs), into the Chain-of-Thought reasoning process. Relevant knowledge is retrieved from KGs and then used to produce or guide the generation of faithful reasoning plans or steps for LLMs.","Improves the faithfulness and accuracy of LLM reasoning by grounding it with external, reliable knowledge, mitigating hallucinations and knowledge gaps.","Knowledge-intensive QA, factual reasoning, reducing hallucinations in CoT, grounding LLM explanations.",19
Faithful Reasoning with Verifier (Entailer),"Large Language Models (LLMs) can generate unfaithful or untruthful reasoning steps, leading to unreliable conclusions.","Scenarios where the faithfulness and truthfulness of each reasoning step are critical, such as in high-stakes applications or when building trust in AI systems.","Introduces a separate 'verifier' component that validates the reasoning steps generated by the LLM. This verifier checks the logical consistency, factual accuracy, or entailment of each step, ensuring faithfulness.","Improves the faithfulness and truthfulness of LLM-generated reasoning chains, enhancing the reliability and trustworthiness of the overall reasoning process.","High-stakes decision support, scientific discovery, legal reasoning, medical diagnosis, any application requiring verifiable AI reasoning.",19
Instruction Tuning for LLM-KG Integration,"Large Language Models (LLMs) typically lack inherent knowledge of Knowledge Graph (KG) structures and relations, making it difficult for them to directly generate KG-grounded plans or reason effectively on retrieved KG paths.","When integrating LLMs with KGs for reasoning tasks (e.g., KGQA) and needing the LLM to understand and utilize KG-specific information and structures.",Fine-tune the LLM using specific instruction-following tasks designed to distill knowledge from KGs and teach the LLM how to interact with KG information. This involves two main optimization tasks: 1) Planning Optimization: Training the LLM to generate faithful relation paths (plans) that are grounded by the KG. 2) Retrieval-Reasoning Optimization: Training the LLM to effectively conduct reasoning based on retrieved KG reasoning paths.,"Equips LLMs with the ability to generate KG-grounded plans and perform faithful reasoning based on retrieved KG paths, significantly improving performance in KG-related tasks.","Adapting LLMs for KG-specific tasks, improving LLM understanding of structured knowledge, enhancing LLM's ability to generate KG-compliant outputs.",19
"KG-Agent Framework (KGAgent, ThinkonGraph)","Large Language Models (LLMs) need to dynamically access, query, and reason over structured knowledge in Knowledge Graphs (KGs) to perform complex tasks that require up-to-date and structured information.","Complex reasoning tasks, question answering, or decision-making processes that benefit from dynamic interaction with a structured knowledge base.","Treats LLMs as autonomous agents that can interact with Knowledge Graphs (KGs) through prompting. The LLM agent generates actions (e.g., queries to the KG, navigation commands) to retrieve specific, latest knowledge from the KG, and then uses this retrieved information to refine its reasoning or generate responses.","Enables LLMs to perform complex reasoning by dynamically accessing and leveraging structured knowledge from KGs, overcoming static knowledge limitations and improving reasoning accuracy.","Complex KGQA, knowledge discovery, interactive reasoning systems, dynamic knowledge retrieval.",19
Knowledge Traceability and Correctability via Explicit Reasoning Paths,"LLMs often lack transparency, explainability, and responsibility, making it difficult to understand their reasoning, identify sources of errors (e.g., hallucinations, outdated knowledge), or update underlying knowledge effectively.","LLM-based systems, particularly those performing knowledge-intensive tasks, where trust, verifiability, and the ability for human oversight and correction of AI outputs and their underlying knowledge are critical.","Design the AI system to generate and expose explicit, step-by-step reasoning paths (e.g., sequences of entity-relation-entity triples from a KG) that lead to the LLM's answer. This allows users or experts to: 1. Trace: Review the exact knowledge provenance and logical steps taken by the LLM. 2. Localize Errors: Pinpoint specific erroneous or outdated triples within the reasoning path. 3. Correct: Provide feedback or directly modify the identified incorrect knowledge in the external Knowledge Graph, thereby improving the system's future reasoning accuracy and facilitating knowledge infusion.","Enhances the explainability and transparency of LLM reasoning, enables human-in-the-loop verification and correction, reduces hallucination by allowing external validation, and provides a mechanism for continuous improvement and updating of the underlying knowledge base.","Fact-checking, critical decision-support systems, knowledge base curation, building trust in AI systems, and any application where verifiable and debuggable AI reasoning is paramount.",19
Formalism-Enhanced Reasoning,"While LLM-based agents are proficient in natural language reasoning, complex reasoning tasks may require more structured, precise, and robust methods than plain natural text.","Complex reasoning tasks, especially those involving logical inference, mathematical computation, or structured knowledge representation, where the inherent ambiguity or less structured nature of natural language can hinder performance.","Incorporate external formalisms, such as mathematical tools (e.g., probabilistic graph models) and non-natural language forms, to facilitate and enhance agents' reasoning and planning capabilities. This integrates structured computational or logical frameworks with the LLM's language understanding.","Significantly enhanced performance in complex reasoning tasks, improved decision-making capabilities, and increased controllability of intelligent agents.","Multi-agent reasoning dynamics (e.g., using PGM), integrating intelligent agents into Robotic Process Automation (RPA) for enhanced intelligence and controllability.",19
LLM-based Topic Entity Extraction,"Identifying the core entities in a natural language question that serve as starting points for knowledge graph exploration can be challenging for traditional methods, especially with complex or ambiguous phrasing, or when entity linking is not robust.",The initial step of any knowledge graph-based question answering or reasoning system where the starting nodes for graph traversal need to be programmatically identified from a natural language query.,"Leverage the LLM's natural language understanding capabilities to automatically extract the most relevant topic entities from the input question. This involves prompting the LLM to identify and list entities or key concepts. The system then uses these extracted entities as initial nodes for subsequent graph search (e.g., beam search).","Provides robust and semantically aware initialization for KG exploration, improving the relevance of the starting points for reasoning paths. Reduces the need for complex, separate entity linking or disambiguation modules.","Knowledge Base Question Answering (KBQA), information retrieval from KGs, semantic search, and any LLM-KG integration requiring intelligent starting point identification.",19
Plug-and-Play LLM-KG Integration,"Integrating LLMs with external knowledge sources (KGs) often requires complex fine-tuning, specific data formats, or rigid architectures, limiting flexibility, generalizability, and increasing deployment costs. Updating knowledge is also slow and expensive if tied to LLM retraining.","Building LLM-enhanced systems that need to adapt to various LLM backbones, different Knowledge Graphs, and evolving prompting strategies without significant re-engineering or retraining.","Design an algorithmic framework (like ToG) that abstracts the interaction with both the LLM and the KG. The framework uses generic interfaces for LLM calls (prompts) and KG queries (predefined formal queries or APIs), allowing different LLMs (e.g., ChatGPT, GPT-4, Llama2) and KGs (e.g., Freebase, Wikidata) to be swapped in and out seamlessly. Knowledge updates are handled primarily within the KG, not by retraining the LLM.","High flexibility and generality across different LLMs and KGs. Reduced training costs and faster knowledge updates. Enables smaller, cheaper LLMs to achieve competitive performance by leveraging external knowledge.","Any application where LLMs need to be augmented with external, structured knowledge, and where adaptability to different underlying models or knowledge sources is crucial.",19
Reasoning on Graphs (RoG) / Planning-Retrieval-Reasoning Framework,"Large Language Models (LLMs) lack up-to-date knowledge and experience hallucinations during reasoning, leading to incorrect processes and diminished trustworthiness. Existing Knowledge Graph (KG)-based LLM reasoning methods often overlook the importance of KG structural information.","Complex reasoning tasks, particularly Knowledge Graph Question Answering (KGQA), where faithful and interpretable reasoning is required, and LLMs alone are insufficient due to knowledge limitations and hallucination tendencies.",A framework that synergizes LLMs with KGs. It involves three main steps: 1) A Planning Module where LLMs generate relation paths grounded by KGs as faithful plans. 2) A Retrieval Module that uses these plans to retrieve valid reasoning paths (instances of relation paths) from the KGs. 3) A Reasoning Module where LLMs conduct faithful reasoning based on the retrieved reasoning paths and generate answers with interpretable explanations. The entire process is optimized through instruction tuning on both planning and retrieval-reasoning tasks.,"Achieves state-of-the-art performance on KG reasoning tasks, generates faithful and interpretable reasoning results, and allows seamless plug-and-play integration with any arbitrary LLMs during inference.","Knowledge Graph Question Answering (KGQA), complex reasoning tasks requiring external, structured knowledge, scenarios demanding high faithfulness and interpretability from LLMs.",19
Triple-Based Path Representation in Prompts,"Effectively conveying complex, multi-hop reasoning paths extracted from a Knowledge Graph to a Large Language Model within a prompt can be challenging. Different representation formats (e.g., natural language sentences, simple sequences) can lead to varying levels of LLM comprehension, processing efficiency, and prompt length, potentially degrading performance or exceeding token limits.","An LLM-KG reasoning system (such as ThinkonGraph) where the LLM needs to interpret and reason over structured knowledge graph paths (sequences of entities and relations) that have been retrieved or constructed. The goal is to present this structured information to the LLM in a format that maximizes its ability to understand, evaluate, and utilize the knowledge for generating accurate answers.","Represent the discovered reasoning paths from the Knowledge Graph as a series of explicit entity-relation-entity (triple) formats within the LLM prompt. For example, instead of a natural language sentence, use a structured representation like 'Canberra capital of Australia; Australia prime minister Anthony Albanese'. This format directly reflects the underlying structure of the KG and provides clear, unambiguous pieces of knowledge.","Leads to higher efficiency and superior performance in LLM reasoning compared to less structured representations (e.g., converting triples into natural language sentences or simple sequences). This structured format helps the LLM to more accurately parse and reason over the knowledge, avoids excessively lengthy or ambiguous prompts, and contributes to better overall answer generation.","Knowledge Base Question Answering, fact-checking, and any LLM-KG integration where structured knowledge needs to be precisely communicated to the LLM for reasoning and answer generation.",19
ThinkonGraph (ToG) Algorithmic Framework,"Implementing the LLM-KG Tight-Coupling Paradigm effectively for deep, responsible, and efficient LLM reasoning on knowledge graphs, requiring a structured approach to iterative exploration and decision-making.","An LLM-KG system where the LLM needs to dynamically navigate a Knowledge Graph to discover and evaluate multi-hop reasoning paths to answer complex questions, and where explainability and efficiency are important.","An algorithmic framework that leverages an LLM as an agent to perform iterative beam search on a KG. The process involves three phases: 1. Initialization: The LLM extracts initial topic entities from the input question. 2. Exploration: Iteratively, the LLM performs a two-step beam search (Search and Prune) to identify and select the most relevant relations and then entities, extending the top-N reasoning paths on the KG. 3. Reasoning: The LLM evaluates if the current reasoning paths are sufficient to answer the question. If yes, it generates the answer; otherwise, it repeats exploration or falls back to its inherent knowledge if the maximum depth is reached.","Achieves state-of-the-art performance in various knowledge-intensive tasks, significantly enhances LLMs' deep reasoning capabilities, provides explicit reasoning paths for traceability and correctability, and offers a flexible, training-free, and computationally efficient solution.","Multi-hop Knowledge Base Question Answering (KBQA), open-domain question answering, fact-checking, and any task requiring LLMs to perform structured, verifiable reasoning over KGs.",19
LLM-KG Tight-Coupling Paradigm,"Large Language Models (LLMs) often struggle with hallucination, deep and responsible reasoning, and keeping knowledge up-to-date, especially for tasks requiring specialized or multi-hop knowledge. Existing loose-coupling approaches (LLM -> KG query -> LLM) are limited by KG completeness and treat the LLM merely as a translator, not a direct participant in graph reasoning.","Scenarios demanding accurate, explainable, and verifiable answers to complex knowledge-intensive questions, where LLMs need to leverage structured, explicit, and editable knowledge from Knowledge Graphs (KGs) beyond their pre-trained data.","Integrate LLMs and KGs in a tight-coupling manner, where the LLM acts as an agent that interactively explores related entities and relations on KGs and performs reasoning based on the retrieved knowledge. The LLM dynamically participates in each step of graph reasoning, complementing the KG's capabilities.","Significantly enhances LLMs' deep reasoning power, provides knowledge traceability and correctability, mitigates hallucination, and offers a flexible, plug-and-play framework for different LLMs and KGs without additional training costs. It can also enable smaller LLMs to achieve performance competitive with larger ones.","Complex knowledge-intensive Question Answering (KBQA), fact-checking, open-domain QA, and any application requiring LLMs to perform verifiable, multi-hop reasoning over structured knowledge.",19
Iterative Prompting for Guided KG Exploration and Reasoning,"How to effectively orchestrate an LLM to perform a sequence of distinct, iterative sub-tasks (e.g., selecting relevant information, evaluating progress, generating final output) within a complex, multi-step AI workflow that interacts with external structured data like a Knowledge Graph.","An LLM-KG system (such as ToG) where the LLM acts as an agent, requiring precise instructions at various stages of an iterative reasoning process to guide graph exploration, make decisions, and synthesize information.","Employ a set of specialized, context-dependent prompts, each meticulously designed to elicit a specific behavior or decision from the LLM at different points in the iterative loop. These prompts include: 1. Relation Prune Prompt: Guides the LLM to identify and score relevant relations from a candidate set. 2. Entity Prune Prompt: Directs the LLM to score the contribution of candidate entities. 3. Reasoning Prompt: Asks the LLM to evaluate the sufficiency of the current reasoning paths for answering the question. 4. Generate Prompt: Instructs the LLM to synthesize the final answer based on the accumulated knowledge and reasoning paths.","Enables the LLM to function as an intelligent, adaptable agent, dynamically guiding the KG exploration and reasoning process. This leads to more accurate, context-aware decisions and ultimately, more reliable and responsible answers by breaking down complex tasks into manageable, LLM-executable steps.","Any complex, multi-step AI task where an LLM needs to interact with external tools or data sources, make iterative decisions, and perform structured reasoning.",19
LLM Fallback to Inherent Knowledge,"When external knowledge retrieval (e.g., Knowledge Graph exploration) fails to yield sufficient information within predefined limits (e.g., maximum search depth), the system might fail to answer or provide an incomplete response, leading to user dissatisfaction or system unreliability.","An iterative LLM-based reasoning system that primarily relies on external, structured knowledge (like a Knowledge Graph) but where the LLM also possesses a vast amount of inherent knowledge from its pre-training. This system needs a robust strategy for handling cases where external knowledge is incomplete, outdated, or inaccessible for a given query.","Design the system to include a fallback mechanism where, if the primary external knowledge exploration process (e.g., KG beam search) does not successfully gather enough information to answer the question within its operational limits (e.g., maximum search depth reached, no relevant paths found), the LLM is then prompted to generate an answer based *exclusively* on its own inherent, pre-trained knowledge.","Enhances the robustness and coverage of the system, allowing it to provide an answer even when the targeted external knowledge is insufficient or unavailable. This prevents outright failures or 'refuse to answer' scenarios, though the quality, traceability, and correctness of the fallback answer might be lower than KG-backed responses.","Question Answering systems, conversational AI, and any LLM-augmented application where a graceful degradation of performance is preferred over outright failure when external data sources are insufficient.",19
Inherently Interpretable Models (Interpretability by Design),"High-performing machine learning models are often black-boxes, lacking transparency and interpretability, which is critical in high-stakes applications and can hinder trust and accountability.","Developing machine learning models for critical applications (healthcare, criminal justice, finance) where understanding the model's internal logic and decision-making process is paramount, and a perceived trade-off between accuracy and interpretability exists.","Design and train models that are intrinsically transparent and understandable to humans. This can involve using model architectures that are simple by nature (e.g., Decision Trees, Rule-based models, Linear Models, Generalized Additive Models, K-Nearest Neighbors, Naive Bayes) or by incorporating interpretability criteria directly into the model's optimization problem during training (e.g., minimizing model complexity like number of leaves or rules).","Models that provide direct insights into their decision-making process, fostering trust, enabling error analysis, and facilitating fairness assessment, often overcoming the perceived accuracy-interpretability tradeoff by optimizing for both.","Building trustworthy AI systems, satisfying regulatory requirements (e.g., GDPR), knowledge discovery, error analysis and debugging, fairness assessment, transferability, model comparison.",20
Interactive Tool Use Trajectory Curation,Training tool-integrated agents is challenging due to the absence of interactive tool-use annotations in existing mathematical reasoning datasets.,Preparing high-quality training data for LLMs designed to perform tool-integrated reasoning.,"Utilize a powerful LLM (e.g., GPT4) to synthesize interactive tool-use trajectories. This involves crafting detailed prompts with few-shot examples demonstrating the interleaved rationale-program-output format, then using the LLM with greedy decoding and nucleus sampling to generate trajectories, filtering for correct answers and absence of tool-use errors.","Creation of TORACORPUS, a dataset of 16k high-quality interactive tool-use annotations for mathematical problems (GSM8k and MATH), enabling the training of TORA models.","Generating synthetic, interactive tool-use training data for LLMs when human-annotated datasets are unavailable or insufficient.",21
Curriculum Tool Learning,"Introducing models directly to complex tools and tasks can be overwhelming and inefficient, hindering effective learning and generalization.","Training AI models to use complex tools or perform intricate tasks, where a structured, progressive learning approach can be beneficial.","Employ a pedagogical strategy that starts with simple tools and tasks, gradually introducing the model to more complex ones. This allows the model to build upon its prior knowledge, develop a deeper understanding of the tool's essential features, and progressively master advanced concepts. This can be combined with transfer learning and multi-task learning.","More effective and manageable learning of complex tool functionalities, improved ability to identify similarities and differences between situations, enhanced adaptability, and better generalization across different tools and tasks.","Teaching models to use mathematical software (e.g., Mathematica) starting with basic operations and progressing to calculus, learning to perform simple tasks (e.g., sorting) before complex ones (e.g., solving linear equations).",21
Teacher-Assisted Trajectory Correction,"Sampled trajectories from a student model may contain errors or be incomplete, limiting the diversity and quality of training data for complex interactive tool-use behaviors.","Enhancing the training dataset for LLMs by correcting and diversifying synthetically generated interactive tool-use trajectories, especially when the initial samples from a student model are imperfect.","For invalid or incomplete trajectories sampled from an imitation-learned model (student model), identify the plausible initial segments. A more capable 'teacher model' (e.g., a larger or more refined LLM) is then used to complete or correct the subsequent steps of these trajectories through greedy decoding, generating new, valid, and diverse training examples.","Significantly boosts reasoning performance, particularly for smaller models, by expanding the training data with high-quality, corrected reasoning paths, thereby shaping the model's output space and reducing improper tool-use behavior.","Improving the robustness and diversity of LLM reasoning, especially in interactive tool-use scenarios, by leveraging a teacher model for data refinement.",21
Few-Shot Prompting for Structured Trajectory Generation,"Generating high-quality, structured, and interactive tool-use trajectories from a powerful LLM (e.g., GPT-4) for data curation can be challenging without clear guidance on the desired format and interaction flow.","Leveraging a capable LLM to synthesize complex, multi-modal (natural language and code) interactive demonstrations for training smaller models or for data augmentation, where the output needs to follow a specific interleaved structure.","Craft a detailed prompt that includes explicit instructions on the desired interleaved format (e.g., natural language rationale, program, tool output, next rationale) and provides several diverse, high-quality few-shot examples. These examples serve as demonstrations, guiding the LLM to generate new trajectories that adhere to the specified structure and effectively showcase tool integration and reasoning steps.","Improves the success rate and quality of synthetic data generation, yielding structured and high-quality interactive tool-use trajectories that are crucial for training tool-integrated agents.","Generating synthetic training data for LLMs, especially for complex tasks requiring structured, multi-modal outputs and interactive behaviors.",21
Output Space Shaping,"Imitation learning on datasets with limited valid trajectories (e.g., mostly one per question) restricts a model's output space, hindering its flexibility in exploring plausible reasoning paths during testing and leading to improper tool-use behavior.",Refining the reasoning behavior and improving the robustness of LLMs trained with imitation learning on interactive tool-use trajectories.,"1. Sampling: Generate diverse trajectories by applying nucleus sampling to the imitation-learned model, retaining valid ones. 2. Correction: For invalid trajectories, identify plausible preceding portions and use a teacher model to complete and correct the subsequent steps, generating new valid trajectories. 3. Retraining: Retrain the model on the combined dataset of the initial corpus, sampled valid trajectories, and corrected trajectories.","Yields considerable average improvements (3.4% on GSM8k, 4.0% on MATH), especially for smaller models and difficult problems. Encourages diversity of plausible reasoning steps and reduces improper tool-use behavior.","Enhancing the diversity, flexibility, and robustness of an LLM's reasoning output by expanding its training data with varied and corrected reasoning paths.",21
Imitation Learning for Tool-Use Trajectories,"Training LLMs to effectively perform multi-step, interactive tool-use requires extensive demonstrations of such complex behaviors, which are typically scarce or non-existent in standard datasets.","Developing LLMs capable of interactive tool utilization, where the desired behavior involves a sequence of natural language reasoning, program generation, and processing of tool outputs.","Curate a dataset of high-quality, interactive tool-use trajectories (sequences of problem, rationale, program, tool output, next rationale, etc.). Then, train the LLM using imitation learning, minimizing the negative log-likelihood loss on these trajectories. This teaches the model to predict the next step (rationale or program) given the problem and the preceding interaction history.","Enables LLMs to acquire complex interactive tool-use capabilities, leading to improved performance on tasks requiring such interactions, even when starting from a relatively small corpus of expert demonstrations.","Training LLMs for tasks that involve sequential decision-making, interactive problem-solving, and dynamic tool invocation, particularly when expert demonstrations are available or can be synthetically generated.",21
Learning from Feedback (Reinforcement Learning for Tool Use),"Manually annotating comprehensive tool-use examples for 'Learning from Demonstrations' is often impractical, and models need to adapt to the consequences of their actions in dynamic environments.","AI agents operating in environments where the consequences of actions can be observed, and where a reward signal can be defined (either from the environment or humans).","Optimize the foundation model's parameters through open explorations, where the model learns from trial and error. This involves: 1) Reinforcement Learning (RL): Treating tool learning as an RL scenario where the action space is defined by tools, and the agent learns to select tools and actions to maximize a reward signal. Foundation models can initialize the policy model. 2) Environment Feedback: Using ultimate (result feedback) or intermediate (state change feedback) signals from the environment to update the model's policy. 3) Human Feedback: Incorporating explicit (ratings) or implicit (user behavior) human preferences, often via Reinforcement Learning from Human Feedback (RLHF), to guide the model's behavior.","Models learn to understand action consequences, adapt their tool-use behaviors, and align with human preferences, leading to more robust and effective tool manipulation.","Robotic grasping, multi-agent autocurricula, web-based agents, text summarization, enhancing LLM tool-using capabilities (e.g., ETO).",21
Staged Policy Learning for Agentic LLMs,"Training an effective policy for an LLM-orchestrating agent often requires large amounts of human-machine interaction data, which is costly and time-consuming to collect from scratch.","Developing a trainable 'Policy' module for an AI agent that controls a blackbox LLM and other tools, especially when real-user interaction data is scarce or expensive to obtain.","Implement policy learning in multiple stages to mitigate data scarcity and progressively build a robust policy: 1) Bootstrapping from a rule-based policy, where domain experts encode initial task-specific knowledge and business logic into IF-THEN rules to provide a baseline. 2) Learning with user simulators, using a language model to simulate human user interactions, generating synthetic training examples for the policy to self-improve. 3) Refinement with human users, where the LLM-augmented agent finally interacts with real human users to further refine and optimize its policy.","Enables the development of robust and effective policies for agentic LLM systems by progressively leveraging expert knowledge, synthetic data, and real-world interactions, significantly reducing the cost and time associated with data collection and improving policy performance.","Training conversational AI agents, optimizing resource allocation in LLM systems, developing robust decision-making for complex tasks, reducing reliance on expensive human data.",21
Learning from Demonstrations (Behavioral Cloning),"Training foundation models to effectively use tools, especially for complex or nuanced interactions, requires substantial supervision, which can be time-consuming and labor-intensive to collect.","Scenarios where human experts can provide examples of correct tool usage, or where large amounts of unlabeled data are available.","Train models to mimic the behavior of human experts or other models through imitation learning, often using 'behavior cloning.' This involves optimizing the model's parameters to predict the actions taken by an expert given certain inputs. This can be: 1) Supervised Learning: Directly finetuning models on human-annotated tool-use demonstrations (e.g., WebGPT, WebShop). 2) Semi-supervised Learning: Using a less capable model to generate pseudo-labels on unlabeled data, then training a more powerful model on these weakly supervised demonstrations. 3) Self-supervised Learning: Leveraging the in-context learning ability of foundation models to iteratively bootstrap tool-use examples from a few human-written examples, then filtering for noise (e.g., Toolformer).","Models acquire tool-use skills, generalize to new situations, and can manipulate tools effectively, reducing the need for extensive manual rule-engineering.","Robotic applications, autonomous vehicles, web search, online shopping agents, general tool-oriented task finetuning.",21
Behavior Cloning for Initial Skill Acquisition,"Training a language model to perform complex, multi-step tasks in a novel environment (e.g., a text-based browser) where valid actions and interaction formats are unknown to the pretrained model.","A pretrained language model lacks the specific knowledge or 'grammar' to interact with a custom environment or tool with specific command formats, and direct reinforcement learning might be sample-inefficient or unstable without an initial policy.","Collect demonstrations of humans performing the task in the environment. Fine-tune the pretrained language model using supervised learning (behavior cloning), treating the human-issued commands as labels.","The model acquires the necessary skills to operate within the text-based browsing environment, providing a functional baseline policy that can be used directly or as a starting point for further optimization.","Bootstrapping AI agents in new environments, teaching models specific interaction protocols, providing a stable initial policy for RL.",21
Structured Action Space for Agentic LLM,"LLMs, by default, generate free-form text, which is unsuitable for direct interaction with structured environments or tools requiring specific commands.","An LLM needs to operate as an agent within a defined environment (e.g., a web browser, a game, a software tool) by issuing discrete, valid commands.","Define a finite, structured set of commands or actions that the LLM can generate. These commands map directly to operations in the environment (e.g., 'Search [query]', 'Click [link ID]', 'Quote [text]'). The LLM is trained (e.g., via behavior cloning) to generate these specific command strings.","Enables the LLM to interact deterministically and effectively with the environment, transforming its free-form generation capability into structured, actionable commands.","Enabling LLMs to control external tools, navigate structured interfaces, perform sequential tasks requiring specific actions.",21
Automated Dataset Generation for Tool-Augmented LLMs,"Manually creating high-quality, unbiased question-answer datasets that *mandate* the use of external tools for correct answers is labor-intensive, time-consuming, and risks overlap with LLM pre-training data, leading to inaccurate evaluation.",Accurate evaluation of LLMs' tool-use capabilities requires benchmarks where questions cannot be answered solely by the LLM's internal knowledge.,"Employ a multi-phase automated pipeline: 1. **Reference Data Collection**: Curate external knowledge sources (text, tables, graphs) with minimal overlap with LLM pre-training data. 2. **Human-Guided Question Generation with LLMs**: Use LLMs, guided by human-validated templates, to generate questions that are specifically designed to be answerable *only* by querying the collected reference data via tools. 3. **Programmatic Answer Generation**: Implement 'operators' (functions corresponding to tools) and 'tool chains' to programmatically derive accurate ground-truth answers from the reference data for the generated questions.","Scalable and efficient creation of faithful benchmarks (like ToolQA) that precisely evaluate LLMs' ability to use external tools, minimizing bias from internal knowledge and ensuring answer correctness.","Creating evaluation benchmarks for tool-augmented LLMs, generating synthetic training data for tool-use fine-tuning, developing robust LLM agents.",21
Self-Instruct Finetuning for API Generation,"Off-the-shelf LLMs struggle to accurately generate API calls for a vast, overlapping, and frequently updated set of millions of available APIs, often leading to incorrect or hallucinated API calls. Manually creating a comprehensive training dataset for such a scale is impractical.","An LLM needs to be trained to translate natural language instructions into precise, actionable API calls, including relevant packages and step-by-step explanations. The target API ecosystem is large and constantly evolving.","Employ the self-instruct paradigm to generate synthetic instruction data. This involves providing a few in-context examples along with reference API documentation to a powerful LLM (e.g., GPT-4) and tasking it to generate real-world use cases that call upon the API. The generated instruction-API pairs are then used to finetune a base LLM (e.g., LLaMA) in a user-agent chat-style conversation format.","The finetuned LLM (Gorilla) significantly outperforms both open-source and closed-source models in terms of API functionality accuracy and reduction in API argument hallucination errors, even in a zero-shot setting. It enables the LLM to accurately select from a large and changing set of tools.","Training LLMs for code generation (specifically API calls), scaling dataset creation for specialized LLM tasks, improving LLM's ability to follow complex instructions for tool use.",21
Verbalized Score (for confidence calibration),Eliciting confidence scores from LLMs to gauge their certainty.,"When needing to understand the LLM's confidence in its answer, especially when overconfidence is a concern.","A simple calibration technique that generates a confidence score (e.g., 'How confident are you from 1 to 10?') directly in the prompt.","Aims to provide a score representing the model's confidence, though its efficacy is debated.","Assessing LLM confidence, informing user reliance on model outputs.",22
Pairwise Evaluation,Comparing the quality of two texts using an LLM.,When needing to determine which of two given texts is superior according to certain criteria.,"Directly compare the quality of two texts by prompting the LLM to make a judgment. Note that the order of inputs can heavily affect evaluation, and explicitly asking for individual scores might be more effective than direct comparison.","Allows for direct comparison of text quality, but requires careful consideration of input order and potential biases.","A/B testing of generated content, comparative quality assessment, ranking.",22
Linear Scale (for Evaluation),"Obtaining a quantifiable, graded assessment from an LLM for evaluation tasks.",When using an LLM as an evaluator and needing a numerical score to represent quality or performance.,"Prompt the LLM to output a score on a linear scale (e.g., 1-5, 1-10, 0-1), which can be discrete or continuous.",Provides a simple and interpretable numerical rating for the evaluated content.,"Automated evaluation, quality scoring, benchmarking.",22
Output Formatting,It is often desirable for the GenAI to output information in certain structured formats.,"When the downstream system or user expects the GenAI's response to adhere to a specific structure (e.g., CSV, Markdown, XML, custom formats).",Include instructions in the prompt that explicitly specify the desired output format.,"The GenAI generates information in the requested structured format, which can facilitate parsing and integration with other systems. May reduce performance on some tasks but can also improve it.","Data extraction, API integration, structured content generation, facilitating automated parsing.",22
Styling (for Evaluation),Improving the accuracy of LLM-generated judgments in evaluation tasks.,"When using an LLM as an evaluator, and the clarity and consistency of its judgment output can be enhanced.",Format the LLM's evaluation response using structured formats like XML or JSON.,"Improves the accuracy of the judgment generated by the evaluator, making the evaluation more reliable and easier to parse.","Automated evaluation, quality assessment, structured feedback generation.",22
Binary Score (for Evaluation),"Obtaining a simple, categorical (yes/no, true/false) assessment from an LLM for evaluation tasks.",When using an LLM as an evaluator for tasks requiring a straightforward pass/fail or presence/absence judgment.,"Prompt the LLM to generate binary responses like 'Yes' or 'No', or 'True' or 'False'.","Provides a clear, unambiguous binary judgment for the evaluated content.","Automated evaluation, content filtering, factual verification.",22
Likert Scale (for Evaluation),"Obtaining a nuanced, ordinal assessment from an LLM for evaluation tasks, with predefined qualitative categories.",When using an LLM as an evaluator and needing a graded assessment that aligns with human-interpretable qualitative levels.,"Prompt the GenAI to make use of a Likert Scale, providing the scale's categories (e.g., Poor, Acceptable, Good, Very Good, Incredible) within the prompt.","Gives the LLM a better understanding of the meaning of the scale, leading to more nuanced and consistent qualitative judgments.","Automated evaluation, subjective quality assessment, user feedback simulation.",22
Style Prompting,Shaping the output of a GenAI to a desired stylistic quality.,"When the user wants to control the tone, genre, or overall writing style of the GenAI's output.","Specify the desired style, tone, or genre directly in the prompt (e.g., 'Write a clear and curt paragraph').",The GenAI produces output that adheres to the specified stylistic requirements.,"Content generation, creative writing, formal/informal communication.",22
Emotion Prompting,Improving LLM performance on benchmarks and open-ended text generation.,When seeking to enhance the model's understanding or motivation for a task.,"Incorporate phrases of psychological relevance to humans (e.g., 'This is important to my career') into the prompt.",May lead to improved LLM performance on benchmarks and open-ended text generation.,"Enhancing task performance, generating more empathetic or contextually aware responses.",22
Role Prompting,Achieving more desirable outputs or improving accuracy for open-ended tasks by influencing the GenAI's persona.,"When the desired output style, tone, or content can be enhanced by assigning a specific persona to the GenAI.","Assign a specific role or persona to the GenAI within the prompt (e.g., 'Pretend you are a shepherd').",Creates more desirable outputs for open-ended tasks and can improve accuracy on benchmarks.,"Content generation, creative writing, specific domain interactions.",22
Role-based Evaluation,Improving and diversifying LLM-based evaluations.,When using LLMs as evaluators and needing to generate diverse or specific perspectives on text quality.,"Create prompts with the same instructions for evaluation but different roles (e.g., 'act as a literary critic'). This can also be used in a multi-agent setting where LLMs debate the validity of text.",Effectively generates diverse evaluations and can improve evaluation quality.,"LLM evaluation, quality assessment, multi-agent debate.",22
Efficient LLM Fine-tuning,"Fine-tuning large language models (LLMs) for personalization systems is computationally intensive, demanding significant memory and time, which hinders their practical deployment in industrial settings.","LLMs need to be adapted to domain-specific personalization tasks to achieve optimal performance. However, their massive scale makes traditional fine-tuning approaches resource-prohibitive.","Employ efficient fine-tuning strategies that reduce the computational burden and accelerate the adaptation process. Examples include Low-Rank Adaptation (LoRA) and Quantized LoRA (QLoRA), which achieve comparable performance by updating only a small subset of parameters or by quantizing the model weights, respectively. Other strategies like 'option tuning' also fall into this category.","Enables more cost-effective and faster adaptation of LLMs to specific personalization tasks, making their deployment in real-world industrial scenarios feasible by addressing memory and time constraints.","M6Rec (option tuning), LoRA, QLoRA.",23
Long Context Management for LLMs,"Large Language Models (LLMs) have a limited maximum number of input tokens (context window size), which poses challenges when dealing with long user behavior sequences, multi-round conversations, or extensive historical information in personalization systems. Truncating input discards valuable historical context, harming model performance.","Personalization systems often require processing long sequences of user interactions or maintaining context over extended dialogues to provide accurate and relevant recommendations. LLMs, while powerful, are constrained by their fixed context window.","1. Prioritization and Selection: Develop strategies to prioritize and select the most relevant parts of long user behavior sequences or conversation history to fit within the LLM's context window. Criteria can include recency, importance, or task relevance. 2. Summarization and Compression: Employ techniques like extractive summarization or other compression methods to condense lengthy input while preserving essential information, allowing more context to be passed to the LLM. 3. Architectural Modifications/Memory Augmentation: Explore hierarchical or memory-augmented models that incorporate mechanisms to store and retrieve relevant information efficiently, effectively extending the LLM's ability to access and utilize long-term context. This can involve external memory modules or user profile storage.","Enables LLMs to effectively handle long user behavior sequences and multi-round conversations, prevents loss of valuable historical information, and improves the accuracy and relevance of personalized recommendations by maintaining richer context.","MemPrompt (enhances GPT-3 with a memory module for long dialogues), RecLLM (leverages LLM to extract and store user profiles as factual statements in user memory, retrieving relevant facts for queries).",23
LLMs as Conversational Agent,"Traditional Conversational Recommender Systems (CRSs) face challenges with scalability, component synergy (pipeline approach), or require extensive supervised data (end-to-end approach). LLMs inherently lack awareness of private domain data and struggle with memory and comprehension in long conversations due to token limits.","CRSs aim to understand user interests through dialogue for personalized, adaptive recommendations. Large Language Models (LLMs), with their vast open-domain training and emergent intelligence (e.g., InstructGPT, ChatGPT), possess strong inherent conversational capabilities.","1. LLM-powered Dialogue Module: Leverage LLMs' natural language understanding and generation abilities to serve as the core dialogue module, enabling real-time understanding of user intents and fluent communication for open-domain recommendations. 2. Domain Adaptation (Fine-tuning): Fine-tune LLMs with private domain-specific dialogue data to enhance their awareness and modeling capabilities for enterprise-level CRSs. This often involves generating high-quality synthetic dialogue data using LLM-based user simulators. 3. Domain Adaptation (Tool Learning): Treat traditional, domain-specific recommendation models (e.g., Matrix Factorization, DeepFM) as external tools that LLMs can invoke. LLMs act as controllers, translating user natural language requests into tool calls and integrating tool outputs into conversational responses. This requires prompt engineering (e.g., Chain of Thought, ReAct) to guide tool utilization. 4. Long Conversation Memory Management: Implement memory modules (e.g., MemPrompt) or leverage LLMs to extract and store user profiles/facts from conversations. When processing new queries, relevant facts are retrieved based on text similarity to overcome token limits and maintain context.","Real-time understanding of user intents, adaptive and personalized recommendations through natural dialogue, improved dialogue intelligence, effective handling of private domain data, and enhanced comprehension and memory in long conversational interactions.","ChatRec (LLM-augmented recommender system using conventional models and text embedding models as tools), RecLLM (LLM-enhanced dual-encoder model and text retrieval methods as recommendation engines, user profile module for memory), MemPrompt (memory module for GPT3), iEvaLM (user simulator for data generation).",23
LLM-based Tool Learning,"Large Language Models (LLMs), despite their vast knowledge, struggle with specific, private, or specialized domain knowledge (e.g., item corpora, user profiles) and face challenges with temporal generalization (external knowledge evolving). They can also suffer from hallucinations.",Tool learning is an emerging field where foundational models are combined with specialized tools to enhance task-solving capabilities. LLMs are powerful natural language processors capable of breaking down complex tasks and converting them into executable instructions.,"1. LLMs as Controllers/Orchestrators: LLMs act as central components that comprehend problem statements, decide which external tools or AI models to execute, and aggregate their outcomes. This allows LLMs to leverage specialized capabilities beyond their internal knowledge (e.g., HuggingGPT, Visual ChatGPT, TaskMatrixAI, AutoGPT). 2. Reasoning and Acting (ReAct): LLMs are prompted to alternately generate reasoning paths and task-specific actions. Actions are delegated to external tools, and the feedback from these tools is used to validate and guide the LLM's reasoning process. 3. Integrated Tool Use (Toolformer): LLMs are trained to self-supervise and integrate diverse tools (e.g., calculator, QA system, search engine, translation system) within a single model, providing flexible decision-making and improved generalization. 4. LLMs as Tool Makers: LLMs can be empowered to directly generate new tools, enabling a division of labor where LLMs at different scales act as tool makers, users, and dispatchers (LATM). 5. Specific Tools for Recommendation: Search Engines (provide external, up-to-date knowledge), Recommendation Engines (equip LLMs with specialized recommendation models), Databases (Vector, User Profile) (supplement information for cold-start items, alleviate temporal generalization, store/retrieve user facts).","Enhanced task-solving capabilities for LLMs, access to external and domain-specific knowledge, reduced hallucinations, improved accuracy and efficiency in personalized systems, better handling of cold-start and temporal generalization problems.","HuggingGPT, Visual ChatGPT, TaskMatrixAI, AutoGPT, WebGPT, ReAct, Toolformer, LATM, BlenderBot 3, LaMDA, RETALLM, ChatREC, RecLLM.",23
LLMs for Automated ML Selection,"Automated Machine Learning (AutoML) in recommender systems involves a complex and costly manual setup with trials and errors across various search spaces (embedding size, feature selection, feature interaction, model architecture). The search space is vast and lacks a strong foundation of knowledge regarding informative components.","Large Language Models (LLMs) possess excellent memorization and reasoning capabilities, making them suitable for decision-making and problem-solving in complex search spaces.","1. Architecture Generation: Utilize LLMs' generative capabilities to formulate network architectures as sequential characters, allowing LLMs to generate reasonable candidate architectures, thereby reducing the search space for subsequent optimization algorithms (e.g., genetic algorithms). 2. Blackbox Agent for Optimization: Employ LLMs (e.g., GPT-4) as blackbox agents to analyze previous trials (tried architectures and their performance) and generate potentially better-performing architectures for iterative optimization. 3. Genetic Operator Integration: Integrate LLMs into genetic search algorithms as mutation and crossover operators. LLMs are prompted to generate new offspring (crossovers or mutations) based on the current generation, guiding the evolutionary search process more effectively.","Automation and optimization of the AutoML process in recommender systems, reduced manual effort, more efficient exploration of complex search spaces, and generation of high-performing ML architectures.","GPTNAS (neural architecture search with generative pretraining models), GENIUS (GPT4 as a blackbox agent for architecture generation), LLMatic (LLMs as mutation/crossover operators for genetic NAS), EvoPrompting (LLMs for code-level neural architecture search).",23
LLMs as Content Interpreter,"Content features in content-based recommendation can be sparse, and conventional content interpretation methods (statistical models, neural networks, advanced NLP) struggle to capture deep semantic information, generalize effectively, or align with specific recommendation objectives. Online inference latency is also a challenge.","Pretrained Language Models (PLMs) like BERT and GPT have demonstrated exceptional performance in NLP tasks, capable of capturing deep semantic representations and incorporating extensive world knowledge. Large Language Models (LLMs) further offer emergent abilities in reasoning and generalization.","1. Fine-tuning: Adapt PLMs/LLMs to recommendation tasks by fine-tuning them with task-specific pretraining tasks (e.g., masked opinion token prediction, opinion rating prediction) or instruction tuning for recommendation-specific objectives. 2. Knowledge Distillation/Model Optimization: Reduce online inference latency by distilling knowledge from large models to obtain lightweight and efficient models. 3. Text-only Recommendation: Leverage PLMs/LLMs to obtain universal continuous representations from textual features, enabling zero-shot and cross-domain recommendations, especially for cold-start problems. 4. Reasoning-enhanced Interpretation: Use LLMs to generate reasoning as additional features based on user behavior history, which can then be converted into natural language instruction data for fine-tuning.","Enhanced understanding and interpretation of textual content, improved recommendation accuracy, better handling of cold-start problems, facilitation of cross-domain recommendations, and improved generalization and reasoning abilities for recommendation tasks.","UBERT (BERT as content interpreter), news recommendation (BERT, ERNIE), tag/tweet/code example recommendation, ZESREC (zero-shot recommendation), Unisrec (cross-domain sequential recommendation), VQRec (vector quantization for textual embeddings), TALLRe (sequential recommendation with instruction tuning), LLMsRec (rating prediction), PALR (item recommendation with reasoning generation), InstructRec (recommendation as instruction following).",23
LLMs as Personalized Content Creator,"Traditional recommender systems primarily suggest existing items, and content creation for personalization (e.g., ad titles, descriptions) is often manual, template-based, or struggles to fully capture nuanced user interests due to sparse feedback.","AI Generated Content (AIGC) involves AI models creating digital content. Large Language Models (LLMs) possess powerful generative abilities, deep understanding of human intention from instructions, and extensive cross-modal knowledge bases.","1. Enhanced User Intent Reasoning: Leverage LLMs' advanced reasoning capabilities to better understand and interpret complex, personalized user intents and interests, going beyond what tailored pretraining models can achieve. 2. Reinforcement Learning from Human Feedback (RLHF): Apply RLHF strategies to fine-tune LLMs, enabling them to more effectively capture user intent and generate content that aligns with user preferences, similar to existing RL-based frameworks for text ad generation. 3. Realistic and High-Quality Generation: Utilize LLMs' powerful generative abilities and access to sufficient cross-modal knowledge bases to create realistic and high-quality personalized content (e.g., text, images, videos). 4. Feedback-driven Iterative Generation: Design recommendation paradigms where the content generation process receives explicit user feedback and involves multiple rounds of conversion, allowing LLMs to iteratively refine and guide content generation based on evolving user preferences, significantly alleviating sparse feedback issues.","Creation of more appealing, customized, and high-quality digital content (e.g., ad copy, product descriptions) that precisely matches individual user interests and preferences; improved understanding of explicit user expressions; enhanced user experiences and business growth in scenarios like e-commerce and customer service.","ChatGPT, DALL-E 2, Codex, Midjourney for AIGC; recommendation paradigm based on ChatGPT for feedback-driven content generation; text ad generation (CREATER, COBART).",23
LLMs for Direct Recommendation (In-context Learning & CoT),"Traditional machine learning models for recommendation require explicit training or fine-tuning for each new task or domain. LLMs, when used directly, may not possess a significant advantage in personalized modeling compared to fine-tuned specialized recommenders.","Large Language Models (LLMs) exhibit emergent abilities like in-context learning (ICL), allowing them to learn from few-shot examples provided in the prompt without explicit training, and step-by-step reasoning (Chain-of-Thought, CoT) for complex tasks.","1. Zero/Few-shot Recommendation: Formulate recommendation tasks (e.g., rating prediction, ranking prediction) as natural language instructions and provide a few input-output examples (shots) in the prompt, allowing LLMs to generate recommendations directly without tuning. 2. Multi-step Reasoning (Chain-of-Thought): Employ CoT prompting strategies to guide LLMs to break down complex recommendation tasks (e.g., user preference capture, item filtering, reranking) into intermediate reasoning steps, iteratively approaching the final recommendation. 3. Candidate Generation Integration: For ranking tasks, integrate a candidate generation module to narrow down the pool of items, as generative LLMs may not recall existing ID-based items directly.","Ability to make predictions on new recommendation cases without extensive training or tuning; improved recommendation performance for complex tasks through structured, step-by-step reasoning; leveraging LLMs' commonsense knowledge for open-domain recommendations.","Studies evaluating zero-shot/few-shot recommenders for rating prediction, sequential recommendation, direct recommendation, and reranking (e.g., using gpt3.5turbo, text-davinci-002/003, FlanT5); NIR (three-step prompt for reranking).",23
LLMs as Knowledge Base,"Existing knowledge graphs (KGs) for recommender systems are limited, sparse, and expensive to construct and complete, leading to missing entities or relations and hindering recommendation performance.","Large Language Models (LLMs) possess an impressive ability to retrieve factual knowledge and store vast amounts of commonsense knowledge, which can be leveraged to enrich KGs.","Utilize LLMs for knowledge graph completion (predicting missing facts) and knowledge graph construction (entity discovery, coreference resolution, relation extraction, end-to-end KG building from raw text). LLMs can also distill commonsense facts to smaller student models. This involves designing tailored prompts for LLMs to predict entities or extract relations.","More extensive, accurate, and up-to-date knowledge graphs; enhanced recommendation accuracy, relevance, and personalization; improved cross-domain recommendations by leveraging LLMs' cross-domain information.","KAR (generating factual knowledge for CTR prediction), LLMRec (LLM-based graph augmentation), LLMKRec (determining complementary relationships for industrial recommenders).",23
LLMs as Explainer,"Recommender systems are often 'black boxes,' lacking transparency and diminishing user trust. Traditional explanation methods (template-based, early NLG) are inflexible, lack personalization, suffer from diversity/coherence issues, and are tightly coupled with specific recommendation models, limiting generalizability.","Users desire comprehensible justifications for recommendations to improve trust and decision-making. Large Language Models (LLMs) possess remarkable generative abilities in language tasks, extensive training data, and in-context learning capabilities.","1. Customized and Natural Explanations: Leverage LLMs' deep understanding of human language (context, metaphors, complex syntax) to generate precise, natural, and adaptable explanations tailored to various user preferences, moving beyond formulaic templates. 2. Interactive and Bidirectional Alignment: Utilize LLMs' in-context learning (zero-shot, few-shot, Chain-of-Thought prompting) to gather real-time user feedback during interactions and provide dynamic explanations, fostering better human-machine alignment. 3. Model-Agnostic Interpretation: Employ LLMs to interpret the internal workings of complex, deep learning-based recommendation models (e.g., explaining neuron functions) in a way that is independent of the specific model architecture, offering a versatile interpretational framework.","Improved transparency, persuasiveness, and reliability of recommendations; enhanced user trust and satisfaction; a versatile and scalable interpretational framework with broader applicability across different recommendation algorithms.","LLM4Vis (explainable visualization recommendation through ChatGPT), RecExplainer (aligning LLMs for recommendation model interpretability), studies on GPT4 interpreting GPT2 neurons.",23
Human-in-the-Loop Data Collection Interface,"Collecting high-quality, consistent human demonstrations and preference comparisons for complex AI tasks is often challenging, time-consuming, and requires clear guidance for human labelers.","Training or evaluating AI models, especially for tasks involving complex interactions or subjective judgments (e.g., web browsing, long-form question answering), requires large datasets of human-generated actions or preference ratings.","Develop a specialized graphical user interface (GUI) designed for human demonstrators and labelers. This interface should simplify complex interactions (e.g., by providing visual cues, structured input fields, and clear action buttons), display relevant information concisely, and allow for detailed auxiliary annotations (e.g., claim support, relevance, trustworthiness ratings) to enrich the collected data.","Improves the efficiency, consistency, and quality of human data collection, making it easier for contractors to provide accurate demonstrations and nuanced comparisons, which directly contributes to better training and evaluation data for AI models.","Generating training data for imitation learning, collecting human preferences for reward modeling, facilitating complex data annotation tasks, scaling human feedback collection.",24
Dual Data Collection for Agentic LLM Training,"Training an agentic LLM for complex tasks requires both initial behavioral guidance and subsequent preference-based refinement, which cannot be achieved effectively with a single type of human feedback.","An LLM needs to learn both how to perform a multi-step task (e.g., browsing) and how to generate high-quality, preferred outputs (e.g., answers).","Collect two distinct types of human-generated data: 1. **Demonstrations:** Human experts perform the task, providing sequences of actions and observations, used for initial Behavior Cloning to teach the model *how* to interact. 2. **Comparisons:** Humans rate pairs of model-generated outputs (or model vs. human outputs) based on subjective quality criteria, used for training a Reward Model to teach the model *what* constitutes a good output.","Provides a comprehensive dataset that enables both initial skill acquisition and subsequent alignment with human preferences, leading to models that are both capable and aligned.","Training complex AI agents, combining imitation learning with preference learning, bootstrapping and refining AI behavior.",24
Prompt Optimization with Textual Gradients (ProTeGi),Improving a prompt template through a multi-step process involving criticism and selection.,When a prompt template needs iterative improvement based on LLM-generated criticism.,"First, pass a batch of inputs through the template. Then, pass the output, ground truth, and prompt into another prompt that criticizes the original prompt. Generate new prompts from these criticisms, then use a bandit algorithm to select one.",Demonstrates improvements over methods like APE and GrIPS by leveraging LLM criticism and bandit algorithms.,"Automated prompt optimization, prompt refinement.",24
Rejection Sampling (Best-of-N),"Improving the quality of generated outputs from a language model without further training or complex reinforcement learning setups, especially when a good reward model is available.","A language model can generate multiple candidate outputs for a given input, and a reward model can reliably score these outputs based on desired criteria.","Sample a fixed number (N) of answers from the language model (e.g., a Behavior Cloned model or an RL-tuned model). Use a pre-trained Reward Model to score each of these N samples. Select the sample with the highest reward model score as the final output.","Substantially improves the quality of the final output, often outperforming direct RL optimization, by leveraging inferencetime compute to select better samples. It requires no additional training of the generative model.","Enhancing output quality, leveraging a reward model for inference-time improvement, trading off compute for quality.",24
Iterative Self-Correction with Automated Feedback,"Initial LLM-generated responses may not meet desired quality standards (e.g., factuality, coherence, alignment with task-specific rules) and require refinement, but manual correction is not scalable.","Improving the quality and alignment of responses from a fixed, blackbox LLM for tasks where accuracy and adherence to specific criteria are crucial, without modifying the LLM's parameters.","Implement a 'Utility Module' that evaluates candidate LLM responses against a set of task-specific utility functions (e.g., factuality score, adherence to conversational rules). If a response fails to meet the criteria, the Utility Module generates verbalized feedback (e.g., 'The response is inconsistent with the knowledge. Please generate again' or 'self-criticism' feedback). This feedback is then incorporated into a revised prompt, and the LLM is queried again to generate an improved candidate response. This process can iterate until a satisfactory response is produced.","Leads to substantial improvements in response quality, groundedness, and alignment with user expectations or business requirements by iteratively guiding the LLM's generation process.","Reducing hallucinations, ensuring responses are grounded in evidence, aligning responses with specific conversational styles or business logic, improving overall response quality in multi-turn interactions.",24
KL Regularization in RLHF,"During Reinforcement Learning from Human Feedback (RLHF), fine-tuning a language model with a reward model can lead to policy divergence, where the model's behavior drifts significantly from its initial (e.g., behavior-cloned) policy, potentially exploiting reward model flaws or generating undesirable outputs.","A language model is being fine-tuned using Reinforcement Learning (e.g., PPO) with a reward signal derived from a human preference model, starting from a pre-trained or behavior-cloned policy.","Incorporate a Kullback-Leibler (KL) divergence penalty into the RL objective function. This penalty measures the difference between the current policy and a reference policy (typically the initial behavior-cloned policy), encouraging the RL-tuned policy to remain close to the original distribution of generated text.","Stabilizes RL training, prevents overoptimization of the reward model, and ensures that the fine-tuned policy maintains desirable characteristics (e.g., coherence, style) from the initial policy while still optimizing for the human preference signal.","Stabilizing RLHF training, preventing policy collapse or divergence, maintaining stylistic consistency during fine-tuning, mitigating reward hacking.",24
Perceiver (Feedback Processing),"The controller (foundation model) needs to make informed decisions based on diverse feedback from both the user and the environment, but this feedback can be raw, multi-modal, or require summarization.","Any tool learning framework where a foundation model acts as a controller and needs to process feedback from its interactions to refine its plans or actions. Feedback can be text, vision, audio, or execution results.","Implement a 'Perceiver' component that processes user feedback (e.g., clarification requests, preferences) and environment feedback (e.g., tool execution results, state changes). This processing can range from simple concatenation/formatting to complex neural models capable of handling multiple modalities (text, vision, audio) and generating a concise summary for the controller.","The controller receives structured, summarized, and potentially multi-modal feedback, enabling it to determine plan effectiveness, identify anomalies, and adjust its decision-making process more effectively.","Iterative planning, error handling, adapting to dynamic environments, human-in-the-loop systems.",24
Hybrid Utility Function Design,"Evaluating LLM responses for complex tasks requires assessing multiple dimensions (e.g., factuality, fluency, adherence to rules) and generating actionable feedback, which might not be achievable with a single evaluation method.","Developing a robust evaluation and feedback mechanism for LLM-generated content, especially in scenarios with specific business requirements, human preferences, or a need for both objective and subjective quality assessment.","Design the 'Utility' module to incorporate a hybrid approach to evaluation, combining: 1) Model-based utility functions, trained on human preference data or annotated logs to assign scores for subjective dimensions like fluency, informativeness, or factuality. 2) Rule-based utility functions, implemented using heuristics or programmed functions to measure compliance with specific, objective rules or business logic. Additionally, develop a utility function (e.g., a text generation model or rule-based NLG) to generate informative and actionable textual feedback based on these evaluations.","Provides a comprehensive and flexible way to evaluate LLM responses, combining subjective quality assessment with objective rule compliance, and generates specific feedback to guide iterative improvement, leading to better alignment with desired outcomes.","Quality assurance for LLM outputs, automated feedback generation, aligning LLM behavior with complex criteria, supporting iterative refinement processes.",24
Human Feedback for Quality Optimization (Reward Modeling & RLHF),"Directly optimizing language model outputs for subjective qualities like factual accuracy, coherence, and overall usefulness is challenging with traditional loss functions. Imitation learning alone may not surpass human performance or directly optimize for desired qualities.","A language model's outputs (e.g., long-form answers) need to be aligned with human preferences regarding quality, factual accuracy, and coherence.","Collect human comparisons of model-generated answers (and potentially human-generated answers). Train a separate Reward Model (RM) to predict human preferences (e.g., Elo scores). Then, use this RM to optimize the language model, either through Reinforcement Learning (RL) (e.g., PPO) or by selecting the best output from multiple samples (Rejection Sampling).","The language model's outputs are significantly preferred by humans, achieving or surpassing human-level performance on subjective quality metrics. The RM provides a scalable proxy for human judgment.","Aligning LLMs with human values, improving subjective quality of generated text, fine-tuning models for complex, hard-to-quantify objectives.",24
VoteK Exemplar Selection,"Selecting effective and diverse exemplars for FewShot Prompting, especially when some data is unlabeled.","When a pool of unlabeled candidate exemplars exists, and diversity is desired in the selected examples.","In a two-stage process, a model proposes useful unlabeled candidate exemplars for human annotation. The labeled pool is then used for FewShot Prompting, ensuring newly added exemplars are sufficiently different to increase diversity and representativeness.",Improves FewShot Prompting performance by selecting similar and diverse exemplars.,"Improving FewShot Prompting performance, especially with human-in-the-loop labeling for diversity.",25
KNN Exemplar Selection,Selecting effective exemplars for FewShot Prompting can be difficult due to context window limitations and performance dependency on exemplar quality.,When a training dataset (Dtrain) is available and exemplars need to be dynamically selected for a test instance (Dtest_xi).,Select exemplars from Dtrain that are similar to the Dtest_xi using a K-Nearest Neighbor algorithm.,"Boosts performance by providing relevant examples, though it can be time and resource-intensive during prompt generation.",Improving FewShot Prompting performance by selecting relevant examples.,25
FewShot Prompting,GenAIs need to learn skills and tasks with limited examples without weight updates.,When training data is unavailable or model parameters cannot be updated.,Provide the GenAI with a few examples (exemplars) of a task being completed within the prompt.,"The GenAI learns to complete the task, often improving model performance, especially in larger models.","Task completion, classification, question answering.",25
Self-Generated InContext Learning (SGICL),Lack of actual training data for FewShot Prompting.,"When training data is unavailable, but a GenAI can be leveraged to create examples.",Leverage a GenAI to automatically generate exemplars for use in FewShot prompts.,"Provides samples for FewShot Prompting, performing better than zero-shot scenarios, though generated samples may not be as effective as actual data.",Creating synthetic exemplars for FewShot Prompting when real data is scarce.,25
Prompt Mining,Finding optimal prompt structures or 'middle words' for improved LLM performance.,When seeking to optimize prompt performance by discovering effective prompt templates or formats.,"Analyze large corpora to discover optimal 'middle words' or formats that occur frequently in the corpus, which are then used as prompt templates.",Improved prompt performance by using formats that align with the model's training data distribution.,Discovering effective prompt templates and formats for various tasks.,25
ZeroShot Prompting,Guiding GenAIs to complete tasks without any examples.,"When no exemplars are available or desired, and only an instruction in natural language is given to the model.","Provide a prompt with zero exemplars, relying solely on instructions to guide the GenAI's output.",The GenAI performs the task based on its pre-trained knowledge and the given instructions.,"Open-ended tasks, initial task exploration, when no examples are available.",25
Max Mutual Information Method,Selecting the optimal prompt template from multiple variations.,When multiple prompt templates with varied styles and exemplars have been created.,Select the optimal template as the one that maximizes mutual information between the prompt and the LLM's outputs.,Identifies the most effective prompt template for a given task.,"Prompt optimization, A/B testing of prompt variations.",25
Exemplar Ordering,The order of exemplars in a few-shot prompt can significantly affect the LLM's behavior and accuracy.,"When designing few-shot prompts, especially for tasks where the model's performance is sensitive to the sequence of examples.","Carefully arrange the order of exemplars within the prompt. The text notes that 'randomly order exemplars' is a design decision, implying that *controlled* ordering (or even random ordering as a strategy) is a technique.","Can cause accuracy to vary significantly, implying that optimal ordering can lead to improved performance, while suboptimal ordering can degrade it.","Optimizing few-shot prompt performance, mitigating prompt sensitivity.",25
Dense Passage Retrieval (DPR),"Traditional sparse vector methods (TFIDF, BM25) for document retrieval may not capture semantic similarity effectively, leading to lower retrieval precision for Open-Domain Question Answering (ODQA) and other knowledge-intensive tasks.","Building efficient and accurate neural information retrieval systems, especially for ODQA, where semantic understanding of questions and passages is crucial. Used as the retriever component in architectures like RAG.","Employ two independent BERT-based neural networks: a Question Encoder (EQ) and a Passage Encoder (EP). Both encoders generate dense vector representations (embeddings) for questions and text passages, respectively. The similarity between a question and a passage is then calculated using the dot product of their respective embeddings. The model is typically pre-trained on large datasets (e.g., Wikipedia-based QA pairs) to learn effective representations.",Achieves higher retrieval precision by modeling textual similarity at a more semantic level compared to sparse methods. Higher retrieval precision directly results in higher end-to-end QA accuracy when integrated into a QA system.,"Neural information retrieval, retriever component in QA systems, semantic search, pre-training for RAG models.",26
Domain-Specific Finetuning (DSF),"General-purpose Large Language Models (LLMs) may not align with the specific answering style, terminology, or nuances required for specialized domains, leading to suboptimal performance or inappropriate responses.","Adapting a pretrained LLM to a specific domain (e.g., medical, legal, code APIs, enterprise documents) where the primary goal is to align its output style and familiarize it with domain-specific knowledge, without necessarily relying on external retrieval at inference time (though it can be combined with RAG).","Apply standard supervised finetuning (SFT) to a pretrained LLM using a dataset of question-answer pairs (or other task-specific data) that are entirely within the target specialized domain. This training is typically done without providing external documents in the context during the finetuning phase, allowing the model to internalize domain knowledge and adapt its generation style.",Improves the LLM's performance by aligning its answering style with the domain's requirements and familiarizing it with domain-specific context. It serves as a strong baseline for domain adaptation and can significantly enhance performance compared to a base model.,"Initial adaptation of a general LLM to a new domain, establishing a baseline for domain-specific performance, or when the primary need is style alignment and knowledge internalization rather than real-time external document grounding.",26
Retrieval Augmented Fine-Tuning (RAFT),"Pretrained Large Language Models (LLMs) struggle to effectively adapt to specialized domains for Retrieval Augmented Generation (RAG). They often fail to leverage fixed domain learning opportunities, account for imperfect retrieval during training, or robustly handle distracting information while maximizing accuracy based on provided documents.","Adapting LLMs for high-accuracy question answering in specialized, fixed domains (e.g., legal, medical, enterprise documents, code APIs) where external documents are provided at inference time (an 'open-book' setting). The primary goal is to improve the LLM's ability to effectively use retrieved documents and ignore irrelevant ones.","RAFT is a training recipe that combines instruction finetuning with RAG. It involves preparing training data with questions (Q), a set of retrieved documents (Dk) including both relevant 'golden' documents and irrelevant 'distractor' documents, and a Chain-of-Thought style answer (A). The model is trained to generate answers by citing verbatim sequences from relevant documents and to ignore distractors. A key aspect is varying the proportion (P) of training instances that include golden documents, sometimes presenting only distractors to enhance robustness.","Consistently and significantly improves LLM performance in domain-specific RAG across various datasets (PubMed, HotpotQA, Gorilla). It enhances the model's ability to read, extract information, align answering style, and be robust against distractors, outperforming standard supervised finetuning with or without RAG.","Adapting pretrained LLMs for high-accuracy, context-grounded question answering in specialized domains, particularly when robustness to noisy or imperfect retrieval is critical.",26
Instruction Finetuning (IFT),"Pretrained Large Language Models (LLMs) may not consistently follow user instructions or generate responses in a desired format, even if they possess the underlying knowledge. Their outputs might be generic or not aligned with specific task requirements.","Adapting a general-purpose pretrained LLM to better understand and execute human instructions, perform specific tasks (like QA), and align its output style and behavior with user expectations. This is often a prerequisite for effective zero-shot or few-shot prompting.","Finetune the LLM on a dataset composed of diverse instructions paired with their desired outputs. This dataset teaches the model to interpret and respond to instructions effectively, often leading to improved performance on a wide range of downstream tasks without further task-specific finetuning. The training data can include examples of questions and answers, or other instruction-response pairs.","Enhances the LLM's ability to follow instructions, improves its performance on various tasks (especially in zero-shot settings), and aligns its behavior with human preferences, making it more usable and versatile.","Creating more obedient and capable LLMs that can perform a wide array of tasks based on natural language instructions, forming the basis for many chatbot and general-purpose AI applications.",26
Fixed Document Index (during Fine-tuning),"Updating the document encoder and rebuilding the entire document index during fine-tuning of a retrieval-augmented model is computationally costly and time-consuming, especially for large document corpora.","Fine-tuning a retrieval-augmented generation (RAG) model where the document corpus is large, and the document encoder is part of the end-to-end training loop.","Keep the document encoder (e.g., BERT_d) and the associated document index fixed during the fine-tuning phase. Only the query encoder (e.g., BERT_q) and the generator (e.g., BART) are fine-tuned. The document index is built once before fine-tuning.","Reduces computational cost and training time significantly, making fine-tuning more practical, while still achieving strong performance. This strategy is found to be 'not necessary for strong performance' in this context.","Optimizing the fine-tuning process for RAG models, reducing computational overhead.",26
Dense Passage Retrieval (DPR),"Efficiently and effectively retrieving relevant text passages from a large corpus based on a query, especially for knowledge-intensive tasks, where simple keyword matching (like BM25) might be insufficient.","Open-domain question answering, fact verification, or any knowledge-intensive NLP task requiring semantic matching between a query and a vast collection of documents to find relevant evidence.","Employ a bi-encoder architecture where a query encoder (e.g., BERT-based) produces a dense vector representation of the query, and a document encoder (e.g., BERT-based) produces dense vector representations for all documents. Retrieval is then performed by finding documents whose dense representations have the highest inner product similarity with the query's dense representation (Maximum Inner Product Search - MIPS). The document index is built using these dense embeddings.","Enables semantic retrieval, outperforming word-overlap based methods (like BM25) for many knowledge-intensive tasks. It forms the nonparametric memory component in RAG models.","Retriever component in RAG, Open-domain Question Answering, Fact Verification.",26
Distractor-Aware Finetuning,"Large Language Models (LLMs) are vulnerable to irrelevant text in retrieved documents, which is a common occurrence in Retrieval Augmented Generation (RAG) settings. Training only with highly relevant documents can diminish the model's ability to discern and disregard irrelevant information, leading to decreased performance when distractors are present.",Finetuning LLMs for RAG tasks where the retrieval mechanism might provide a mix of relevant and irrelevant documents. The model needs to be robust to noise and effectively identify pertinent information within a potentially noisy context.,"During finetuning, integrate both golden (highly relevant) documents and distractor (irrelevant) documents into the training context. The model is explicitly trained to process this mixed context, learning to identify and utilize the relevant information while ignoring the irrelevant parts. This can involve varying the number of distractors and even presenting contexts with only distractors for a portion of the training data (e.g., 1-P fraction in RAFT).","Enhances the LLM's robustness against irrelevant text, making it more resilient to fluctuations in the number of documents encountered during testing. Improves performance on RAG tasks compared to training solely with golden documents, as the model learns to sift through noise.","Improving the reliability and accuracy of LLMs in RAG systems by making them robust to imperfect retrieval, ensuring they can effectively extract answers even when surrounded by irrelevant or misleading information.",26
End-to-End Retriever Training (for RAG Domain Adaptation),"The original RAG architecture, by keeping the passage encoder and external knowledge base fixed during finetuning, struggles to adapt effectively to specialized domains where the knowledge distribution differs significantly from its pre-training data (e.g., Wikipedia). This can lead to suboptimal retrieval and generation performance in new domains.","Adapting Retrieval Augmented Generation (RAG) models to perform Open-Domain Question Answering (ODQA) or other knowledge-intensive tasks within specific, specialized domains (e.g., healthcare, news, conversations) that utilize a domain-specific external knowledge base.","Extend the RAG architecture to enable full end-to-end trainability of all its components. This involves finetuning both the question encoder (EQ) and the passage encoder (EP) of the Dense Passage Retrieval (DPR) component, and dynamically updating (re-encoding and re-indexing) the external knowledge base embeddings during the training process. This allows gradients to propagate through the entire retrieval and generation pipeline, optimizing the retriever for the target domain.","Achieves significant performance improvements in domain adaptation for RAG models across various metrics (Exact Match, F1, Top-K retrieval accuracy). It enables the retriever to learn domain-specific representations, outperforming models with fixed retrievers or independently finetuned retrievers.","Improving the domain adaptation capabilities of RAG models, training domain-specific neural retrievers, enhancing performance in specialized ODQA tasks.",26
Synthetic QA Data Generation,"The scarcity or complete absence of large-scale, human-annotated question-answering (QA) datasets for specialized domains makes it challenging to train and adapt AI models like RAG or its components (e.g., DPR) for those domains.","Developing and adapting AI models for Open-Domain Question Answering (ODQA) or other knowledge-intensive tasks in data-scarce or emerging domains (e.g., COVID-19 research, new topics in news or conversations) where manual annotation of QA pairs is prohibitively expensive or time-consuming.","Leverage existing pre-trained generative models (e.g., a BART seq2seq model finetuned on a general QA dataset like SQuAD) to automatically generate synthetic question-answer pairs from raw text passages within the target domain's knowledge base. Employ filtering techniques, such as round-trip consistency, to improve the quality and relevance of the generated synthetic data.","Provides a scalable and cost-effective method to create extensive domain-specific QA datasets, enabling effective training and domain adaptation of AI models even when human-labeled data is minimal. This allows for the finetuning of components like DPR or the entire RAG model.","Bootstrapping training data for new or specialized domains, reducing reliance on manual data annotation, enabling domain adaptation in data-limited scenarios, generating hard negative examples for retriever training.",26
Open-Book Question Answering with InContext RALM,"Enabling frozen, pre-trained Language Models to answer open-domain questions accurately by leveraging external knowledge, without requiring fine-tuning or specific pre-training for QA.","A large, pre-trained LM (e.g., LLaMA) is available, but it lacks the specific knowledge or reasoning capabilities to answer open-domain questions reliably in a 'closed-book' setting. Fine-tuning is not desired or possible.","Extend the standard question-answering prompt by prepending relevant documents retrieved from a knowledge source (e.g., Wikipedia via DPR) to the LM's input. The LM then processes this augmented prompt (documents + question) to generate the answer. The number of documents can be optimized.","Significantly boosted performance in open-domain question answering tasks (e.g., exact match scores on NQ and TriviaQA) for frozen LMs, demonstrating their ability to leverage retrieved documents without further training.","Building robust open-domain question answering systems, improving factual accuracy in QA, leveraging general-purpose LMs for knowledge-intensive tasks.",26
Retriever-Aware Training (RAT),"Large Language Models (LLMs) struggle to effectively use external tools via API calls, especially when the available APIs are massive, frequently updated, and the LLM is unaware of their existence or usage. Naive retrieval augmentation can sometimes hurt performance if the retrieved documents are irrelevant or inaccurate.","An LLM needs to interact with a dynamic and extensive set of external APIs, whose documentation changes over time. The LLM must not only leverage this documentation but also learn to critically evaluate its relevance and accuracy during inference.","The LLM is finetuned on an instruction-tuned dataset where the user prompt is augmented with retrieved documentation. Crucially, the training process includes potentially incorrect retrieved documentation alongside the accurate ground truth in the LLM's response. This teaches the LLM to 'judge the retriever' at inference time, enabling it to use relevant documentation and ignore irrelevant context, relying on its baked-in domain-specific knowledge when retrieval fails. During inference, a retriever (e.g., BM25, GPTIndex) fetches the most up-to-date API documentation, which is then concatenated to the user prompt.","The LLM demonstrates a strong capability to adapt to test-time document changes (e.g., API version updates, argument changes), improves performance compared to in-context learning, and substantially mitigates API argument hallucination errors. It maintains efficacy and accuracy over time despite documentation evolution.","Enhancing LLMs for dynamic tool use, improving reliability of API generation, reducing hallucination in tool invocation, adapting to frequently updated documentation.",26
Finetuning for Controlled Abstention,"LLMs, especially in RAG settings, often hallucinate incorrect answers rather than abstaining when they lack sufficient information, leading to untrustworthy outputs.",Developing and deploying LLMs where it is crucial for the model to explicitly indicate uncertainty ('I don't know') instead of generating potentially harmful or incorrect information.,"1. Prepare a finetuning dataset where a subset of training examples (e.g., 20% randomly selected or 20% identified as having insufficient context) has its ground truth answer replaced with an explicit 'I don't know' response. 2. Apply LoRA adaptation to finetune the LLM (e.g., Mistral 3 7B) on this modified dataset.","Finetuned models show a higher rate of correct answers in some cases and increased abstention compared to models trained without 'I don't know' examples. However, it's noted that this approach alone doesn't easily reduce the hallucination rate and may increase abstention at the cost of fewer correct answers, suggesting further research is needed for optimal strategies.",Attempting to reduce LLM hallucinations by encouraging explicit abstention; Steering LLM behavior during training to improve trustworthiness.,26
Prompt-Tuning a Frozen LM as a Reader,"Adapting a pre-trained, frozen Language Model for a specific downstream task (e.g., Open-Domain Question Answering) without fine-tuning its weights, which can be costly or impossible.","A frozen LM is available, and it needs to perform a specific 'reading' task (like extracting answers from text or understanding a query) within a larger system (e.g., a RALM or QA system).","Instead of fine-tuning the LM's weights, design specific prompts or 'prompt-tunes' that guide the frozen LM to perform the desired task. This involves crafting the input text to elicit the correct behavior from the LM.","Enables frozen LMs to achieve competitive performance on downstream knowledge-intensive tasks like Open-Domain Question Answering, leveraging their pre-trained capabilities without expensive retraining.","Adapting LMs for QA, summarization, information extraction, or other specific tasks where fine-tuning is not an option.",26
Selective Generation with Sufficient Context Signal,"Large Language Models (LLMs) in Retrieval Augmented Generation (RAG) systems frequently hallucinate incorrect answers instead of abstaining, particularly when the provided context is insufficient. This reduces trustworthiness and overall accuracy.","Deploying RAG systems in applications requiring high accuracy and reliability, where abstaining from an answer is preferable to generating a hallucinated one (e.g., medical, legal domains).","1. Utilize two abstention signals: Model Self-rated Confidence (the LLM's estimated probability of correctness) and Sufficient Context Autorater Output (a binary label indicating context sufficiency). 2. Train a simple logistic regression model using these signals to predict the likelihood of hallucination. 3. At inference time, apply a threshold to the logistic regression model's score: if the score falls below the threshold, the LLM abstains; otherwise, it generates an answer. This allows for a controllable accuracy-coverage tradeoff.","Significantly improves the selective accuracy-coverage tradeoff, leading to gains of 2-10% in the fraction of correct answers among total model responses compared to using self-rated confidence alone. Offers a controllable mechanism for tuning abstention behavior.",Reducing hallucinations and improving trustworthiness in RAG systems; Implementing controllable accuracy-coverage tradeoffs in LLM applications; Guiding LLMs to abstain when uncertain or when context is inadequate.,27
Sample-Efficient RL with Reference Reuse,"Reinforcement Learning (RL) for multi-stage tasks can be highly sample-inefficient, particularly when certain critical stages (e.g., answer generation) are shorter or less frequent but contribute significantly to the overall reward, while other stages (e.g., browsing) are longer and more exploratory.","An RL agent is trained on a task composed of sequential phases (e.g., a browsing phase followed by an answering phase), where the reward is primarily determined by the outcome of a later, shorter phase, and the initial phases are computationally expensive or time-consuming.","To improve sample efficiency, after a complete multi-phase episode, generate additional 'answering-only' episodes. In these supplementary episodes, the agent is provided with the references (or relevant information) collected during the preceding full browsing phase and is tasked solely with optimizing its answer generation based on that fixed set of references.","Significantly boosts the sample efficiency of RL training by allowing the model to practice and optimize the high-impact, shorter phases (like answer generation) more frequently without repeatedly incurring the computational cost of the longer, exploratory phases (like web browsing). This leads to faster learning and improved overall performance.","Accelerating RL training for multi-stage tasks, optimizing specific sub-components of an agent's behavior, reducing computational costs in complex RL environments.",27
Sufficient Context Autorater,"Accurately and scalably determining if a given context provides enough information for an LLM to answer a query, especially in Retrieval Augmented Generation (RAG) systems, without relying on a ground truth answer. Prior methods lacked a precise definition of relevance.","Analyzing and improving Retrieval Augmented Generation (RAG) systems, evaluating LLM performance with varying context quality, or preparing datasets for RAG. The goal is to distinguish between LLM failures to utilize context and context insufficiency.","1. Define 'Sufficient Context': An instance (Q, C) has sufficient context if there exists a plausible answer A to Q given C. This definition does not require a pre-existing ground truth answer. 2. Implement an 'Autorater': Use a powerful LLM (e.g., Gemini 1.5 Pro 1-shot) prompted with the sufficient context definition and examples to classify query-context pairs as 'sufficient' or 'insufficient'.","Achieves 93% accuracy in classifying sufficient context, enabling scalable labeling of instances. Provides crucial insights into LLM behavior in RAG, revealing that models hallucinate even with sufficient context and often hallucinate more than abstain with insufficient context.","Analyzing RAG system performance and error stratification; Data labeling for RAG datasets; Providing a signal for downstream AI components, such as selective generation.",27
Iterative Retrieval and Decision-Making,"In RAG systems, a single retrieval step might not provide sufficient or optimal context, leading to suboptimal answers or hallucinations. The system needs to dynamically decide if more information is needed or if it should abstain.","RAG systems where the quality or completeness of initial retrieved context is uncertain, and the system needs to adapt its information gathering strategy dynamically to improve accuracy and reduce errors.","After an initial retrieval, the system evaluates the sufficiency of the current context (e.g., using a 'Sufficient Context Autorater' or a confidence score). Based on this evaluation, it makes a decision: 1) Generate an answer if the context is deemed sufficient and confidence is high. 2) Refine the query and perform another retrieval step if the context is insufficient but potentially retrievable. 3) Abstain if the context is insufficient and further retrieval is unlikely to help, or if overall confidence is too low.","Potentially higher accuracy by ensuring more relevant and sufficient context, reduced hallucinations by abstaining when truly uncertain, and more efficient use of retrieval resources by avoiding unnecessary generation or retrieval.","Enhancing RAG systems with dynamic information gathering, improving robustness to initial retrieval failures, optimizing resource usage in complex question answering, enabling adaptive LLM behavior.",27
Conditional Retrieval,"Fixed-interval retrieval (e.g., using a retrieval stride) can be inefficient, leading to unnecessary retrieval calls when not needed, or missing critical retrieval opportunities when needed more frequently. This impacts latency and cost.","An InContext RALM system is operating, and there's a desire to optimize retrieval operations beyond fixed intervals, making them more dynamic and efficient.","Employ a specialized model to predict when retrieval is needed. Instead of retrieving documents at a fixed stride, the system only triggers a retrieval operation when this predictive model indicates that external knowledge is likely to be beneficial or necessary for the upcoming generation.","Potential for large latency and cost gains by retrieving more sparsely and only when contextually relevant, leading to a more efficient and potentially more performant RALM system.","Optimizing resource utilization in RALM, reducing inference costs, improving real-time performance of knowledge-augmented LMs.",27
Self-Reflection for RAG (SelfRAG),Retrieval-Augmented Generation (RAG) systems can suffer from suboptimal retrieval or generation if they lack an internal mechanism to critically evaluate and improve their own process and outputs.,Advanced RAG systems where the LLM needs to not only retrieve and generate but also actively critique its own performance and adapt its strategy to enhance the quality and reliability of its responses.,"Train an LLM to dynamically retrieve, generate, and *critique* its own generated text and the relevance of retrieved documents through a self-reflection mechanism. This involves the model predicting a special retrieval token to trigger document retrieval, and then evaluating the retrieved content and its own generated answer, potentially leading to further retrieval, refinement, or alternative generation paths.","Significantly improves the quality, accuracy, and reliability of RAG outputs by enabling the LLM to self-correct, refine its responses, and make more informed decisions about when and what to retrieve, acting as a more sophisticated and autonomous agent.","Advanced RAG systems, self-improving AI agents, quality assurance in generative AI, complex reasoning tasks requiring internal evaluation and adaptation.",27
Dynamic Speculative Pipelining for RAG,"In RAG workflows, the retrieval step (often CPU-bound) and the LLM generation step (GPU-bound) are typically executed sequentially. This sequential execution leads to idle GPU resources during retrieval and prolonged end-to-end latency, particularly when retrieval latency is significant.","RAG systems where vector search can produce preliminary candidate documents early in its process, even before the final, most relevant documents are fully determined. The goal is to reduce overall latency by overlapping these two distinct computational phases.","Dynamically overlap the knowledge retrieval and LLM inference steps. The retrieval process is split into stages, and at each stage, candidate documents are sent to the LLM engine for 'speculative generation.' If subsequent stages yield different candidate documents, the previous speculative generation is terminated, and a new one is initiated. If the documents remain the same, the LLM continues processing the current speculative generation. The strategy is dynamically enabled based on system load (e.g., only when the number of pending LLM requests falls below a certain threshold).",Achieves up to 16% Time to First Token (TTFT) reduction. Decreases non-overlapping vector search time by 15-43%. Minimizes end-to-end latency by parallelizing retrieval and generation while managing the overhead of incorrect speculations.,"Reducing end-to-end latency in RAG systems by intelligently overlapping the knowledge retrieval and LLM generation phases, especially when retrieval is time-consuming.",27
Adaptive Retrieval (Entity Frequency-based),"Incurring retrieval overhead for simple queries or failing to retrieve for complex ones, without a dynamic decision mechanism. Traditional RAG approaches apply retrieval uniformly, which is inefficient for queries easily answerable by the LLM's parametric memory.","LLM-based Question Answering (QA) systems where some queries might be answerable by the LLM's parametric memory, while others require external knowledge, and a simple heuristic can differentiate between them to optimize retrieval usage.","Dynamically decide whether to retrieve documents or not based on the frequency of entities found in the query. If the frequency of entities in a query falls below a certain predefined threshold (implying less common or more specific knowledge), activate the retrieval module. Otherwise, rely solely on the LLM's internal knowledge (no retrieval).","Reduces unnecessary retrieval operations for common knowledge queries, thereby improving efficiency and reducing computational costs. However, this approach is often overly simplistic and may not be sufficient for complex multi-hop queries that require deep reasoning or synthesis from multiple documents.","Simple adaptive RAG decision-making, efficiency optimization for LLM applications with varying query types, as a baseline for more sophisticated adaptive systems, scenarios where a quick heuristic is preferred over a complex classifier.",27
Iterative Retrieval-Augmented Generation (Iterative RAG),"A single retrieval step at the beginning of RAG might not provide sufficient or optimally refined context for complex, multi-step, or evolving generation tasks, potentially leading to less accurate or complete responses.","RAG systems dealing with complex queries, multi-turn conversations, or tasks requiring dynamic information gathering where the relevance of retrieved documents might change or new information is needed as the LLM generates parts of the response.","Instead of a single retrieval at the beginning, the RAG process involves multiple retrieval steps interleaved with the generation process. After an initial generation, the LLM (or a controller) can identify new information needs or refine existing queries, triggering subsequent retrieval rounds. The newly retrieved documents then augment the context for further generation. RAGCache supports this by treating intermediate iterations as separate requests and caching their KV cache.","Improved response quality for complex tasks, better contextual understanding, and the ability to dynamically adapt to evolving information needs during generation.","Multi-hop question answering, complex content creation, reasoning tasks that require dynamic information gathering, conversational AI.",27
Confidence-Based Iterative Retrieval,"LLMs may generate responses with low confidence or factual inaccuracies when their internal knowledge or initially retrieved documents are insufficient, leading to unreliable outputs.","Generative LLM applications, particularly in Question Answering, where the quality and factual accuracy of generated text are critical, and the system needs a mechanism to dynamically seek more information when uncertain.","Monitor the confidence level of tokens or sentences generated by the LLM. If the confidence falls below a predefined threshold, trigger an additional retrieval step to fetch new, potentially more relevant or supplementary documents. This process can be repeated iteratively until the LLM's confidence in its generated output is sufficient or a maximum number of retrieval steps is reached.","Improves the reliability and accuracy of LLM-generated responses by dynamically addressing knowledge gaps or uncertainties. It allows the system to self-correct and enhance its information base during generation, reducing factual errors and improving overall output quality.","Real-time fact-checking, dynamic knowledge augmentation, improving robustness of generative AI, reducing hallucinations based on internal uncertainty signals.",27
Adaptive Retrieval-Augmented Generation (AdaptiveRAG),"Existing Retrieval-Augmented Large Language Models (LLMs) either incur unnecessary computational overhead for simple queries (e.g., multi-step approaches) or fail to adequately address complex multi-step queries (e.g., single-step or no retrieval approaches), leading to suboptimal efficiency and accuracy across diverse query complexities. One-size-fits-all approaches are inadequate for real-world scenarios with varying query difficulties.","Developing Question Answering (QA) systems or other LLM applications that need to provide accurate and efficient responses to user queries, where the complexity of these queries can range from straightforward to highly complex and multi-hop.","Dynamically select the most suitable strategy for retrieval-augmented LLMs from a range of options (no retrieval, single-step retrieval, or multi-step iterative retrieval) based on the predicted complexity level of the incoming query. This selection process is operationalized by a smaller Language Model (Classifier) trained to predict the query's complexity. The system seamlessly adapts its operational strategy without changing internal model architecture or parameters.","Significantly enhances the overall efficiency and accuracy of QA systems. It optimizes resource allocation by applying simpler, more efficient methods for straightforward queries and more rigorous, iterative methods for complex queries, thereby avoiding unnecessary computational overhead or insufficient handling.","Question Answering systems, information retrieval, dynamic resource management in LLM applications, any LLM-based system where query complexity varies and different processing strategies are optimal.",27
Query Complexity Classifier,"To enable dynamic adaptation of LLM-based strategies (e.g., retrieval augmentation) to incoming queries, there is a need for an accurate and efficient mechanism to determine the complexity level of each query.","An adaptive LLM system (such as AdaptiveRAG) that needs to choose among different processing strategies (e.g., no retrieval, single-step retrieval, multi-step retrieval) based on the query's inherent difficulty, reasoning requirements, or external knowledge needs.","Employ a dedicated, smaller Language Model (Classifier) that is trained to classify incoming queries into predefined complexity levels (e.g., 'straightforward' requiring no retrieval, 'moderate' requiring single-step retrieval, 'complex' requiring multi-step retrieval). This classifier takes the raw query as input and outputs a corresponding complexity label.","Provides the necessary input for adaptive LLM frameworks to select the most appropriate and efficient strategy for each query, leading to improved overall performance, reduced latency for simple queries, and better accuracy for complex ones.","Dynamic strategy selection in LLM applications, resource optimization in AI systems, personalized query handling, intelligent routing of queries in complex AI pipelines.",27
Multi-step Retrieval-Augmented Generation (Multi-step RAG),"Complex queries, particularly 'multi-hop' questions that require connecting and aggregating information from multiple documents, cannot be adequately answered by single-step retrieval or no-retrieval approaches. These queries demand iterative reasoning and information synthesis.","Question Answering tasks or other knowledge-intensive applications where queries necessitate synthesizing information from multiple source documents, performing iterative reasoning, or decomposing a complex problem into simpler sub-problems.","The LLM interacts with a Retriever in multiple rounds. In each step, new documents are retrieved from an external knowledge base based on the current query and an evolving context (which can include previous documents and intermediate answers). The LLM progressively refines its understanding and generates intermediate answers, often employing techniques like Chain-of-Thought reasoning, until a final, comprehensive answer is formulated.","Effectively handles complex, multi-hop queries by building a more comprehensive and extensive foundation of information and reasoning steps. This leads to higher accuracy for questions that require deep understanding and synthesis across multiple knowledge sources, albeit at a higher computational cost.","Complex open-domain QA, multi-document summarization, iterative information seeking, knowledge graph completion, advanced reasoning tasks requiring external knowledge.",27
Automatic Classifier Training Data Generation (for Query Complexity),"The absence of pre-annotated datasets for query-complexity pairs makes it challenging to train a query complexity classifier, which is crucial for adaptive LLM systems.","Developing a query complexity classifier for an adaptive LLM framework (e.g., AdaptiveRAG) where manual labeling of query complexity is impractical, time-consuming, or unavailable.","Automatically construct training datasets for the query complexity classifier using a combination of two strategies: 1) **Model Prediction Outcomes:** Label queries based on which of the different retrieval-augmented LLM strategies (no retrieval, single-step, multi-step) correctly answers them, prioritizing simpler models if multiple succeed. 2) **Inherent Dataset Biases:** For queries that remain unlabeled after the first step, assign labels based on the known inductive biases of existing benchmark datasets (e.g., queries from single-hop datasets are labeled as moderate, and multi-hop datasets as complex).","Enables the training of an effective query complexity classifier without requiring human labeling, making the development of adaptive LLM systems more feasible and scalable. This approach improves classifier accuracy and generalization, especially for handling diverse query types.","Bootstrapping training data for new AI tasks, reducing reliance on human annotation, developing classifiers for dynamic AI system control, data generation pipelines for ML models.",27
Denoising Sequence-to-Sequence Pretraining (BART-like),"Training a robust and versatile sequence-to-sequence model that can perform well across a diverse set of generation, translation, and comprehension tasks, and serve as a strong parametric memory component in hybrid systems.","Developing a general-purpose language model capable of both understanding and generating text, often used as a base for fine-tuning on specific downstream tasks.","Pretrain an encoder-decoder transformer model using a denoising objective. This involves corrupting text with various noising functions (e.g., token masking, token deletion, text infilling, sentence permutation) and then training the model to reconstruct the original uncorrupted text. The encoder processes the corrupted input, and the decoder generates the original sequence.",Produces a powerful pretrained seq2seq model (parametric memory) that achieves state-of-the-art results on diverse generation tasks and can be effectively integrated into hybrid architectures like RAG.,"Generator component in RAG, general text generation, translation, comprehension, fine-tuning for various NLP tasks.",28
Thorough Decoding (RAGSequence),"Accurately approximating arg max_y p(y|x) for the RAGSequence model, where the likelihood p(y|x) does not break into a conventional per-token likelihood, making standard beam search insufficient.","Decoding for the RAGSequence model, which marginalizes over a single latent document for the entire output sequence, requiring a more complex approach than standard autoregressive decoding.","Run beam search independently for each of the top-K retrieved documents. This yields a set of candidate hypotheses. For any hypothesis y that did not appear in the beam of all documents, run an additional forward pass for each document z for which y was not generated. Then, multiply the generator probability p(y|x, z) with the retriever probability p(z|x) and sum these probabilities across all documents to estimate the marginal probability p(y|x).","Provides a more accurate estimation of the marginal likelihood for RAGSequence, leading to potentially better decoding quality, but at the cost of higher computational expense due to multiple forward passes.",High-quality decoding for RAGSequence models when computational resources allow.,28
Marginalization over Latent Documents,"How to robustly integrate information from multiple potentially relevant retrieved documents into a single generation process, accounting for uncertainty or varying relevance of each document, rather than relying on a single 'best' document.","Retrieval-augmented generation where multiple documents are retrieved for a given query, and the model needs to synthesize or select the most appropriate information from these documents to generate an output, especially when no single document contains the complete answer.","Treat the retrieved documents as latent variables. The final probability of the generated sequence (or next token) is obtained by summing (marginalizing) the probabilities conditioned on each retrieved document, weighted by the retriever's probability of that document given the query. This is typically approximated using the top-K retrieved documents.","Allows the model to leverage information from multiple sources, leading to more robust and accurate generation. It can even generate correct answers when the exact answer is not explicitly present in any single retrieved document, by combining clues.","Probabilistic integration of multiple retrieved contexts in language generation, enhancing robustness and completeness of generated responses.",28
Fast Decoding (RAGSequence),"The computational cost of Thorough Decoding for RAGSequence can be prohibitive for longer output sequences, as it requires many additional forward passes.","Decoding for the RAGSequence model, where a more efficient, albeit approximate, decoding strategy is needed to manage computational resources.","Make an approximation that p(y|x, z) = 0 if hypothesis y was not generated during beam search from x and document z. This avoids the need to run additional forward passes for hypotheses that did not appear in the beam of a particular document. The candidate set Y is generated from the initial beam searches.","Significantly more efficient decoding for RAGSequence models, reducing runtime, especially for longer sequences, by trading off some accuracy for speed.","Efficient decoding for RAGSequence models, particularly when speed is critical or output sequences are long.",28
RAGSequence,How to ensure a consistent and coherent context from retrieved documents is used for generating an entire output sequence in retrieval-augmented generation.,"Retrieval-augmented generation tasks where the complete target sequence should ideally be conditioned on a single, unified context derived from retrieved information.","The model uses the same retrieved document (or set of top-K documents) to generate the complete target sequence. It treats the retrieved document as a single latent variable that is marginalized to compute the sequence-to-sequence probability. During decoding, beam search is performed for each document, and the probabilities are then marginalized across these document-specific hypotheses.","Produces coherent and factually grounded complete sequences. In some cases, it leads to more diverse generations compared to RAGToken.","Open-domain Question Answering, Abstractive Question Answering, Fact Verification.",28
End-to-End Joint Training (Retriever-Generator),"Optimizing the entire retrieval-augmented generation pipeline where both the retrieval mechanism and the generation model need to be aligned and improved for the downstream task, especially when direct supervision for the retrieval step (e.g., which document is 'best') is unavailable.","Hybrid AI systems combining distinct components (e.g., a retriever and a generator) where the performance of one heavily influences the other, and the goal is to optimize the overall system for a specific task.","Jointly train the retriever and generator components by minimizing the negative marginal log-likelihood of the target sequence, treating the retrieved document as a latent variable. The retriever's query encoder and the generator are fine-tuned, while the document encoder and its index can be kept fixed for computational efficiency during training.","Significantly improves results for all tasks compared to freezing the retriever, demonstrating the effectiveness of learning to retrieve relevant information specifically for the downstream generation task.",Training RAG models for various knowledge-intensive NLP tasks.,28
RAGToken,"How to allow the generator maximum flexibility to draw information from different retrieved documents at different points in the output sequence, enabling more granular control and synthesis of facts.","Retrieval-augmented generation tasks where different parts of the output sequence might benefit from distinct pieces of retrieved evidence, or when combining information from multiple sources is advantageous for a richer response.","The model can draw a different latent document for each target token and marginalize accordingly. This allows the generator to dynamically select content from several documents when producing each token. During decoding, it functions as a standard autoregressive seq2seq generator, where the transition probability for the next token marginalizes over the retrieved documents.","Often performs better on tasks requiring the combination of information from multiple sources (e.g., Jeopardy question generation, where questions might contain two separate pieces of information).","Open-domain Question Answering, Abstractive Question Answering, Jeopardy Question Generation, Fact Verification.",28
Feedback Loop / Self-Correction,"Agents may make errors, get stuck in dead loops, or produce plans that violate constraints, requiring a mechanism to detect and rectify these issues dynamically.","An agent's actions or generated plan needs to be evaluated against criteria (e.g., environmental feedback, constraints, commonsense), and adjustments are needed if discrepancies are found.","The agent receives feedback from the environment or an evaluation mechanism (e.g., 'Observation' in ReAct, 'reflection model' in Reflexion, or explicit constraint checks). Based on this feedback, the agent modifies its internal state, reasoning, or subsequent actions/plans to correct errors or improve adherence to requirements.","Enhances the agent's adaptability, robustness, and ability to converge on a correct solution by iteratively refining its approach based on observed outcomes, preventing persistent errors and dead loops.","Dynamically adjusting plans based on environment feedback (e.g., unavailable flights), rectifying persistent errors, optimizing costs, and ensuring plans adhere to commonsense and hard constraints.",29
Multi-Round Self-Correction,"LLMs can make errors during complex reasoning or tool execution (e.g., syntax errors, runtime errors, inappropriate tool use, incorrect reasoning steps), and a single-pass approach does not allow for recovery or refinement.","Designing an agent to robustly solve complex, multi-step problems that involve external tools, where intermediate feedback and error recovery are crucial.","The agent operates in an iterative loop. After generating a rationale and executing a program with an external tool, it processes the tool's output. If the output indicates an error (e.g., `SyntaxError`, `RuntimeError`, unexpected result) or if the reasoning path is suboptimal, the agent generates a new rationale to analyze the feedback, identify the error, and formulate a correction or an alternative plan for the next step, then attempts tool use again. This process continues until a correct answer is derived or a maximum number of attempts is reached.","Enhances the agent's ability to recover from errors, refine its problem-solving approach, and adapt to unexpected tool feedback, leading to higher accuracy and robustness in complex tasks like mathematical reasoning.","Complex problem-solving, interactive systems, tasks requiring dynamic adaptation and error recovery, especially when integrating external tools.",29
World Model / Simulation-based Planning,"Traditional AI agents often operate without a comprehensive understanding of the environment's dynamics, limiting their ability to anticipate outcomes and explore alternative futures.",An agent needs to make complex decisions in a dynamic environment where predicting the consequences of actions is crucial for effective planning.,"The agent maintains or constructs an internal 'world model' that represents the environment's state and dynamics. It uses this model to run 'simulations' of potential action sequences, predicting their outcomes without actual execution. This allows for exploration of alternative plans and deliberation.","Enables more robust and informed planning by allowing the agent to 'try out' actions virtually, anticipate future states, and select optimal or near-optimal plans, especially in complex, uncertain, or long-horizon scenarios.","Exploring alternative plans, deliberation, anticipating outcomes of actions in complex planning scenarios like travel.",29
Self-Correction / Feedback Loop,"LLMs, especially when interacting with external tools or performing multi-step reasoning, can make errors (e.g., incorrect tool calls, invalid arguments, logical flaws) that lead to incorrect final answers.","LLM-based agents operate in environments where actions have observable outcomes, and these outcomes can indicate success or failure.","Design the LLM's operational loop to incorporate feedback, either from the environment (e.g., tool execution results, error messages) or through internal self-reflection on its generated thoughts and actions. The LLM then uses this feedback to identify mistakes, revise its plan, or correct its subsequent actions.","Improves the robustness, accuracy, and adaptability of LLM-based systems, allowing them to recover from errors, refine their strategies, and achieve better performance on challenging tasks.","Robust tool-augmented LLMs, complex reasoning agents, interactive systems requiring adaptive behavior.",29
Reflexion,"Language agents can make errors, get stuck in loops, or produce suboptimal plans, and need a mechanism to learn from and correct these mistakes over multiple attempts.","An agent has attempted a task, but its execution resulted in failure, suboptimal outcomes, or violations of constraints, requiring a meta-level correction mechanism.","After an attempt, a 'reflection model' (often another LLM call) analyzes the agent's past trajectory, identifies errors or inefficiencies, and generates 'high-level insights' or verbal reinforcement. This feedback is then used to guide subsequent attempts or refine the agent's strategy for future planning and execution.","Improves the agent's ability to self-correct, learn from failures, and refine its planning and execution over multiple attempts, leading to more robust and successful task completion.","Providing 'high-level insights on previous erroneous attempts' to improve planning, especially in scenarios where initial plans fail or are suboptimal.",29
Debate-Style Evidence Aggregation,"AI models, when tasked with generating answers with references, might 'cherry-pick' sources that support a particular claim, even if the overall evidence is mixed or contradictory, leading to biased or incomplete answers.","An AI system needs to provide a balanced and comprehensive assessment of information, especially for complex or controversial topics, rather than just supporting a single viewpoint.",Train models in a 'debate-like' setup where they are incentivized to find and present evidence both *for* and *against* different claims. This could involve having multiple models argue different sides or a single model generating arguments and counter-arguments.,"The AI system provides more balanced, nuanced, and robustly supported answers, reducing the risk of cherry-picking evidence and fostering a more complete understanding of complex issues.","Generating balanced summaries, critical analysis of information, mitigating confirmation bias, improving robustness of factual claims.",30
Trust Calibration through Transparency,"Users tend to over-rely on AI outputs (automation bias), especially when they appear authoritative (e.g., with citations), even if the AI makes mistakes, particularly on out-of-distribution questions.","An AI system generates information that users might consume and act upon, and it's crucial to prevent over-reliance and foster appropriate trust.","Design the AI system and its interface to provide transparency into its operation and limitations. This includes: explicitly documenting limitations and potential failure modes (e.g., struggles with out-of-distribution questions), providing mechanisms for users to verify information (e.g., traceable references), and designing the output style to avoid undue authoritativeness where uncertainty exists.","Users are better informed about the AI's capabilities and limitations, leading to more appropriate trust levels, reduced automation bias, and more critical engagement with AI-generated content.","Building trustworthy AI systems, mitigating risks of misinformation, promoting critical thinking in AI users, designing user-friendly AI interfaces.",30
Vanilla Prompting (for bias mitigation),Reducing biases in LLM outputs.,When LLMs might perpetuate biases or stereotypes in their responses.,Consists simply of an instruction in the prompt that tells the LLM to be unbiased (also referred to as moral self-correction).,Aims to elicit less harmful and more fair outputs from LLMs.,"Bias mitigation, ethical AI development, fair content generation.",30
AttrPrompt,Avoiding text biased towards certain attributes when generating synthetic data.,"When generating synthetic data, and traditional approaches might produce data biased towards specific lengths, locations, or styles.","1. Ask the LLM to generate specific attributes that are important to alter for diversity (e.g., location). 2. Prompt the LLM to generate synthetic data by varying each of these attributes.","Generates synthetic data with varied attributes, avoiding bias towards specific characteristics.","Synthetic data generation, bias mitigation in data creation.",30
Bias-Aware Design & Mitigation,"AI models, especially large language models, can inherit and amplify biases present in their training data, leading to perpetuation of stereotypes, reinforcement of existing beliefs, and exclusion of certain identities.","An AI system is being developed or deployed, and there's a risk of it exhibiting harmful biases in its outputs, search behavior, or synthesis of information.","Actively analyze and acknowledge potential sources of bias (e.g., base model biases, internet search data, human contractor viewpoints). Implement strategies to mitigate these biases, such as improving the base model, refining training objectives (e.g., using debate-like setups to find evidence for/against claims), controlling application usage, and providing clear documentation of limitations.","Increased awareness and understanding of model biases, and a framework for developing and implementing strategies to reduce the perpetuation and reinforcement of harmful biases in AI systems.","Developing ethical AI systems, ensuring fairness, reducing harmful societal impacts of AI.",30
Constitutional AI for Ethical Alignment,"Large Language Models (LLMs) in personalization systems can generate inaccurate or misleading information (factuality issues), reflect biases from their training data (discrimination), and potentially produce harmful content (ethical concerns). This undermines user trust and responsible AI deployment.","Personalization systems require LLMs to be not only helpful but also honest and harmless. However, the vast and often uncurated nature of LLM training data can lead to undesirable behaviors that conflict with human values and ethical guidelines.","Implement 'Constitutional AI' approaches, which involve training LLMs to align with a predefined set of principles or a 'constitution.' This is achieved through a process of AI-driven critiques, revisions, and supervised learning from AI feedback, guiding the model to generate responses that adhere to desired ethical standards (e.g., helpfulness, honesty, harmlessness) without extensive human labeling.","Improved factuality, reduced bias, enhanced privacy protection, and more ethical content generation in personalized recommendations. This fosters greater user trust, ensures responsible AI behavior, and mitigates risks associated with misinformation, discrimination, and harmful content.","The concept of Constitutional AI (Bai et al., 2022) is proposed as a solution to address the tradeoff between helpfulness, honesty, and harmlessness in LLMs.",30
Selecting Balanced Demonstrations (for bias mitigation),"Reducing biases in LLM outputs, particularly in FewShot settings.","When using FewShot Prompting, and the distribution of exemplars might introduce or amplify biases.",Select balanced demonstrations or obtain demonstrations optimized over fairness metrics.,Can reduce biases in LLM outputs by providing a fair representation in exemplars.,"Bias mitigation in FewShot learning, fair content generation.",30
Tool Retrieval and Selection,"After task planning, efficiently and accurately identifying the most appropriate tool(s) for each subquestion from a potentially vast and diverse array of available tools, especially under context length and latency constraints.","Subquestions generated from task planning, and a tool library that can range from a few to thousands of tools, each with descriptions and parameter lists.","Employ a two-step approach: 1) **Retriever-based Selection** (for large tool sets) using sparse (e.g., TFIDF, BM25) or dense (e.g., SentenceBert, neural networks) retrieval methods to filter and identify the top-K most relevant tools. 2) **LLM-based Selection** (for limited sets or after retrieval) where the LLM selects the optimal tool(s) from the provided list based on tool descriptions and the subquestion, often requiring reasoning for serial tool calling.","Efficiently narrows down the pool of potential tools and selects the most suitable one(s) for a given subquestion, optimizing efficiency and effectiveness in tool usage while managing context limitations.","Choosing the right API, function, or external resource for a specific subtask, managing large tool libraries.",31
LLM-API Integration System,"Large Language Models (LLMs) are limited by their static training data and unawareness of external, frequently updated tools/APIs, hindering their ability to perform complex computational tasks and act as flexible interfaces to the digital world.","An LLM needs to extend its capabilities beyond traditional natural language processing by interacting with a dynamic, massive, and diverse ecosystem of external APIs to accomplish complex tasks and provide actionable code.","Design and implement a comprehensive system that tightly integrates an LLM with a vast API database. This system involves: 1) **API Database Curation:** Aggregating, filtering, and structuring API documentation into a standardized format (e.g., JSON objects with detailed fields like domain, functionality, arguments, performance). 2) **Synthetic Instruction Generation:** Employing a self-instruct paradigm with a powerful LLM to generate a large dataset of natural language instruction-API call pairs, including examples with user-defined constraints. 3) **Retriever-Aware Finetuning:** Finetuning a base LLM (e.g., LLaMA) using this dataset, where retrieved API documentation is incorporated during training to teach the LLM to adapt to test-time changes and critically evaluate the relevance of retrieved context. 4) **Inference with Dynamic Retrieval:** During inference, using a retriever to fetch the most up-to-date API documentation based on user queries, augmenting the prompt with this documentation, and having the finetuned LLM generate the precise API call and supporting explanation. 5) **AST-based Evaluation:** Utilizing Abstract Syntax Tree (AST) subtree matching for robust, offline evaluation of functional correctness and hallucination in the generated API calls.","The integrated system (Gorilla) achieves state-of-the-art performance in generating accurate API calls across thousands of functions and libraries. It demonstrates strong adaptability to API changes, effectively reasons about user-defined constraints, and substantially mitigates API argument hallucination, transforming the LLM into a reliable tool-user.","Enabling LLMs to interact with external software and services, automating complex workflows requiring external tool use, building LLM-powered agents for digital interaction, enhancing LLM reliability and applicability in dynamic environments.",31
Parameter Extraction and Tool Invocation,"After selecting a tool, the LLM needs to correctly extract the necessary parameters from the user query or current context, format them precisely according to the tool's specifications, and then invoke the tool, while also handling potential invocation errors.",A selected tool with its detailed documentation (specifications for parameters and format) and a subquestion or current context containing the required information.,"The LLM extracts required parameters (content and format) from the user query or context, adheres strictly to the tool's prescribed output format, and then invokes the tool. Error handling mechanisms are integrated to refine actions based on error messages returned upon calling failure (e.g., incorrect formatting, out-of-range parameters, server errors). This can be achieved via tuning-free (e.g., few-shot, rule-based, documentation compression) or tuning-based methods.","Successful and accurate execution of external tools, leading to the generation of tool-specific outputs that can be used for subsequent steps or final response generation, with resilience against common invocation errors.","Making API calls, executing code, querying databases, interacting with external systems, handling real-time data requests.",31
"Modular Reasoning, Knowledge and Language (MRKL) System","Allowing LLMs to make use of external systems for tasks like mathematical computations, reasoning, and factuality.",When LLMs have shortcomings in specific areas and can benefit from external tools.,"Contains an LLM router providing access to multiple tools (e.g., calculator, weather API). The router makes multiple calls to get information and combines it to generate a final response.","Extends LLM capabilities by integrating external tools, improving accuracy and factuality.","Complex problem-solving, factual question answering, real-world interaction.",31
Domain-Specific Tool Integration,"LLMs, trained on general knowledge, often exhibit deficiencies in specialized domains such as complex mathematics, code generation, chemistry, biology, economics, or medicine.",LLMs needing to perform tasks requiring deep expertise or precise calculations beyond their general training data.,"Employ specific external tools like online calculators, Python interpreters, or specialized scientific/economic/medical tools to augment the LLMs' domain-specific expertise.","Mitigates the expertise gap in LLMs, enhancing their utility in specialized applications by providing access to domain-specific knowledge and computational capabilities.","Performing complex calculations, solving equations, analyzing statistical data, executing programming code, addressing chemistry/physics problems, generating recommendations.",31
Agentic Policy for LLM Orchestration,"In complex, multi-step AI systems involving LLMs and external tools, there's a need for a mechanism to intelligently decide the next action to take (e.g., when to retrieve knowledge, when to query the LLM, when to send a final response).",Building an AI agent that dynamically interacts with a blackbox LLM and other plug-and-play modules (like knowledge bases or utility functions) to achieve a goal in a conversational or task-oriented environment.,"Introduce a 'Policy' module that, based on the current dialog state stored in 'Working Memory' (including user query, history, evidence, candidate responses, and feedback), selects the most appropriate next system action. This policy can be implemented using manually crafted rules (for initial deployment) or trained using reinforcement learning (e.g., REINFORCE) to maximize an expected reward (e.g., utility score). Actions include calling the Knowledge Consolidator, calling the Prompt Engine to query the LLM, or sending a verified response to the user.","Enables flexible, adaptive, and optimized control over the interaction flow within an LLM-augmented system, allowing the system to make strategic decisions about tool use and response generation.","Managing complex dialog flows, optimizing resource usage (e.g., when to access external knowledge), improving overall task completion efficiency and accuracy in agentic systems.",31
Browser-Assisted LLM,"Traditional LLMs struggle with up-to-date information retrieval and factual accuracy, especially for long-form, open-ended questions, and lack the ability to interact with dynamic, real-world information sources.","An LLM needs to answer complex, long-form questions that require searching and synthesizing information from the web, and its answers need to be factually accurate and supported by evidence.","Fine-tune a large language model (e.g., GPT-3) to interact with a text-based web browsing environment. The model is prompted with the current state (question, page text, cursor location) and issues commands (search, click, scroll, quote, end browsing). It collects references (quotes) during browsing to support its final answer.","The model can search and navigate the web, retrieve up-to-date information, and synthesize paragraph-length answers with supporting references. This significantly improves factual accuracy and human preference for answers compared to base LLMs and even human demonstrators.","Long-form question answering, information synthesis, fact-checking, enabling LLMs to perform tasks requiring external tool use.",31
Live Web Access with Controlled Interaction,"Enabling AI models to access up-to-date, real-world information from the internet while mitigating the significant risks associated with uncontrolled interaction (e.g., exploiting real-world side effects, malicious actions).","An AI system requires access to dynamic, current information from the web to perform its task effectively (e.g., answering questions), but direct, unrestricted web access is too risky.","Provide the AI model with a text-based, controlled web browsing environment. Restrict the model's actions to a predefined, safe set of commands (e.g., sending queries to a search API, following existing links, quoting text). Prevent actions that could have real-world side effects like editing forms or interacting with sensitive systems. Implement monitoring and 'tripwire tests' for exploitative behavior.","The AI gains access to a vast, up-to-date knowledge base, enhancing its capabilities, while the risks of unintended or malicious actions are significantly reduced through environmental constraints and monitoring.","Real-time information retrieval, dynamic knowledge acquisition, enabling AI agents to operate in external environments safely.",31
State Representation for Agentic LLM,"Large Language Models (LLMs) lack inherent memory and perception of dynamic external environments, making it challenging for them to understand the current state and make informed decisions in interactive tasks.","An LLM is designed to act as an agent in a dynamic, interactive environment (e.g., a text-based web browser) and needs to receive comprehensive, structured information about the current state to choose its next action.","Design a structured, text-based summary that encapsulates the current state of the environment. This summary includes critical information such as the user's question, the text content of the current page, the cursor's location, and a record of past actions. This summary is then provided as the primary input (prompt) to the LLM for each decision step.","The LLM can effectively interpret the dynamic state of the environment, enabling it to generate appropriate and contextually relevant commands or actions, thereby facilitating its agentic behavior and interaction with the external world.","Enabling LLMs to operate as agents in interactive environments, providing context for sequential decision-making, facilitating tool use by LLMs.",31
Task Automation via Tools,"LLMs are fundamentally language processors and lack the inherent capability to execute external actions independently, such as reserving conference rooms or booking flight tickets.",Users requiring LLMs to perform real-world actions or automate repetitive tasks that involve interacting with external systems.,"Integrate LLMs with external task automation tools, scheduling tools, project management tools, online shopping assistants, or data table processing tools by populating their interfaces with the necessary parameters.","LLMs can facilitate the execution of external actions, automate repetitive tasks (e.g., scheduling, setting reminders, filtering emails), and enhance practical user assistance, efficiency, and user experience.","Scheduling appointments, setting reminders, filtering emails, managing project tasks, assisting with online shopping, performing data analysis and visualization.",31
Agentic Architecture,"LLMs alone lack the ability to autonomously perform complex, multi-step tasks, interact with dynamic environments, or maintain state over long durations.","When building intelligent systems that need to operate autonomously, interact with external tools, manage information, and plan over extended periods to achieve complex goals.","Design an LLM-powered agent with a modular architecture comprising distinct components such as: Memory (for retaining and processing information), Tool Use (for interacting with external environments, APIs, or databases), and Planning (for decomposing tasks, strategizing action sequences, and making decisions). These modules work in concert, often in an iterative loop, guided by the LLM's reasoning capabilities.","Enables LLMs to exhibit more autonomous, adaptive, and capable behavior, tackling complex real-world problems that are beyond the scope of a single LLM call.","Building autonomous agents like AutoGPT, BabyAGI, HuggingGPT, or the TravelPlanner agent itself, which require sophisticated interaction with the environment and internal state management.",31
Plug-and-Play LLM Augmentation Framework,"Blackbox LLMs are hard to improve for mission-critical tasks due to hallucinations and lack of external knowledge, and fine-tuning is prohibitively expensive.","When needing to enhance a fixed, blackbox LLM's capabilities (e.g., factual grounding, iterative refinement, dynamic decision-making) without modifying its internal parameters.","Design an architecture around the LLM with a set of independent, interchangeable 'plug-and-play' modules (e.g., Working Memory, Policy, Action Executor, Utility). These modules interact with the LLM and the environment, allowing for flexible integration of external knowledge, automated feedback, and dynamic control over the LLM's operation.","Enables significant improvement in LLM performance (e.g., reduced hallucination, increased usefulness) for mission-critical tasks, offering flexibility and cost-effectiveness compared to fine-tuning. The modularity allows for easy updates and customization of individual components.","Enhancing blackbox LLMs, building adaptable AI systems, rapid prototyping of LLM applications, integrating diverse AI capabilities.",31
Agentic Working Memory,AI agents interacting with LLMs need to maintain a coherent and comprehensive understanding of the ongoing conversation or task state across multiple turns and interactions with various modules.,"Designing an AI agent that orchestrates an LLM and other tools in a multi-turn, dynamic environment (e.g., dialog systems, complex question answering) where context persistence is crucial.","Implement a 'Working Memory' module that tracks and stores all essential information related to the current interaction. This includes the user query, consolidated external evidence, LLM-generated candidate responses, utility scores, verbalized feedback, and the complete dialog history. This memory serves as the central state for the 'Policy' module to make decisions and for other modules (like the Prompt Engine) to access relevant context.","Provides a persistent and structured representation of the agent's state, enabling informed decision-making by the policy, consistent context for the LLM, and effective iterative refinement, leading to more coherent and goal-oriented interactions.","Maintaining context in dialog, supporting multi-step reasoning, enabling iterative refinement loops, facilitating policy learning in conversational AI.",31
Memory Management (Working Memory / Short-Term Memory / Long-Term Memory),"LLMs have limited context windows (short-term memory) and may struggle to retain and recall relevant information over long interactions or complex, long-horizon tasks (long-term memory), leading to 'Lost in the Middle' issues.","Language agents need to keep track of intermediate plans, collected information, past interactions, and constraints to inform future decisions and maintain coherence over extended tasks.","Implement distinct memory modules: Short-term memory (working memory/in-context learning) utilizes the LLM's context window, often managed by explicit mechanisms like a 'NotebookWrite' tool to record necessary information. Long-term memory (parametric memory/retrieval) leverages the LLM's inherent knowledge or external retrieval mechanisms (e.g., RAG) and techniques like 'memory summarization' to access broader or previously learned information.","Improves the agent's ability to manage information, maintain context, avoid repetition, and make coherent decisions over extended interactions, preventing context overflow and information confusion.","Recording necessary information for planning, keeping track of multiple constraints, remembering past actions and observations, managing context in long-horizon tasks.",31
Tool Use / Tool Augmentation,"Large Language Models (LLMs) have limited inherent capabilities, access to real-world, up-to-date information, and ability to perform complex calculations or external actions.","Language agents need to interact with external systems, databases, or APIs to gather information, perform calculations, or execute actions beyond their internal knowledge.","Equip the language agent with a 'Toolbox' of external tools (e.g., search engines, calculators, APIs like FlightSearch, CitySearch, RestaurantSearch, DistanceMatrix, AccommodationSearch, AttractionSearch) and the ability to select and use the appropriate tool based on the current task and context. The agent formulates tool calls and processes their observations.","Expands the agent's capabilities, allows access to dynamic and specific information, and enables interaction with the environment, significantly improving performance in real-world tasks.","Information collection (e.g., searching flights, cities, restaurants, attractions, accommodations), calculations (e.g., distance and cost), interacting with external data sources.",31
Multimodal Interaction Augmentation,"LLMs often struggle to consistently understand diverse and multifaceted user queries that encompass multiple languages and modalities (e.g., speech, images), leading to ambiguities in discerning actual user intent.","User interactions involving varied input types beyond pure text, such as spoken commands, images, or foreign languages.","Deploy specialized tools like speech recognition, image analysis, or machine translator tools to significantly enhance the perceptual capabilities of LLMs.","Improved understanding and response to a broader spectrum of user inputs, optimizing dialogue management and intent recognition, and enabling LLMs to manage intricate user interactions more effectively.","Understanding speech inputs, analyzing images, translating languages, enhancing linguistic understanding for dialogue management.",31
Plug-and-Play LLM Module,"Enhancing the capabilities of diverse Large Language Models (LLMs) with specialized functionalities (e.g., external knowledge access, structured planning) often requires extensive retraining or complex integration, limiting flexibility and reusability.","When a specific, specialized AI capability is developed (e.g., a module for generating KG-grounded plans) and needs to be easily and efficiently integrated with various pre-trained or fine-tuned LLM backbones without altering their core architecture or requiring full retraining.","Design a specialized AI module (e.g., a planning module, a retrieval module) that can operate independently or be fine-tuned for its specific task. This module is then integrated with different LLMs during inference, providing its output (e.g., retrieved knowledge, generated plans) as additional context or input to the LLM. The LLM then leverages this augmented input for its reasoning or generation task. This allows for modular enhancement without deep architectural changes to the base LLM.","Substantially improves the performance of various LLMs by augmenting them with specialized, external capabilities. It promotes modularity, reusability, and flexibility, allowing for rapid deployment of new functionalities across different LLM systems without significant overhead.","Augmenting LLMs with external knowledge bases, specialized planning algorithms, domain-specific data processing, or other modular AI functionalities.",31
Tool Orchestration / Chaining,"Solving complex tasks often requires LLMs to interact with and combine multiple distinct external tools in a logical, multi-step sequence.","LLMs are equipped with a diverse set of specialized tools (e.g., text retrieval, database operations, mathematical calculators, code interpreters, graph tools), each designed for a specific function.","Enable LLMs to plan and execute a sequence of tool calls, breaking down a complex problem into intermediate steps. This involves selecting the appropriate tool for each step, generating correct arguments, and passing outputs from one tool as inputs to another, forming a 'tool chain.'","Allows LLMs to solve challenging tasks that require complex reasoning and the synergistic use of multiple functional tools, going beyond single-tool usage.","Multi-step question answering, complex data analysis, automated task execution involving multiple external systems.",31
Tool Augmentation,"Large Language Models (LLMs) suffer from challenges such as hallucination, weak numerical reasoning, and lack of access to up-to-date or specific external knowledge.","LLMs possess vast internal knowledge from pretraining but have inherent limitations in factual accuracy, numerical computation, and real-time information access.","Enhance LLMs' capabilities by integrating them with external specialized tools such as retrieval systems, math tools (e.g., WolframAlpha), code interpreters (e.g., Python, SQL), and structured databases.","Improves LLMs' question-answering abilities, mitigates hallucinations by providing verified information, and enhances numerical and logical reasoning by offloading tasks to specialized systems.","Enhancing factual accuracy, improving numerical reasoning, accessing real-time data, solving domain-specific problems.",31
Memory Augmentation for LLMs,"LLMs have limited context windows and struggle to retain and leverage information from past interactions or long-term experiences, hindering their ability to learn and adapt over extended periods or complex dialogues.","LLMs need to maintain state, learn from successes and failures, and adapt their behavior based on historical data beyond the immediate prompt.","Augment LLMs with external memory capabilities that allow them to store and retrieve past experiences, observations, or learned knowledge. This memory can be used to inform future decisions, adapt strategies, and overcome context window limitations.","Enhances LLMs' ability to learn and adapt based on past experiences, whether successes or failures, leading to more consistent, context-aware, and personalized interactions over time.","Long-term conversational agents, adaptive problem-solvers, personalized AI experiences, agents operating in dynamic environments.",31
Context Window Management,"The limited context window of LLMs restricts the amount of information (tool descriptions, few-shot examples, interaction history, reasoning traces) that can be provided in a single prompt, leading to truncated context or difficulty in processing complex instructions.","Tool-augmented LLMs require extensive contextual information to operate effectively, including detailed tool specifications, multiple examples of tool use, and a history of interactions for self-correction and planning.","Employ strategies to efficiently manage and utilize the LLM's context window. This can involve techniques like summarizing past interactions, dynamically selecting the most relevant few-shot examples, using more concise tool descriptions, abstracting or pruning less critical information, or employing external memory systems to offload context.","Enables LLMs to handle more complex tool-use scenarios and longer interaction histories within their context limitations, improving their ability to understand and follow intricate instructions without exceeding token limits or suffering from 'Too Long Context' errors.","Designing robust prompts for complex tool-augmented LLMs, managing long-running agentic conversations, optimizing performance under context window constraints.",31
Retrieval Query Length Optimization,"Determining the optimal length of the LM's prefix to use as a query for the retriever. Too short a query might lack sufficient context, while too long a query might dilute the relevance of the most recent tokens.","Designing the document selection component of an InContext RALM system, where a portion of the LM's input prefix is used to query an external retriever.","Experiment with varying the 'retrieval query length' (λ), which is the number of tokens from the end of the LM's prefix used to form the retrieval query. Identify a 'sweet spot' where the query is contextual enough without diluting the most relevant recent information.","Improved LM performance by using an optimally sized retrieval query (e.g., 32 tokens for BM25, 64 tokens for dense retrievers), leading to more relevant retrieved documents.","Enhancing the relevance of retrieved documents, improving the overall grounding quality of RALM systems.",32
Retrieval Stride Optimization,Balancing the performance gains from frequent document retrieval with the computational cost and latency of calling the retriever and recomputing LM embeddings during generation.,"Implementing an InContext RALM system where documents are retrieved and prepended to the LM input. Retrieval operations have a cost, but documents' relevance can degrade over time.","Define a 'retrieval stride' (s), which is the number of tokens generated between consecutive retrieval operations. Experiment with different stride values to find an optimal balance. Smaller strides (more frequent retrieval) generally improve performance but increase cost.","Improved LM performance (lower perplexity) by retrieving documents more frequently (smaller stride), allowing for higher-resolution grounding. A practical trade-off (e.g., s=4) can be chosen to balance performance and runtime.","Optimizing the runtime and performance of RALM systems, managing computational resources for real-time generation.",32
Optimal Document Count for In-Context Learning,"When augmenting an LM with retrieved documents, providing too few documents might miss relevant information, while providing too many can exceed context window limits, dilute relevance, or increase computational cost.","Implementing InContext RALM or similar retrieval-augmented generation (RAG) systems where multiple retrieved documents are candidates for inclusion in the LM's input, especially for tasks like Open-Domain Question Answering.","Empirically determine the optimal number of retrieved documents to prepend to the LM's input. This involves evaluating performance (e.g., perplexity, QA exact match) with varying numbers of documents (e.g., 1, 2, 4...). The paper suggests that often, most gains are achieved with a small number (e.g., one or two documents).",Maximized performance for retrieval-augmented LMs while efficiently utilizing the context window and managing computational resources. Significant improvements can be seen even with a minimal number of documents.,"Optimizing the input for retrieval-augmented LMs, managing context window constraints, improving efficiency of RAG systems, enhancing factual accuracy in QA.",32
Zero-Shot LM Reranking,"Improving the relevance of documents retrieved by a general-purpose retriever (e.g., BM25) for the specific task of language modeling, especially when training a dedicated reranker is not feasible or desired.",An InContext RALM system has retrieved a set of 'k' candidate documents using an initial retriever. A more semantically aware ranking is needed to select the best document to prepend to the LM input.,"Use an existing Language Model (LM) to perform zero-shot reranking. For each candidate document, concatenate it with a portion of the LM's prefix (e.g., the last 's'' tokens) and have the LM estimate the probability of the upcoming text (or a proxy like the preceding stride). The document yielding the highest probability is selected. This can be done with the generation LM itself or a smaller, faster LM, even via API.","Consistently better LM performance (lower perplexity) compared to using only the top-1 document from the initial retriever, by incorporating a semantic signal from the LM for document selection. Enables reranking without additional training.","Improving document selection for RALM, leveraging existing LMs for semantic ranking, reducing the need for dedicated reranker training.",32
Predictive Reranking (Trained LM-Dedicated Reranker),"Further optimizing document selection for InContext RALM beyond general-purpose or zero-shot methods, by training a reranker specifically to predict which document will best aid the LM in generating the upcoming text.","An InContext RALM system has retrieved a set of 'k' candidate documents. There is availability of training data from the target corpus, and the goal is to achieve maximal LM performance by selecting the most relevant document.","Train a dedicated reranker (e.g., a fine-tuned RoBERTa model) as a classifier. For each training example, given an LM prefix and a candidate document, the reranker learns to produce a scalar score resembling the document's relevance for the continuation of the prefix. This training uses the LM's own signal (e.g., p(y|d, prefix)) as a target for relevance. The reranker then selects the document with the highest predicted relevance score.","Significant additional gains in LM performance (lower perplexity) compared to zero-shot reranking, demonstrating the effectiveness of domain-specific training for document selection.","Maximizing document relevance for RALM, achieving state-of-the-art performance in knowledge-intensive LM tasks, fine-tuning document selection for specific domains.",32
SelfCalibration,Gauging confidence levels of LLM answers and deciding when to accept or revise them.,"When an LLM provides an answer, and its reliability or confidence needs to be assessed.","First, prompt an LLM to answer a question. Then, build a new prompt that includes the question, the LLM's answer, and an additional instruction asking whether the answer is correct.",Useful for gauging confidence levels and informing decisions on accepting or revising original answers.,"Assessing LLM confidence, improving answer reliability.",33
Demonstration Ensembling (DENSE),Reducing variance and improving accuracy of LLM outputs in FewShot Prompting.,"When a training set is available, and robustness of FewShot Prompting is desired.","Create multiple fewshot prompts, each containing a distinct subset of exemplars from the training set. Aggregate their outputs to generate a final response.","Reduces the variance of LLM outputs and often improves accuracy, at the cost of increasing model calls.",Improving robustness and accuracy of FewShot models.,33
SelfConsistency,Reducing the variance of LLM outputs and improving accuracy in reasoning tasks.,"When multiple different reasoning paths can lead to the same answer, and robustness is desired.",Prompt the LLM multiple times to perform CoT (with a non-zero temperature to elicit diverse reasoning paths). Use a majority vote over all generated responses to select a final response.,"Shown improvements on arithmetic, commonsense, and symbolic reasoning tasks by leveraging diverse reasoning paths.","Improving reasoning accuracy, reducing errors in complex tasks.",33
SelfVerification,Scoring multiple candidate solutions generated with ChainofThought to select the best one.,"When multiple CoT solutions are generated, and a method is needed to evaluate their quality.",Generate multiple candidate solutions with ChainofThought (CoT). Score each solution by masking certain parts of the original question and asking an LLM to predict them based on the rest of the question and the generated solution.,Shown improvement on eight reasoning datasets by selecting more reliable solutions.,"Improving reasoning accuracy, selecting optimal solutions from multiple candidates.",33
VerifyandEdit,Improving self-consistency in ChainofThought by editing reasoning paths with retrieved external information.,"When multiple CoT reasoning paths are generated, and their accuracy can be improved by external knowledge.","Generates multiple chains of thought, then selects some to be edited. This is done by retrieving relevant external information to the CoTs and allowing the LLM to augment them accordingly.",Improves the quality and accuracy of reasoning paths by incorporating external verification and editing.,"Improving reasoning accuracy, fact-checking, reducing errors in CoT.",33
Universal SelfConsistency,Aggregating diverse freeform text generations or slightly different answers from SelfConsistency.,"When SelfConsistency is applied, but direct programmatic counting of majority responses is difficult due to freeform text or minor variations.","Similar to SelfConsistency, but rather than programmatically counting, insert all outputs into a prompt template that selects the majority answer (e.g., by having another LLM decide).",Helpful for freeform text generation and cases where the same answer may be output slightly differently by different prompts.,"Robust aggregation of LLM outputs, handling freeform text in ensembling.",33
DiVeRSe,Improving reasoning performance by scoring and selecting reasoning paths from multiple prompts.,"When multiple prompts and reasoning paths are generated for a problem, and a robust selection mechanism is needed.","Create multiple prompts for a given problem, then perform SelfConsistency for each, generating multiple reasoning paths. Score reasoning paths based on each step in them, then select a final response.",Enhances reasoning performance by evaluating and selecting high-quality reasoning paths.,"Improving reasoning accuracy, robust decision-making in complex tasks.",33
Program-aided Language Model (PAL),Solving problems that can be translated directly into executable code.,When a problem involves mathematical or logical operations that are best handled by a code interpreter.,"Translates a problem directly into programming code, which is then sent to a Python interpreter to generate an answer.",Generates accurate answers for mathematical and programming-related problems by offloading computation to a code interpreter.,"Mathematical problem-solving, code generation, algorithmic tasks.",34
TaskWeaver,Transforming user requests into code and leveraging user-defined plugins.,"When user requests need to be fulfilled by executing code, potentially with custom functionalities.","Similar to PAL, it transforms user requests into code but can also make use of user-defined plugins.","Provides flexible code-based task execution, extensible with custom plugins.","Automating tasks, integrating custom functionalities, code generation.",34
Program-Aided Language Models (PAL) Prompting,"LLMs, while capable of natural language reasoning, often lack precision in numerical computation, symbolic manipulation, and executing complex algorithms, leading to errors in tasks requiring exact results.",Enabling LLMs to perform tasks that require rigorous computation or interaction with external computational tools.,"The LLM generates executable code (e.g., Python) as part of its reasoning process. This code is then executed by an external interpreter, and the numerical or symbolic output is used by the LLM to formulate the final answer.","Leverages the strengths of programming languages for precise computation and external tools for complex operations, overcoming the computational limitations of natural language reasoning.","Solving tasks that demand high computational accuracy, symbolic manipulation, or the execution of specific algorithms, by offloading these operations to a programming environment.",34
Tool-Integrated Reasoning Agent (TORA),"Large language models (LLMs) struggle with complex mathematics, as natural language reasoning is suitable for semantic analysis and planning but struggles with precise computation, while program-based methods excel in rigorous operations but face challenges in nuanced reasoning and planning.",Solving challenging mathematical problems using LLMs where both abstract reasoning and precise computation are required.,"Design an agent that synergistically interleaves natural language reasoning with program-based tool use. The agent generates a natural language rationale, then a program for tool use (e.g., computation libraries, symbolic solvers), executes the program, and incorporates the output back into its reasoning process, repeating until the answer is finalized.",TORA models significantly outperform open-source models (13-19% absolute improvements on average across 10 mathematical reasoning datasets). TORA7B surpasses WizardMath70B by 22% absolute on MATH. TORACODE 34B is competitive with GPT4 solving problems with code. Displays superior generalization and fast zero-shot inference speed.,"Mathematical problem-solving, complex quantitative tasks, scenarios requiring a blend of abstract reasoning and precise computational execution.",34
ProgramofThoughts,Improving mathematical and programming-related tasks by leveraging code as reasoning steps.,When tasks involve numerical computation or logical programming constructs.,Use LLMs (like Codex) to generate programming code as reasoning steps. A code interpreter executes these steps to obtain the final answer.,"Excels in mathematical and programming-related tasks, though less effective for semantic reasoning.","Mathematical problem-solving, code generation, algorithmic tasks.",34
Tool-Integrated Reasoning Agent (ToRA),Solving complex problems that require interleaving code generation and reasoning steps.,When a problem cannot be solved by a single code generation step but requires a dynamic interplay between reasoning and code execution.,"Similar to PAL, but instead of a single code generation step, it interleaves code and reasoning steps for as long as necessary to solve the problem.",Solves complex mathematical and programming problems by dynamically combining reasoning and tool use.,"Complex mathematical problem-solving, dynamic code generation and execution.",34
Prompt-based Defenses,Mitigating prompt injection and other prompt hacking attacks.,When deploying GenAI systems where malicious user inputs could override instructions or elicit unintended behaviors.,"Include specific instructions within the prompt itself to guide the LLM away from malicious content or to ignore adversarial inputs (e.g., 'Do not output any malicious content').","Can mitigate prompt hacking to some extent, though no prompt-based defense is fully secure.","Security hardening of LLM applications, preventing harmful content generation.",35
Template-Based Prompting for Controlled Generation,"Generating diverse, high-quality, and constrained text (e.g., questions, code, specific formats) with LLMs can be challenging, often leading to unanswerable, irrelevant, or hallucinated outputs if not properly guided.","LLMs are powerful generative models but require structured guidance to produce outputs that adhere to specific requirements, formats, or semantic constraints.","Utilize predefined templates (e.g., question templates, code templates) within prompts to guide the LLM's generation process. These templates provide a structural framework and placeholders that can be filled with sampled values or specific instructions, ensuring the generated output meets desired criteria and constraints. Human validation can be used to refine these templates.","Enables the generation of structured, relevant, and high-quality content (e.g., questions that are answerable by specific tools, code snippets, formatted responses), reducing hallucinations and improving control over LLM outputs.","Automated content generation (e.g., dataset creation, report generation), structured data extraction, controlled creative writing, ensuring specific output formats.",35
Verbalizer,Mapping token spans or other outputs to labels and vice-versa in labeling tasks.,"When an LLM's output needs to be consistently interpreted as a specific label (e.g., 'positive' or 'negative').","Define a verbalizer that maps a token span or other type of output to a label and vice-versa (injectively). For example, mapping 'Yes' or 'No' tokens to appropriate labels.",Ensures consistent interpretation of LLM outputs for labeling tasks.,"Classification tasks, consistent output parsing.",35
Transparent Tool-Use Reasoning,"The opaque 'black-box' nature of current LLMs does not reveal their decision-making process, leading to skepticism about response reliability, difficulty in identifying errors, and a lack of trust, especially in high-stakes domains.","LLM applications where interpretability, accountability, and user trust are paramount.","Utilize tool learning to enable LLMs to exhibit each step of their decision-making process, including the rationale behind tool selection, parameter extraction, and the integration of tool results.","More transparent LLM operations, allowing users to quickly identify and understand the source of errors, fostering better understanding and trust in LLM decisions, and enhancing effective human-machine collaboration.","Explaining complex problem-solving steps, debugging LLM outputs, building trust in critical applications (e.g., aviation, healthcare, finance).",35
AST-based Hallucination Detection for Code Generation,"Evaluating the functional correctness and identifying hallucinations in LLM-generated API calls is challenging. Traditional NLP metrics are insufficient for code structure, and unit tests are often impractical due to multiple correct answers, overlapping functionality, and complex execution environments. Hallucinations can manifest as invoking entirely imagined tools or incorrect API usage.","An LLM generates API calls in response to natural language prompts. A robust, automated, and offline method is needed to verify the correctness of these generated calls and distinguish between functional errors and outright hallucinations.","Define a hallucination as an API call that is not a subtree of any API in a predefined database. Use Abstract Syntax Tree (AST) subtree matching to compare the generated API call's AST with the ASTs of known, correct APIs in the dataset. This involves matching on API names and specified arguments, while accounting for optional arguments.","The AST-based metric provides a precise measure of both functional correctness and API hallucination, showing a strong correlation with human evaluation. It enables efficient and robust offline evaluation of LLM-generated API calls, making it tractable to assess performance at scale.","Automated evaluation of LLM-generated code, benchmarking LLMs for tool use, improving the reliability and trustworthiness of AI-generated programs, identifying and quantifying hallucination in code generation.",35
Constraint-Aware API Selection,"LLMs need to select APIs that not only fulfill a functional description but also adhere to specific user-defined constraints (e.g., performance, resource usage, accuracy, cost, latency). This requires the LLM to reason about and categorize API calls based on multiple constraint parameters.","Users provide natural language prompts for API calls that include explicit or implicit constraints (e.g., 'image classification model that uses less than 10M parameters but maintains an ImageNet accuracy of at least 70%'). The LLM must navigate a landscape of available APIs, each with varying characteristics, to find the optimal match.","Incorporate instructions with embedded constraints into the LLM's training dataset. The API documentation used during training (and potentially retrieval during inference) includes detailed information about parameters, performance, efficiency, and other relevant metrics for each API. This allows the LLM to learn to comprehend and reason about these constraints when making API selections.","The LLM demonstrates the ability to navigate APIs while considering trade-offs between constraints, even in challenging scenarios where accuracy drops across all models. It can select appropriate APIs that satisfy both functional and non-functional requirements.","Enabling LLMs to make informed API choices based on user preferences, optimizing tool usage based on operational requirements, enhancing the practical applicability of LLM-generated code.",35
Confidence Estimation (Self-rated Probabilities),"LLMs often generate incorrect answers (hallucinations) with high confidence, making it difficult for users or downstream systems to trust their outputs or know when to abstain from answering.","Applications requiring high reliability and trustworthiness, where the LLM's uncertainty needs to be communicated or leveraged for decision-making (e.g., selective generation, human-in-the-loop systems, safety-critical applications).","Implement mechanisms (e.g., specific prompting strategies like PTrue or PCorrect, sampling multiple responses and reflecting, or specialized model architectures) that encourage the LLM to output an explicit confidence score or probability alongside its generated answer, reflecting its internal assessment of correctness.","Provides a valuable signal for the LLM's internal uncertainty, which can be used to improve selective generation, guide abstention, or flag outputs for human review, thereby enhancing overall system trustworthiness and enabling controllable accuracy-coverage tradeoffs.","Guiding abstention mechanisms, flagging potentially incorrect answers, improving overall system reliability, enabling controllable accuracy-coverage tradeoffs, enhancing human-AI collaboration.",35
Structured API Documentation for LLM Consumption,"Large Language Models (LLMs) struggle to effectively understand and utilize diverse, unstructured, and frequently updated API documentation from various sources, hindering their ability to generate accurate and constrained API calls.","An LLM-powered system needs to interact with a vast and dynamic ecosystem of external APIs. The raw documentation for these APIs is often inconsistent, incomplete, or difficult for an LLM to parse and reason about directly.","Curate and standardize API documentation by converting disparate model cards and documentation into a uniform, machine-readable JSON object format. This format includes specific, semantically rich fields such as `domain`, `framework`, `functionality`, `apiname`, `apicall`, `apiarguments`, `environmentrequirements`, `examplecode`, `performance`, and `description`. This structured representation serves as a canonical knowledge base for the LLM and its associated retrieval system.","Provides a consistent and easily parsable knowledge source for the LLM, enabling it to better comprehend API capabilities, parameters, and constraints. This leads to improved accuracy in API call generation, facilitates constraint-aware reasoning, and supports dynamic adaptation to API changes through efficient retrieval.","Building robust LLM-tool integration systems, enhancing LLM's ability to reason about external services, creating scalable knowledge bases for AI agents, improving the reliability of AI-generated code.",35
Robust Tool-Augmented Processing,"LLMs are highly sensitive to user inputs, where minor modifications can elicit substantial changes in responses, highlighting a lack of robustness. Additionally, external tools introduce new safety concerns like adversarial attacks on tool outputs.","Real-world applications with diverse and potentially noisy user inputs, and the need for stable, reliable, and secure LLM performance.","Integrate specialized tools to reduce reliance on the statistical patterns in training data, as tools provide a consistent input/output interface. Implement rigorous validation of tool outputs and mechanisms to detect harmful information to prevent adversarial attacks.","Increased resistance of LLMs to input perturbations, enhanced adaptability to new environments, stabilized model behavior, and reduced risks associated with input errors and malicious tool outputs, leading to safer and more reliable systems.","Building reliable LLM applications in production, mitigating hallucination, defending against adversarial attacks on tool outputs, ensuring consistent behavior across varied user prompts.",35
Tool-Augmented Response Synthesis,"Raw outputs from diverse tools (e.g., text, numbers, code, videos, images) are often complex, varied, and impractical to present directly to users; they require synthesis and integration with the LLM's internal knowledge.","Outputs received from one or more external tools, the LLM's internal knowledge base, and the original user query that needs a comprehensive answer.","The LLM synthesizes information relevant to the user query from the tool outputs and integrates its own knowledge to construct a comprehensive, coherent, and user-friendly response. Methods include direct insertion (for simple outputs) or more sophisticated information integration (e.g., compression, schema-based simplification, dynamic function generation) to handle lengthy or complex outputs. Refining the response using tool feedback is also employed.","A superior, well-informed, and contextually relevant final response to the user, enhancing user experience by leveraging both external data and the LLM's generative capabilities, while also mitigating biases from the LLM itself.","Generating final answers to user queries, summarizing tool results, explaining complex calculations, creating multimodal responses, providing well-informed explanations.",35
Reference-Supported Generation,"Ensuring factual accuracy and transparency in AI-generated long-form answers, and making human evaluation of factual claims easier and more objective.","An AI system generates answers that need to be verifiable and trustworthy, especially in domains where factual correctness is paramount (e.g., question answering).","Design the AI system (e.g., a Browser-Assisted LLM) to actively collect and include references (e.g., quoted passages from web pages with source information) alongside its generated answer. These references are then used by human evaluators to judge the factual accuracy and support for claims in the answer.","Improves the factual accuracy of generated answers, increases transparency by allowing users/evaluators to trace claims back to sources, and makes human evaluation more efficient and less subjective.","Long-form question answering, summarization, content generation where verifiability is critical, reducing hallucinations.",35
Voyager,Enabling agents to acquire new skills and explore open-world environments through self-directed learning.,"When an agent needs to operate in complex, open-ended environments (e.g., Minecraft) and continuously learn new skills.",Composed of three parts: 1. Proposes tasks for itself to complete to learn more about the world. 2. Generates code to execute these actions. 3. Saves these actions to be retrieved later as part of a long-term memory system.,"Agents can acquire new skills and navigate open-world environments, applicable to real-world tasks requiring lifelong learning.","Lifelong learning, open-world exploration, skill acquisition.",36
Ghost in the Minecraft (GITM),Enabling agents to achieve arbitrary goals in open-world environments by recursive subgoal decomposition and iterative planning.,"When an agent needs to operate in complex, open-world environments (e.g., Minecraft) and achieve high-level goals.","Starts with an arbitrary goal, breaks it down into subgoals recursively, then iteratively plans and executes actions by producing structured text (rather than writing code). Uses an external knowledge base and a memory of past experience.","Generates capable agents for open-world environments, achieving complex goals through structured planning.","Open-world task completion, complex goal achievement, hierarchical planning.",36
Commonsense Reasoning,"Language agents, despite their vast knowledge, may fail to incorporate implicit, real-world knowledge and common-sense rules into their plans, leading to illogical or impractical outcomes.","An agent is generating plans for human users in real-world scenarios where adherence to unstated, generally accepted rules and expectations (e.g., not visiting the same attraction repeatedly, not teletransporting) is crucial for plan feasibility and user satisfaction.","The agent is designed or prompted to explicitly consider and apply 'commonsense constraints' during the planning process. This involves understanding implicit rules of the world and human behavior, beyond explicit instructions, to ensure the generated plan is logical and practical.","Produces more realistic, practical, and human-aligned plans by preventing violations of common-sense expectations, thereby increasing the plan's feasibility and user acceptance.","Ensuring travel plans include reasonable city routes, diverse restaurants/attractions, non-conflicting transportation, and other implicit logical rules.",37
Constraint Satisfaction,"Real-world planning tasks involve numerous explicit and implicit constraints that an agent's plan must adhere to simultaneously, making plan generation complex.","An agent is generating a plan (e.g., a travel itinerary) that must satisfy multiple conditions, such as budget, user preferences (hard constraints), logical consistency, and commonsense rules (commonsense constraints).","The agent incorporates mechanisms to perceive, understand, and integrate various types of constraints (e.g., 'Environment Constraints', 'Commonsense Constraints', 'Hard Constraints') throughout its planning process. This involves checking proposed actions/plans against these constraints and adjusting if violations occur, often requiring a holistic approach.","Produces feasible and acceptable plans that meet all specified requirements, improving the utility, reliability, and user satisfaction of the agent's output.","Adhering to budget, room rules, cuisine preferences, reasonable city routes, diverse restaurants/attractions, non-conflicting transportation, and minimum night stays in travel planning.",37
Long-Horizon Planning,"Planning tasks that span many steps or a long duration involve a large number of interdependent decisions, making it difficult for agents to maintain coherence, optimality, and manage information over time.","An agent needs to create a plan for an extended period (e.g., a multi-day trip) where early decisions significantly impact later options and overall feasibility, and the complexity increases with duration.","The agent employs strategies to manage the complexity of long sequences of actions, such as task decomposition, maintaining a robust working memory of intermediate states, and potentially using forward-looking heuristics or backtracking to explore future implications of current decisions. This addresses the challenge of managing 'a large number of interdependent decisions'.","Enables the agent to generate comprehensive, consistent, and feasible plans for extended periods, overcoming the limitations of short-sighted decision-making and improving performance in complex, multi-step scenarios.","Planning multi-day travel itineraries (e.g., 3-day, 5-day, 7-day trips) where decisions on places, lodging, transportation, and dining are interdependent.",37
LACE (Local Agnostic attribute Contribution Explanation),"Black-box models lack interpretability for individual predictions, especially regarding feature interactions, and existing methods are either qualitative, quantitative, or computationally expensive for interactions.","Structured data, black-box classification models, need for local (individual instance) explanations.","Captures local behavior using K-nearest neighbors, trains a rule-based local surrogate model (e.g., L3) to extract relevant patterns (attribute-value conjunctions). Quantifies influence of individual attributes and these patterns via 'prediction difference' (change in probability upon omission, approximated by marginalization). Automatically tunes K for locality scope. Visualizes contributions with bar plots.","Provides both qualitative (local rules) and quantitative (prediction difference for individual attributes and their interactions) explanations for individual black-box predictions, efficiently handling feature interactions. Enhances trust and debugging.","Explaining individual black-box classifier predictions, debugging models, building trust, satisfying regulatory requirements (e.g., GDPR's right to explanation).",38
Counterfactual Explanations,"Users want to understand the minimal changes to an instance's features that would alter its prediction to a desired outcome, providing actionable insights into how to achieve a specific model decision or avoid an undesired one.","Model-agnostic local interpretability for black-box classification or regression models, particularly in high-stakes decision-making or fairness analysis, where users need to know 'what if' scenarios for individual predictions.","Identify the smallest possible modifications to the feature values of a given instance that would cause the black-box model to produce a different, desired prediction outcome. This involves searching for an instance close to the original but on the other side of the decision boundary.","A modified instance (the counterfactual) that is very similar to the original but receives a different prediction, along with the specific, minimal feature changes required to achieve that outcome. This provides an intuitive and actionable explanation.","Understanding decision boundaries, providing actionable advice to users, debugging model biases, fairness analysis, building user trust by showing control over outcomes, satisfying 'right to explanation' requirements.",38
DivExplorer (Divergent Subgroup Exploration),"Lack of understanding of how black-box classification models behave differently across specific data subgroups, hindering model validation, fairness assessment, and debugging. Existing methods are often supervised or incomplete.","Structured data, black-box classification models, need to identify and characterize data subgroups with 'peculiar divergent behaviors' (e.g., higher FPR/FNR).",Defines 'h-divergence' to quantify behavioral differences in subgroups. Uses Bayesian statistics for significance. Employs efficient Frequent Pattern Mining (FPM) algorithms for exhaustive identification of all 'frequent itemsets' (subgroups). Applies Shapley values to attribute local item contributions to itemset divergence and generalizes Shapley values for 'global item divergence' to capture overall item influence. Identifies 'corrective items' that reduce divergence. Includes redundancy pruning for summarization. Generalizes to scoring and ranking systems.,"Automatically identifies and characterizes all sufficiently represented divergent data subgroups, providing local and global insights into item contributions and revealing corrective behaviors. Enables comprehensive model validation, fairness auditing, and debugging.","Model validation, testing, error analysis, evaluation of model fairness, debugging, identifying bias, understanding model behavior at subgroup granularity.",38
DivExplorer Interactive System (Interactive Divergent Subgroup Exploration),"Users need an interactive way to explore, analyze, and understand divergent model behaviors in data subgroups, particularly for identifying bias or debugging.","Structured data, black-box classification models, need for interactive exploration of divergent subgroups identified by the DivExplorer algorithm.","Integrates the DivExplorer algorithm into a web application. Provides a UI with a sortable table of divergent itemsets (with pruning), bar graphs for local item contributions (Shapley values), a lattice visualization for exploring subset relationships and corrective items, global item influence visualizations, and search/drilldown functionalities.","Facilitates interactive exploration and analysis of divergent subgroups, helping users identify bias, debug models, and understand classifier behavior at a granular level.","Analyzing and debugging classifiers, identifying bias in classifiers, exploring model behavior in data subgroups, model validation.",38
xPlain (Interactive Human-in-the-Loop Explanation Framework),"Human experts struggle to interactively inspect, understand, debug, and compare black-box model behaviors for individual predictions.","Structured data, black-box classification models, need for interactive exploration and comparison of local explanations.","Integrates LACE for generating class-dependent and model-agnostic explanations. Provides an interactive UI for instance-level explanation, comparison across target classes and classifiers, 'what-if' analysis by tweaking attributes, and user-defined rules. Summarizes multiple local explanations into 'explanation metadata' (attribute, item, local rule views) for global insights.","Facilitates interactive exploration, debugging, and comparison of black-box model predictions, enhancing trust and providing actionable insights for model improvement.","Model validation, debugging, comparison of classifiers, understanding specific predictions, testing hypotheses, satisfying interactivity desiderata of XAI.",38
ChainofDictionary (CoD),Improving machine translation by providing explicit dictionary definitions of words.,When translating phrases where specific word meanings in multiple languages are crucial.,"First, extract words from the source phrase. Then, make a list of their meanings in multiple languages (automatically via retrieval from a dictionary). Prepend these dictionary phrases to the prompt, asking the GenAI to use them during translation.",Improves translation accuracy by providing explicit lexical context.,"Machine translation, handling polysemy in translation.",39
InteractiveChainPrompting (ICP),Dealing with potential ambiguities in translation by involving human clarification.,When a phrase to be translated contains ambiguities that the GenAI might misinterpret.,"First, ask the GenAI to generate sub-questions about any ambiguities in the phrase to be translated. Humans later respond to these questions, and the system includes this information to generate a final translation.","Resolves translation ambiguities through human-in-the-loop interaction, leading to more accurate translations.","High-quality machine translation, ambiguity resolution.",39
Iterative Prompting (for MT),Refining draft translations by integrating human feedback or automated retrieval signals.,When an initial draft translation needs further improvement and quality assurance.,Prompt LLMs to create a draft translation. This initial version is further refined by integrating supervision signals obtained from either automated retrieval systems or direct human feedback.,Produces more refined and accurate translations through iterative human or automated supervision.,"High-quality machine translation, post-editing assistance.",39
Question Clarification,Resolving ambiguity in questions by allowing the LLM to identify and ask clarifying questions.,"When a user's question is ambiguous, and the LLM needs more information to provide an accurate answer.","Allows the LLM to identify ambiguous questions and generate clarifying questions to pose to the user. Once these questions are clarified by the user, the LLM can regenerate its response.","Resolves ambiguity, leading to more accurate and contextually appropriate answers.","Ambiguity resolution, interactive problem-solving, improving user experience.",39
